<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>mushding 的小小天地</title>
  
  <subtitle>歡迎來到 mushding 的雜七雜八生活筆記</subtitle>
  <link href="https://mushding.space/atom.xml" rel="self"/>
  
  <link href="https://mushding.space/"/>
  <updated>2023-04-10T07:30:56.683Z</updated>
  <id>https://mushding.space/</id>
  
  <author>
    <name>mushding</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(十六)：AdamW 與 Adam 差在哪裡…？</title>
    <link href="https://mushding.space/2023/03/25/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E5%85%AD-%EF%BC%9AAdamW-%E8%88%87-Adam-%E5%B7%AE%E5%9C%A8%E5%93%AA%E8%A3%A1%E2%80%A6%EF%BC%9F/"/>
    <id>https://mushding.space/2023/03/25/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E5%85%AD-%EF%BC%9AAdamW-%E8%88%87-Adam-%E5%B7%AE%E5%9C%A8%E5%93%AA%E8%A3%A1%E2%80%A6%EF%BC%9F/</id>
    <published>2023-03-25T08:21:17.000Z</published>
    <updated>2023-04-10T07:30:56.683Z</updated>
    
    <content type="html"><![CDATA[<p>AdamW 在 2017 年提出，它與在 2014 年提出的 Adam 差在哪裡，而 AdamW又是發現了 Adam 有什麼可以改進的地方嗎？</p><p>keywords: AdamW、Adam <span id="more"></span></p><h2 id="一句話總結">一句話總結</h2><p>簡單用一句話總結 AdamW，因為 Adam 加上 Weight decay實作方法不合理，所以微微修改 Weight decay 加上去的地方，使得 AdamW有計算量少、數學公式較合理等特色</p><h2 id="weight-decay-發生什麼事">Weight decay 發生什麼事？</h2><p>在前一章介紹了 Weight decay，它是由 L2 Regularization延伸出來的概念，當在損失函數中加入權重的平方項，將損失函數值對權重值作偏微分得到<span class="math inline">\(2\lambda\eta w\)</span> 這一項，這一大坨就是Weight decay (更詳細的推導過程可以參考：<ahref="https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%BA%94-%EF%BC%9A%E6%88%91%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%A8%93%E7%B7%B4%E5%A5%BD%EF%BC%9B%E5%8F%AF%E6%98%AF%E6%B8%AC%E8%A9%A6%E4%B8%8D%E5%A5%BD%E6%80%8E%E9%BA%BC%E8%BE%A6%E2%80%A6%EF%BC%9F-overfitting-%E8%88%87-regularization/">你所不知道的Pytorch 大補包(十五)：我的模型訓練好；可是測試不好怎麼辦…？- overfitting與 regularization</a>)</p><p><span class="math display">\[\mathcal{L} = \mathcal{L_{\mathrm{class}}(f(x,w),y)} + \lambda\sum_{i=0}^n w_i^2\]</span></p><p><span class="math display">\[w_{t+1} = w_t - \eta \frac{\partial \mathcal{L}_\mathrm{class}}{\partialw_t}-2\eta\lambda w_t\]</span></p><p>然而在這篇文章中有一個假設，假設我們的優化器是用最原始的 SGD，連動量Momentum 都沒有，才會推導出 <span class="math inline">\(2\lambda\etaw\)</span> 這一項</p><p>那如果是 Adam 會變成怎樣呢？首先是 Adam 的公式：</p><p><span class="math display">\[w_{t+1} = w_t-\eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}\]</span></p><p><span class="math display">\[m_{t} = \beta_1\cdot m_{t} + (1-\beta_1)\cdot \nabla g_{t-1}\]</span></p><p><span class="math display">\[v_{t} = \beta_2\cdot v_{t} + (1-\beta_2)\cdot (\nabla g_{t-1})^2\]</span></p><p>再把 <span class="math inline">\(\nabla g_t\)</span> 拆開：</p><p><span class="math display">\[\begin{aligned}m_{t} &amp;= \beta_1\cdot m_{t} + (1-\beta_1)\cdot \nabla g_{t-1}\\&amp;=\beta_1\cdot m_t + (1-\beta_1) \cdot \nabla g_ {t-1} +\color{red}(1-\beta_1) \cdot 2\lambda w\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}v_{t} &amp;= \beta_1\cdot v_{t} + (1-\beta_1)\cdot \nabla (g_{t-1})^2\\&amp;=\beta_1\cdot v_t + (1-\beta_1) \cdot \nabla (g_ {t-1})^2 +\color{red}(1-\beta_1) \cdot (4w\nabla g+4\lambda w^2)\end{aligned}\]</span></p><p>可以看到在公式後面紅紅的地方就是因 Weight decay 而多產生的常數項</p><p>AdamW 這篇作者認為，在 SGD時，因為優化器額外項不多不複雜，所以最後的常數項數值都會是 <spanclass="math inline">\(2\lambda w\)</span></p><p>但後來的優化器加上動量、加上動態學習率的分母，早早就加在損失函數上的L2Regularization，會隨著各種微分，數值不僅會散掉，同時還會增加不少額外的計算量</p><p>因此作者提出 Adam with decoupled weight decay (AdamW)，如果要在 Adam中使用 Weight decay，不會使用 L2 Regularization加在損失函數上的概念，而是直接加在優化器上，如圖 (論文原圖)：</p><p><img src="https://i.imgur.com/1SoW9fl.png" alt="Image" /></p><p>也就是剛剛 Adam 一大坨看不懂的東西會直接變成這樣：</p><p><span class="math display">\[w_{t+1} =w_t-\eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}-\color{red}2\lambdaw\]</span></p><p><span class="math display">\[m_{t} = \beta_1\cdot m_{t} + (1-\beta_1)\cdot \nabla g_{t-1}\]</span></p><p><span class="math display">\[v_{t} = \beta_2\cdot v_{t} + (1-\beta_2)\cdot (\nabla g_{t-1})^2\]</span></p><p>直接套在優化器後面，就不會因經過很多層微分運算而有：計算量大、數值分散等問題，而且從數學式子角度來看，也比較直白好理解</p><p>至於 AdamW 真的會比 Adam好嗎？論文中當然會是說效果比較好啦，但真正情況就要看各個實驗的資料集了，不過可以確定的是AdamW 的運算量比 Adam 小的。</p><p>當然最重要的是，如果實驗中沒有使用到 Weight decay 的話，那 Adam 與AdamW 是一模一樣的！</p><h2 id="reference">Reference</h2><p><ahref="https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html">AdamWand Super-convergence is now the fastest way to train neural nets(fast.ai) (英文很詳細)</a></p><p><ahref="https://blog.csdn.net/weixin_45743001/article/details/120472616">Adam和AdamW的区别(一句話總結)</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;AdamW 在 2017 年提出，它與在 2014 年提出的 Adam 差在哪裡，而 AdamW
又是發現了 Adam 有什麼可以改進的地方嗎？&lt;/p&gt;
&lt;p&gt;keywords: AdamW、Adam</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(十五)：我的模型訓練好；可是測試不好怎麼辦…？- overfitting 與 regularization</title>
    <link href="https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%BA%94-%EF%BC%9A%E6%88%91%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%A8%93%E7%B7%B4%E5%A5%BD%EF%BC%9B%E5%8F%AF%E6%98%AF%E6%B8%AC%E8%A9%A6%E4%B8%8D%E5%A5%BD%E6%80%8E%E9%BA%BC%E8%BE%A6%E2%80%A6%EF%BC%9F-overfitting-%E8%88%87-regularization/"/>
    <id>https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%BA%94-%EF%BC%9A%E6%88%91%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%A8%93%E7%B7%B4%E5%A5%BD%EF%BC%9B%E5%8F%AF%E6%98%AF%E6%B8%AC%E8%A9%A6%E4%B8%8D%E5%A5%BD%E6%80%8E%E9%BA%BC%E8%BE%A6%E2%80%A6%EF%BC%9F-overfitting-%E8%88%87-regularization/</id>
    <published>2023-03-16T08:56:28.000Z</published>
    <updated>2023-03-28T03:38:04.677Z</updated>
    
    <content type="html"><![CDATA[<p>overfitting、underfitting這兩個詞相信有在碰深度學習人一定都不陌生，學校裡有都有教。但是在實作中，遇到什麼樣子的情況可以稱作overfitting？網路會有怎樣的表現？下一步要怎麼來解決？</p><p>以下文章會把目光放在 overfitting 上來講解</p><p>keywords: Overfitting、Regularization、Weight Decay、LabelSmoothing、Warmup <span id="more"></span></p><h2 id="什麼是-overfittingunderfitting">什麼是overfitting、underfitting</h2><p>在深度學習中會使用 Loss表示網路找到的迴歸區線與現實資料分佈的差異，並且利用 Loss進一步算出梯度後更新參數，使網路更符合現實資料的分佈</p><p>在實作中會把資料集分為三種：訓練集 Training Set、驗證集 ValidationSet、測試集 TestingSet，不同的資料集會有著不同的資料分佈，但理論上因為是從同一筆資料分出來的，所以彼此之間應該不會差太多</p><p>Underfitting 的意思是：訓練得很不好 (訓練 Loss 高)</p><p>Overfitting 的意思是：訓練得很好 (訓練 Loss 低)，可是測試時不好 (測試Loss 高)</p><p>如下圖：左圖是 underfitting，中圖是正常，右圖是 overfitting</p><p><img src="https://i.imgur.com/i0fDKv1.png" alt="Image" /></p><p>用下面的網站來進一步解釋(這是一個簡單的迴歸線視覺化網站，裡面有很多東西可以自定義，可以解釋很多深度學習的一些現象)</p><p><a href="http://playground.tensorflow.org/">Tinker With a NeuralNetwork Right Here in Your Browser.</a></p><p>Underfitting 的意思是，訓練 Loss還太高，網路迴歸的能力還沒有很好，常發生在</p><ul><li>網路訓練初期</li><li>網路架構太淺</li></ul><p><img src="https://i.imgur.com/92QDjeE.png" width="50%" height="50%" /></p><p>而 Overfitting 的意思是，訓練 Loss很好、網路在訓練資料集有著很強的能力，可是面對新的驗證資料分佈時，反而效果變很差，驗證Loss 很高</p><p>最明顯的特是：網路訓練到後期，驗證 Loss 與訓練 Loss有一段小差距，甚至這個差距還會越來越大，驗證 Loss 不斷的在上升</p><p><img src="https://i.imgur.com/wjlyCLJ.png" width="50%"></p><p>再舉一個我的親身經驗：下面是我其中一個實驗訓練與驗證 Loss的曲線圖：紅框的部份很明顯訓練跟驗證間隔拉大了</p><p><img src="https://i.imgur.com/h3nhJmv.png" alt="Image" /></p><h2 id="如何解決-overfitting">如何解決 Overfitting</h2><p>相較於 Underfitting，Overfitting的成因複雜的很多，不過倒是可以總結成一句話：網路泛化能力 Generalization不好的時候會發生，也就是網路只要換一個資料集就沒用了，完全沒什麼自行推論沒看過的資料的能力</p><p>那…泛化能力不好又是如何發生？網路中有過度複雜以及不具無意義的特徵，而網路又過度偏好這些複雜的特徵，使得迴歸區線過於複雜</p><p>…聽不懂？一樣來看剛剛網網站的例子，從右上角的圖可以很明顯發現網路Overfitting了，而在左手邊權重值的地方，仔細看可以發現其中一些權重的輸出值特別高(線的顏色特別深)，使得網路過份依賴這些複雜的權重。</p><p><img src="https://i.imgur.com/4Lkyskt.png" alt="Image" /></p><p>這樣會有什麼問題，如果我們換一個資料集，這些權重過大的特徵很有可能與新資料集的特徵完全不符合，導致訓練很好，但是驗證不好的情況發生</p><h2 id="解決-overfitting-的一些方法">解決 Overfitting 的一些方法</h2><p>知道了發生原因之後，接下來就介紹幾個解決 Overfitting 的方法</p><h3 id="增加資料集">增加資料集</h3><p>泛化能力不好 -&gt; 資料集不夠多樣化 -&gt; 需要更多的資料去訓練 -&gt;增加網路的 Robust (強健性)</p><p>這個方法是最最跟本的解決之道，就是…既然問題出在資料集上面嘛…那就想辦法再加更多資料集啦</p><p>但有時礙於資料不好取得，沒有辦法拿到太多真實的資料時，還有資料擴增可以使用(Data Augmentation) 一樣也可以增加資料的複雜程度</p><p>增加資料集、使用資料擴增都是解決 Overfitting的根本之道，那如果我都做了還是發生 Overfitting 呢？下面還有幾個 trick可以試試看</p><h3 id="weight-decay">Weight Decay</h3><p>在損失函數中有一項叫做規則項 Regularization Term，通常都會是以 L2Regularization 為主。定義一損失函數 <spanclass="math inline">\(\mathcal{L}\)</span>，在後面加上<strong>網路權重的平方和</strong>，也就是L2 Regularization，公式如下：</p><p><span class="math display">\[\mathcal{L} = \mathcal{L_{\mathrm{class}}(f(x,w),y)} + \lambda\sum_{i=0}^n w_i^2\]</span></p><p>規則項目的在處罰網路的權重值，使得網路不要太偏重單一複雜的權重，而出現泛化能力不好的問題。式子中用<span class="math inline">\(\lambda\)</span>超參數來調節規則項的強度，而它有一個特別的名字叫做 Weight decay</p><p>為什麼要叫做 Weight decay 呢？我們把新的損失函式套進 SGD計算梯度算一下：</p><p>定義損失函數</p><p><span class="math display">\[\mathcal{L} = \mathcal{L_{\mathrm{class}}(f(x,w),y)} + \lambda\sum_{i=0}^n w_i^2\]</span></p><p>定義 SGD</p><p><span class="math display">\[w_{t+1} = w_t -\eta \nabla g, \quad \nabla g = \frac{\partial\mathcal{L}}{\partial w_t}\]</span></p><p>把 <span class="math inline">\(\mathcal{L}\)</span> 代入到 SGD中，並且對 <span class="math inline">\(w_t\)</span> 做偏微分，得：</p><p><span class="math display">\[\begin{aligned}w_{t+1} &amp;= w_t - \eta \frac{\partial(\mathcal{L}_{\mathrm{class}}(f(x, w), y)+\lambda \sum_{i=0}^nw_i^2)}{\partial w_t}\\&amp;=w_t - \eta \cdot (\frac{\partial\mathcal{L}_\mathrm{class}}{\partial w_t} + 2\lambda w_t)\\&amp;= w_t - \eta \frac{\partial \mathcal{L}_\mathrm{class}}{\partialw_t}-2\eta\lambda w_t\end{aligned}\]</span></p><p>最後得出來的結果前半項 <span class="math inline">\(w_t - \eta\frac{\partial \mathcal{L}_\mathrm{class}}{\partial w_t}\)</span> 就是SGD，而後面多減了一個常數項 <span class="math inline">\(2\eta\lambdaw_t\)</span></p><p>這個常數項就是 Weight decay 的來源，<spanclass="math inline">\(w_t\)</span>在梯度下降權重一直更新的同時，也會一直多減掉這個常數項，而且還是自己減自己，使得<span class="math inline">\(w_t\)</span>會不斷的越來越小，像元素半衰期一樣，越來越小越來越小…直到接近 0</p><p>因為 Weight decay會使得網路中的每一個權重都不斷的減自己，所以較不會有鶴立雞群的權重，網路不會過度依賴特定複雜的特徵，使得網路泛化能力很差</p><p>規則項有另外一個名稱：懲罰項(penalty)，意思指說，加入這一項後對網路而言效果反而會變差，其實也蠻合理的，如果你叫一個小朋友每天自己檢討自己，久了之後心情一定會變差的嘛</p><p>所以其實加上 Weight decay 雖然能解決 overfitting的問題，但其實背後的原理是犧牲複雜特徵 (減少特徵)換來的，放棄一些離群的資料來換取比較好的損失結果，並沒有解決到最根本的問題：資料</p><p>以下是用網站模擬加了 Weight decay 會發生什麼事，這裡實驗Regularization 設定 0.1 (就是 Weight decay的意思)，發現網路中的每一個參數都相對平均沒有某一個特別突出，且在分類的迴歸線中也可看到網路選擇放棄那些離群的點，利用放棄一些資料來換取更好的Loss</p><p><img src="https://i.imgur.com/O11jPdh.png" alt="Image" /></p><p>SGD、RMSProp、AdaGrad、Adam 這些個優化器都可以加上 weight decay，在Pytorch 中兩個函式都有 <code>weight_decay</code>的參數，其作用就是在調節 L2 Regularization 的大小，當<code>weight_dacay</code> 為 0 就代表不使用，應用起來很方便(調整一下參數就可以了)</p><p><img src="https://i.imgur.com/TmVv9Y0.png" alt="Image" /></p><p><img src="https://i.imgur.com/NrfP5PV.png" alt="Image" /></p><p><img src="https://i.imgur.com/rwph3AM.png" alt="Image" /></p><p><img src="https://i.imgur.com/QY9ie8R.png" alt="Image" /></p><p>在實作中依據網路的情況再設定就可，沒有什麼一定的正確解答，但我自己的個人經驗是SGD 可以設大一點 0.1、Adam 要小一點 0.01 左右</p><h3 id="label-smoothing">Label Smoothing</h3><p>Label Smoothing中文稱標籤平滑化，是應用在分類任務上的一個特別做法，原本在網路給定標籤時輸出機率都是0 or 1 的整數，也就是對一定要全對、錯一定全錯 (目標輸出機率為 1 其它都是0)</p><p>例如有一個 5 分類的任務，假設標籤為 1，則網路最好的機率輸出結果為 (1,0, 0, 0, 0) -&gt; 很肯定的的答案</p><p>那 Label Smoothing 就是把目標變一下，全部以 <spanclass="math inline">\(\alpha\)</span>為主往中間靠近一些，把標籤變得模糊一些不要那麼極端，公式如下：</p><p><span class="math display">\[\begin{equation}  y* =    \begin{cases}      1-\alpha &amp; \text{if $y=1$}\\      \alpha/(n-1) &amp; \text{otherwise}    \end{cases}       \end{equation}\]</span></p><p>以上面的例子假設 <span class="math inline">\(\alpha = 0.4\)</span>就會變成 (0.6, 0.1, 0.1, 0.1,0.1)，讓網路在預測時不要給一個那麼肯定的答案。</p><p>那為什麼要做 label smoothing呢？隨著時間訓練網路對於肯定的資料效果一定是越來越好，輸出的機率值越來越肯定，但是如果突然來了一張模稜兩可的資料，或是…離群資料，網路會不會就看錯了呢？</p><p>隨著這個想法，如果我們能在訓練時加強訓練難度，標籤不要給的太肯定去訓練，但是測試是用原本的標籤來做，這樣訓練難測試簡單就可以一部份避免overfitting 的問題了</p><p>如果使用 cross entropy 作為損失函數的話，實作 label smoothing會非常簡單，這是因為 cross entropy 的公式很好修改：</p><p><span class="math display">\[\mathcal{L} = -\sum_{c=1}^Cw_c\log p(f(x_c))y_c\]</span></p><p>公式中只需要把 <span class="math inline">\(y_c\)</span> 把設定從 (0,1) 改成 -&gt; (<span class="math inline">\(\alpha\)</span>, <spanclass="math inline">\(1-\alpha\)</span>, ...) 就可以了</p><p>cross entropy 實作 label smoothing 很簡單，在 Pytorch 的<code>torch.nn.CrossEntropyLoss</code> 中有 <code>label_smoothing</code>這個參數，就是在設定上面公式的 <spanclass="math inline">\(\alpha\)</span> 值</p><p><img src="https://i.imgur.com/3nggRIh.png" alt="Image" /></p><h3 id="warmup">Warmup</h3><p>模型訓練權重的初始值為隨機生成，因此第一個 epoch 通常有較大的loss，較大的梯度使得模型權重每次改變都較大，可能導致訓練時梯度下降至Local minimum 或 Sharpminimum，進而導致：一、訓練還沒到最低點；二、網路不robust，資料一更動就差很多，而這個就是，泛化能力不好 (overfitting)</p><p><img src="https://i.imgur.com/BzZu94O.png" alt="Image" /></p><p>使用 Learning rate Warmup學習率暖身策略可避免這個問題，在網路學習初期用較小的學習率訓練，使隨機初始化的參數先「暖身」這筆資料集的分布，再把學習率接回原策略正常訓練，主要解決：避免過高的學習率容易導致模型不穩的問題。</p><p>通常的做法：在前 5 個 epoch，學習率由 0<strong>線性</strong>調至初始學習率</p><p><img src="https://i.imgur.com/qXUKtQZ.png" alt="Image" /></p><p>在 Pytorch 中沒有一個很像 Warmup 的函式，最接近的是CosineAnnealingLR</p><p><img src="https://i.imgur.com/fDFLFKH.png" alt="Image" /></p><p>但如果是用其它 lr_scheduler 像是 MultiStepLR 或是 ReduceLROnPlateau要加上 Warmup 的話會比較麻煩，需要借助 github上面大神們的幫助，下面是其中一個我用過的 Warmup，用起來也很簡單，只需要pip install 一下就好了，詳細參考 Readme.md</p><p><ahref="https://github.com/ildoonet/pytorch-gradual-warmup-lr">github-&gt; pytorch-gradual-warmup-lr</a></p><p>上面 Repo 有個要注意的地方，就是它的程式其實是錯的 XDD，有人有發issue 修改，但可能原作者已經放推了，所以沒再後續維護 (我也懶得 fork 它XD)，所以有一行程式要修改一下，大家注意一下</p><p><ahref="https://github.com/ildoonet/pytorch-gradual-warmup-lr/issues/18">Mathis wrong for multiplier=1 #18</a></p><h2 id="reference">Reference</h2><p><a href="https://www.ibm.com/topics/overfitting">IBM What isoverfitting? (總覽)</a></p><p><ahref="https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/">ML| Underfitting and Overfitting (GeekforGeeks)</a></p><p><a href="https://hackmd.io/@allen108108/Bkp-RGfCE">Regularization方法 : Weight Decay , Early Stopping and Dropout (weight decay公式推導)</a></p><p><a href="https://ithelp.ithome.com.tw/articles/10306518">[Day27]Weight Decay Regularization</a></p><p><ahref="https://ithelp.ithome.com.tw/articles/10305524?sc=iThelpR">[Day25]Label Smooth</a></p><p><ahref="http://playground.tensorflow.org/">好玩的網路訓練模擬網站</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;overfitting、underfitting
這兩個詞相信有在碰深度學習人一定都不陌生，學校裡有都有教。但是在實作中，遇到什麼樣子的情況可以稱作
overfitting？網路會有怎樣的表現？下一步要怎麼來解決？&lt;/p&gt;
&lt;p&gt;以下文章會把目光放在 overfitting 上來講解&lt;/p&gt;
&lt;p&gt;keywords: Overfitting、Regularization、Weight Decay、Label
Smoothing、Warmup</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
    <category term="regularization" scheme="https://mushding.space/tags/regularization/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(十四)：後起之秀 - Adam 之為什麼我的 Adam 比 SGD 效果差？</title>
    <link href="https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E5%9B%9B-%EF%BC%9AAdam-%E8%88%87-AdamW-%E5%B7%AE%E5%9C%A8%E5%93%AA%E8%A3%A1%EF%BC%9F/"/>
    <id>https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E5%9B%9B-%EF%BC%9AAdam-%E8%88%87-AdamW-%E5%B7%AE%E5%9C%A8%E5%93%AA%E8%A3%A1%EF%BC%9F/</id>
    <published>2023-03-16T08:46:45.000Z</published>
    <updated>2023-03-26T07:38:25.583Z</updated>
    
    <content type="html"><![CDATA[<p>問：為什麼剛剛前幾個介紹的優化器最近都不怎麼出現過，反而較近期的BERT、最近流行的 Transformer 架構 ViT，都是使用 Adam優化器，是…因為新潮所以使用它嗎？還是 Adam 真的有什麼可取之處？</p><p>keywords: Adam <span id="more"></span></p><h2 id="adam">Adam</h2><p>Adam 優化器在 2014 年提出，相較於 SGD、RMSProp來說是相對比較新的優化器。論文連結：<ahref="https://arxiv.org/abs/1412.6980">Adam: A Method for StochasticOptimization</a></p><p>Adam 名稱來自：Adaptive MomentEstimation，直翻就是「動態動量預估」，其特色是融合了 AdaGrad 與 RMSProp各自的優點，並且在這之上額外加入了 bias-correction</p><p>以下是論文原文：</p><blockquote><p>the name Adam is derived from adaptive moment estimation. Our methodis designed to combine the advantages of two recently popular methods:AdaGrad (Duchi et al., 2011), which works well with sparse gradients,and RMSProp (Tieleman &amp; Hinton, 2012)</p></blockquote><p>公式有點複雜，先來看核心公式：</p><p><span class="math display">\[w_{t+1} = w_t-\eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}\]</span></p><p>Adam 公式中可分為兩個部份，一個長得像 Momentum 記作 <spanclass="math inline">\(\hat{m_t}\)</span> 稱做第一動量 (first momentestimate)，一個長得像 RMSProp 記做 <spanclass="math inline">\(\hat{v_t}\)</span> 稱做第二動量 (second raw momentestimate)，最後分母的 <span class="math inline">\(\epsilon\)</span>是平滑項避免除以 0。</p><p><span class="math inline">\(\hat{m_t}\)</span> 如同 Momentum有歷史梯度平均的資訊，優點為更新速度快，<spanclass="math inline">\(\hat{v_t}\)</span> 如同 RMSProp有歷史梯度平方的平均，優點為動態調整學習率，但又不會因數值太使更新值接近0</p><p>而各別的 <span class="math inline">\(\hat{m_t}\)</span> <spanclass="math inline">\(\hat{v_t}\)</span> 公式列記在下面：</p><p><span class="math display">\[m_{t} = \beta_1\cdot m_{t} + (1-\beta_1)\cdot \nabla g_{t-1}\]</span></p><p><span class="math display">\[v_{t} = \beta_2\cdot v_{t} + (1-\beta_2)\cdot (\nabla g_{t-1})^2\]</span></p><p>Adam 有兩個超參數可調整，<span class="math inline">\(\beta_1\)</span>控制 <span class="math inline">\(m_t\)</span> 預設 0.9，<spanclass="math inline">\(\beta_2\)</span> 控制 <spanclass="math inline">\(v_t\)</span> 預設 0.999，兩個超參數超接近 1目的是使權重更新傾向參考<strong>歷史梯度</strong>而非目前梯度，使網路在遇到較複雜的曲面時有比較穩定的表現(不會因為目前梯度變化大而「三心二意」的)</p><p><span class="math inline">\(\beta_2\)</span> 又比 <spanclass="math inline">\(\beta_1\)</span> 更靠近 1，因為 <spanclass="math inline">\(\beta_2\)</span>負責控制<strong>權重的平方和</strong>，使網路非常非常以歷史權重值為依據更新，如果太傾向考量當前權重值的話(<span class="math inline">\(1-\beta_2\)</span>)，容易使 <spanclass="math inline">\(v_t\)</span> 過大，進而使 <spanclass="math inline">\(m_t/\sqrt{v_t}\)</span> 接近 0 更新不了參數了(就這與 AdaGrad 的老毛病一樣)。</p><p>眼睛尖的人可能已經發現了，為什麼在核心公式中 <spanclass="math inline">\(\hat{m_t}\)</span> <spanclass="math inline">\(\hat{v_t}\)</span> 頭上會有一頂帽子 hat 呢？</p><p>這頂帽子代表的是 bias-correction，經由前時刻的梯度計算出來的 <spanclass="math inline">\(m_t\)</span> <spanclass="math inline">\(v_t\)</span>還會再經過一個偏差估算的步驟，校正式子中的計算誤差，使得最後正式參與更新的是<span class="math inline">\(\hat{m_t}\)</span> <spanclass="math inline">\(\hat{v_t}\)</span></p><p><span class="math display">\[\hat{m_t} = \frac{m_t}{1-\beta_1^t}, \quad \hat{v_t} =\frac{v_t}{1-\beta_2^t}\]</span></p><p>計算誤差！？哪裡有誤差？我怎麼沒看到，我覺得到目前為止都沒什麼問題呀。接下來我們來看看如果不加入bias-corretion，並且時間 <span class="math inline">\(t\)</span> 從 0開始慢慢往後推會發生什麼事情：</p><p>令 <span class="math inline">\(t = 0\)</span> 時，初始動量 <spanclass="math inline">\(m_0 = 0, v_0 = 0\)</span>，且 <spanclass="math inline">\(\beta_1 = 0.9\)</span> <spanclass="math inline">\(\beta_2 = 0.999\)</span></p><p>代入得：</p><p><span class="math display">\[\begin{gather}m_1 = \beta_1 \cdot 0+ (1-\beta_1) \nabla g_0 = 0.1 \nabla g_0\\v_1 = \beta_2 \cdot 0+ (1-\beta_2) (\nabla g_0)^2 = 0.001 \nabla g_0^2\end{gather}\]</span></p><p>如果不做 bias-correction 直接放到核心公式中會得：</p><p><span class="math display">\[\begin{aligned}w_1 &amp;= \\&amp;= w_0 - \eta\frac{m_1}{\sqrt{v_1}+\epsilon}\\&amp;= w_0 - \eta \frac{0.1}{\sqrt{0.001}+10^{-8}}\\&amp;\approxeq w_0 - 3.16\eta\end{aligned}\]</span></p><p>式子中的常數 3.16 只是一個很 ~大概除下來的數字，重點是在網路還沒參考任何歷史資訊下，網路對 <spanclass="math inline">\(w_0\)</span> 也就是初始參數 (實作上是亂數生成的)加權比例竟然有 3 倍之多，這個常數完全是因 <spanclass="math inline">\(\beta_1\)</span> <spanclass="math inline">\(\beta_2\)</span>一大一小所導致的。在網路還根本不知道往那裡收斂，就使 <spanclass="math inline">\(w_0\)</span> 占這麼重要的一部份，顯然不合理</p><p>因此 Adam為了避免<strong>網路學習初期</strong>出現這種不合理的現象，Adam 加上了bias-correction</p><p><span class="math display">\[\hat{m_t} = \frac{m_t}{1-\beta_1^t}, \quad \hat{v_t} =\frac{v_t}{1-\beta_2^t}\]</span></p><p>使得 <span class="math inline">\(w_1\)</span> 改為：</p><p><span class="math display">\[\begin{aligned}w_1 &amp;= \\&amp;= w_0 - \eta\frac{\hat{m_1}}{\sqrt{\hat{v_1}}+\epsilon}\\&amp;= w_0 - \eta \frac{g_t}{g_t+10^{-8}}\\&amp;\approxeq w_0 -\eta\end{aligned}\]</span></p><p>把初期因 <span class="math inline">\(\beta_1\)</span> <spanclass="math inline">\(\beta_2\)</span>一大一小所造成的影響降到最低。</p><p>那既然這個 bias-correction 是針對初期網路設計的，是不是我們可以把Adam 設計成兩段式的…？還是 bias-correction對於後續網路也是有影響的…？</p><p>更詳細的討論可以參考這一篇 stackoverflow裡面討論的很不錯，有興趣的人可以沿伸閱讀一下</p><p><ahref="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for">Whyis it important to include a bias correction term for the Adam optimizerfor Deep Learning?</a></p><p>在 Pytorch 實作中有 Adam 套件可以直接呼叫使用：其中 betas不用修改照預設的就可以了，最需要注意的是學習率的部份</p><p><img src="https://i.imgur.com/Be7rCpb.png" alt="Image" /></p><h2 id="使用-adam-還需要調整學習率嗎">使用 Adam還需要調整學習率嗎？</h2><p>AdaGrad、RMSProp、Adam 這三個優化器都有著同樣的目標 -&gt;動態的調整學習率，在 Pytorch 中也有一個函式庫<code>torch.optim.lr_scheduler</code> 它的目標也是 -&gt;動態的調整學習率，那…如果今天我的網路選用這些優化器，我還會需要lr_scheduler 嗎？或是反過來，我的程式裡有 lr_scheduler我還可以用上述三個優化嗎？</p><p>要回答這個問題可以來參考公式：</p><p><span class="math display">\[w_{t+1} = w_t-\eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}\]</span></p><p>可以發現 Adam 中的學習率與 <span class="math inline">\(m\)</span><span class="math inline">\(v\)</span> 無關，因此 Adam中所謂的動態調整學習率，並沒影響到學習率 <spanclass="math inline">\(\eta\)</span>，而在 Pytorch的實作中這兩個東西也是分開實作的，所以這個問題就可以改成：哪一個排別組合對你的網路是正向幫助的</p><p>根據下列文章表示，雖然使用 Adam，但適時的用 lr_scheduler還是有不錯的效果</p><p>參考自：<ahref="https://discuss.pytorch.org/t/with-adam-optimizer-is-it-necessary-to-use-a-learning-scheduler/66477">WithAdam optimizer, is it necessary to use a learning scheduler?</a></p><h2 id="為什我的網路用-adam-比-sgd-還要差">為什我的網路用 Adam 比 SGD還要差…？</h2><p>你可以說當 <span class="math inline">\(\beta_1=0\)</span> <spanclass="math inline">\(v_t=0\)</span> 時，Adam 其實與 SGD是一模一樣的，也就是 SGD 是 Adam 的一種特例，SGD 是 Adam 的子集合</p><p>也因為 Adam 彈性大、可調參數多，相較於 SGD分佈範圍大，所以比較難調出一個好參數使網路收斂</p><p>一般遇到的問題就是學習率設太大，Adam 一般來說初始學習率會設 <spanclass="math inline">\(10^{-3}\)</span>，而 SGD 可以設 <spanclass="math inline">\(0.1\)</span>，所以在實作上如果只是單純的把 Pytorchcode 從 <code>torch.optim.SGD</code> 改成 <code>torch.optim.Adam</code>效果一定不會好到那裡去…</p><p>參考：<ahref="https://medium.com/ai-blog-tw/deep-learning-%E7%82%BA%E4%BB%80%E9%BA%BCadam%E5%B8%B8%E5%B8%B8%E6%89%93%E4%B8%8D%E9%81%8Esgd-%E7%99%A5%E7%B5%90%E9%BB%9E%E8%88%87%E6%94%B9%E5%96%84%E6%96%B9%E6%A1%88-fd514176f805">為什麼Adam常常打不過SGD？癥結點與改善方案</a></p><h2 id="reference">Reference</h2><p><ahref="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for">Whyis it important to include a bias correction term for the Adam optimizerfor Deep Learning?</a></p><p><ahref="https://hackmd.io/@allen108108/H1l4zqtp4">Adagrad、RMSprop、Momentumand Adam – 特殊的學習率調整方式</a></p><p><ahref="https://discuss.pytorch.org/t/with-adam-optimizer-is-it-necessary-to-use-a-learning-scheduler/66477">WithAdam optimizer, is it necessary to use a learning scheduler?</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;問：為什麼剛剛前幾個介紹的優化器最近都不怎麼出現過，反而較近期的
BERT、最近流行的 Transformer 架構 ViT，都是使用 Adam
優化器，是…因為新潮所以使用它嗎？還是 Adam 真的有什麼可取之處？&lt;/p&gt;
&lt;p&gt;keywords: Adam</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(十三)：我可以在 optimizer 中動態的調整學習率嗎？- RMSProp、AdaGrad</title>
    <link href="https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%B8%89-%EF%BC%9A%E6%88%91%E5%8F%AF%E4%BB%A5%E5%9C%A8-optimizer-%E4%B8%AD%E5%8B%95%E6%85%8B%E7%9A%84%E8%AA%BF%E6%95%B4%E5%AD%B8%E7%BF%92%E7%8E%87%E5%97%8E%EF%BC%9F-RMSProp%E3%80%81AdaGrad/"/>
    <id>https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%B8%89-%EF%BC%9A%E6%88%91%E5%8F%AF%E4%BB%A5%E5%9C%A8-optimizer-%E4%B8%AD%E5%8B%95%E6%85%8B%E7%9A%84%E8%AA%BF%E6%95%B4%E5%AD%B8%E7%BF%92%E7%8E%87%E5%97%8E%EF%BC%9F-RMSProp%E3%80%81AdaGrad/</id>
    <published>2023-03-16T08:44:52.000Z</published>
    <updated>2023-03-23T16:34:21.027Z</updated>
    
    <content type="html"><![CDATA[<p>在上章我們介紹了 SGD 與Momentum，接下來進一步介紹可以自己調整學習率的 RMSProp 與 AdaGrad</p><p>keywords: RMSProp、AdaGrad <span id="more"></span></p><h2 id="為什麼要自己調整學習率">為什麼要自己調整學習率？</h2><p>剛剛加入 Momentum 的 SGD似乎看起來很完美，收斂又快，又有跨過小山丘的能力，那…還有什麼地方可以改進的呢…？</p><p>我們一起來看下面這張圖，來源自：<ahref="https://www.jeremyjordan.me/nn-learning-rate/">Setting thelearning rate of your neural network.</a></p><p><img src="https://i.imgur.com/SODSFGm.png" alt="Image" /></p><p>假設我們的網路是一個類似二次多項式的曲線</p><blockquote><p>當我們學習率設太小：收斂太慢 (左圖)</p><p>學習率設太大：完全找不到最低點，一直跳來跳去 (右圖)</p><p>學習率設得剛剛好：完美！(中圖)</p></blockquote><p>可見如何選擇學習率是一個重要的課題，其影響程度甚至可以使你的網路永遠不收斂，效果就是比別人差。</p><p>那倒底要選擇多大的學習率呢？答案是：我也不知道…，每一個網路有著他自己的特性，所以每個網路最佳的學習率都不太一樣，所以最好的做法就是：學習率不是固定的！而是一個從大慢慢變小的過程。</p><p>由剛剛的圖可以得知通常網路在訓練初期梯度較大因此可以設較大的學習率，而隨著網路訓練慢慢的收斂，學習率也要隨之調整變小，以適應較緩的梯度。</p><p>而至於從什麼時候開始變小，則是根據網路自己的權重來動態的決定，自己決定自己的學習率最合理，人為定的都猜不準</p><p>以下兩個優化器就是按著這個思維來設計，希望可以利用網路自身的權重值來自己決定學習率要如何變小。</p><h2 id="最簡單的想法">最簡單的想法</h2><p>介紹前我們先來看最最簡單的想法，由此出發，更能體會到下面的優化器想要解決什麼事情</p><p><strong>學習率會隨著 epoch增加而變小</strong>，這是核心中的核心概念，那既然是跟時間有關，我們可以把學習率與時間成反比就好了呀，可以得到下面的公式：</p><p><span class="math display">\[w_{t+1} = w_t + \frac{\eta}{t} \nabla g\]</span></p><p>直接把學習率除以時間t，這樣學習率就會隨著時間慢慢的變小了！不過這樣子真的就好了嗎？式子中的時間t好像跟網路一點關聯也沒有，不同的網路學習率的變化基本是一模一樣，所以剛剛的那一句要稍微改一下</p><p><strong>學習率會根據網路權重且隨著 epoch 增加而變小</strong>，AdaGrad及 RMSProp 就是在討論網路權重對於學習率的影響。以下介紹兩種優化器</p><h2 id="adagrad">AdaGrad</h2><p>AdaGrad 全名 AdaptiveGradient，其想法是在網路初期干預不多因此學習率大；網路後期干預多因此學習率小。</p><p>公式如下：</p><p><span class="math display">\[\begin{gather}   w_{t+1} = w_t - \frac{\eta}{\sigma_t} \nabla g\\\sigma_t = \sqrt{G_t+\epsilon}\\G_t = \sum^t_{n=1}g_n^2\end{gather}\]</span></p><p><span class="math inline">\(G_t\)</span> 代表權重值，累積到第 t時刻的梯度平方和，<span class="math inline">\(\epsilon\)</span> 是平滑項(smooth term) 用於避免 <span class="math inline">\(\sigma\)</span> 為 0否則會除 0，一般設為 <span class="math inline">\(10^{-8}\)</span></p><p>AdaGrad 使用<strong>網路加權到 t時刻的權重平方和</strong>來做為除以學習率的分母，因為會隨時間加權的原因，學習率這一項會越來越小，直到接近0。也可理解為網路越後期優化器干預的越多，學習率因此降低</p><p>AdaGrad的優點是不需人工調整學習率；而缺點是收斂到最後，調整多，學習率幾乎降為0，而無法再改進參數值</p><p>在 Pytorch 中 AdaGrad可以很方便的直接呼叫函式庫就可以囉，基本上沒有什麼超參數要特別調</p><p><img src="https://i.imgur.com/65ZEpjx.png" alt="Image" /></p><h2 id="rmsprop">RMSProp</h2><p>RMSProp 是 Hinton教授在上課的講義中提定的一個優化器，並沒有正式發表在論文當中。</p><p>公式如下：</p><p><span class="math display">\[\begin{gather}   w_{t+1} = w_t - \frac{\eta}{\sigma_t} \nabla g\\\sigma_t = \sqrt{\alpha(\sigma_{t-1})^2+(1-\alpha)g_t^2+\epsilon}\end{gather}\]</span></p><p>RMSProp 與 AdaGrad 基本上差不多都是學習率 <spanclass="math inline">\(\eta\)</span> 除上一個由權重決定的分母 <spanclass="math inline">\(\sigma\)</span>，<spanclass="math inline">\(\sigma\)</span>同樣是由當前梯度平方來決定，但是多了一個超參數 <spanclass="math inline">\(\alpha\)</span></p><p>分母的意思為：除了加總當前梯度平方和之外，也考慮前一個時刻的梯度平方和</p><p>實作上 <span class="math inline">\(\alpha\)</span> 會設為0.9，代表當網路後期，優化器干預學習率越多時，偏好使用舊梯度做平方和運算</p><p>這樣做相比於 AdaGrad 計算 1 ~ t時刻的梯度平方和，每一個時刻的權重值都會加起來，RMSProp 因設定 <spanclass="math inline">\(\alpha=0.9\)</span>偏好使用舊梯度，做到類似加權平均的概念，可以避免 <spanclass="math inline">\(\sigma\)</span> 值過大的問題。</p><p>同時 RMSProp 這個概念也很像動量Momentum，在更新權重前除了當前的權重值外也考量前一時刻的權重值，使得RMSProp 相比 AdaGrad 在梯度曲面較複雜的情況也有著比較好的表現。</p><p>在 Pytorch 上有 RMSProp 的實作函式：其中 alpha 參數預設 0.99代表高度依靠歷史梯度來更新參數</p><p><img src="https://i.imgur.com/zoCLYKc.png" alt="Image" /></p><h2 id="reference">Reference</h2><p><a href="https://www.jeremyjordan.me/nn-learning-rate/">推！。Settingthe learning rate of your neural network.</a></p><p><ahref="https://hackmd.io/@allen108108/H1l4zqtp4">Adagrad、RMSprop、Momentumand Adam – 特殊的學習率調整方式</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在上章我們介紹了 SGD 與
Momentum，接下來進一步介紹可以自己調整學習率的 RMSProp 與 AdaGrad&lt;/p&gt;
&lt;p&gt;keywords: RMSProp、AdaGrad</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(十二)：一切的開端 - SGD vs Momentum</title>
    <link href="https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%BA%8C-%EF%BC%9ASGD-vs-Momentum/"/>
    <id>https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%BA%8C-%EF%BC%9ASGD-vs-Momentum/</id>
    <published>2023-03-16T08:43:39.000Z</published>
    <updated>2023-03-20T06:10:04.765Z</updated>
    
    <content type="html"><![CDATA[<p>在以前第九章中，有很 ~ 淺的列舉了一些優化器 optimizer，在第十二 ~十四章中，會更詳細一點去介紹，這些 optimizer的原理，以及當初提出是要改進什麼事？</p><p>keywords:SGD、Momentum <span id="more"></span></p><h2 id="梯度下降-gradient-desent">梯度下降 Gradient Desent</h2><p>還記得在第十章中有介紹了什麼是損失Loss、什麼是梯度，以及網路是如何利用梯度來找到最佳解嗎？如果忘記的話可以來這邊複習喔<ahref="https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81-%EF%BC%9APytorch-%E5%A6%82%E4%BD%95%E5%AF%A6%E9%A9%97-Backpropagation-%E4%B9%8B%E4%BB%80%E9%BA%BC%E6%98%AF-Backpropagation/">你所不知道的Pytorch 大補包(十)：Pytorch 如何實做出 Backpropagation 之什麼是Backpropagation</a></p><p>在裡面提到了網路中的函數非常複雜，複雜到我們沒辦法用一般多項式的方法來求解，所以我們將損失函數對權重做一階微分，得到網路的梯度。利用梯度下降法，一步步縮小Loss，像在山坡地上滑溜滑梯一樣，滑到最低點，就可以找到最接近真實答案的結果了。</p><p>Loss 損失函數的公式如下：給定資料 x 與權重 w，經過層層運算 f()得到結果後，再與標記 y 計算損失</p><p><span class="math display">\[\mathcal{L} = \mathcal{L}_{\mathrm{class}}(f(x,w),y)\]</span></p><p>梯度公式如下，在深度學習中我們稱這個符號 <spanclass="math inline">\(\nabla\)</span> ，代表梯度的意思</p><p><span class="math display">\[\nabla g = \frac{\partial\mathcal{L}}{\partial w}\]</span></p><p>而梯度下降法則是利用梯度的值來修改權重 <spanclass="math inline">\(w\)</span>，其中 <spanclass="math inline">\(w_t\)</span> 代表目前的權重，<spanclass="math inline">\(w_{t-1}\)</span> 代表上一次的權重，公式如下：</p><p><span class="math display">\[w_{t}=w_{t-1}-\nabla g\]</span></p><h2 id="什麼是優化器-optimizer">什麼是優化器 optimizer</h2><p>所謂優化器指「優化」網路做梯度下降的「速度」或「效果」，也就是說剛剛介紹的梯度下降其實還存在著許多的缺點，例如：收斂時間久、效果不穩定…等</p><p>而一個最最簡單概念的優化器(這個概念是我自己想的，有些人可能不這麼覺得…) 就是學習率 learningrate，符號通常表示 <span class="math inline">\(\eta\)</span></p><p>學習率設計用來控制梯度大小用，因為通常梯度算出來都很大，所以學習率會設介於0.1 ~ 0.0001的區間來縮小梯度計算結果，詳細可看第十章實驗，實驗結果可知如果不加學習率，梯度會超大，網路永遠都不可能會收斂</p><p>加入學習率的梯度下降公式如下：</p><p><span class="math display">\[w_{t}=w_{t-1}-\eta\nabla g\]</span></p><p>像這種找到梯度下降的缺點，並加以改進的方法，就可以稱作為一種優化器。</p><h2 id="sgd">SGD</h2><p>快速複習完梯度下降 (Gradient Desent, GD)後，緊接來介紹應用最廣、最穩定，也最元老的優化器：SGD</p><p>SGD 全名為 Stochastic Gradient Descent，中文稱：隨機梯度下降法</p><p>其實它跟剛剛上面我們介紹的加入學習率後的公式一模一樣，只是在「計算對象」及「方法」做了一點點的小修改，而這個故事中間有一點點關於歷史淵源，下面做簡單的介紹：</p><p>理論上的梯度下降會把<strong>全部</strong>的資料都看過一遍之後，用<strong>全部</strong>的資料去計算梯度，並更新一次參數。</p><p>但理想很豐滿；現實很骨感，在現實中我們的資料集又大又多，動輒幾 G甚至幾 T起跳的，實作上沒有辦法暫存下這麼多資料，然後再一次更新的，於是有人提出Mini-Batch GradientDesent，我們不看完整個資料集更新一次參數，而是設定一個 mini batch 的數值(其實就是現在說的 batch，可能以前的人覺得 256、512這些數字相對於全部的資料集來說，數字小了不少)，一個 mini batch就更新一次參數。</p><p>而為了增加網路複雜度，每次都會「隨機」取樣mini-batch，直到看完全部的資料集，使網路每一次看資料集的順序都不太一樣。這個隨機取樣的方法就稱作Stochastic Gradient Descent (SGD)，在現在來說，常說 SGD 指的就是mini-batch 的 SGD，在命名上有一點小小落差。</p><p>SGD的公式如下：基本上與上一章介紹的差不多，只有在計算損失函數中參與計算的資料集，只限定在一個batch 之中。</p><p><span class="math display">\[\begin{gather}w_{t+1}=w_t-\eta\nabla g \\\nabla g = \frac{\partial\mathcal{L}}{\partial w} \\\mathcal{L} =\mathcal{L}_{\mathrm{class}}(f(x_{\mathrm{batch}},w),y_{\mathrm{batch}})\end{gather}\]</span></p><p>在 Pyroch 中，torch.optim 提供了非常多的優化器選擇，要使用 SGD非常簡單，<ahref="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGDdocument 在這裡</a></p><p>只需要給定 learning 與 網路中的參數就可以了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><h2 id="momentum">Momentum</h2><p>SGD的優點從數學公式中可以看出來，就是穩定，每一次在重新參數的時候只會根據當前的梯度來計算要更新的權重值，一步一腳印的慢慢更新。而缺點與優點相同，慢慢更新的代價是網路收斂的速度慢了點。</p><p>因此有人提出 Momentum，在原本 SGD更新權重時除了考量當前的梯度外也會考量前一時刻的算出來的梯度，這個概念類似於物理動量的概念，在一個時間點上物體的速度等於目前當下的速度加上前幾個時刻累積的動量。</p><p>寫成數學公式的話如下：</p><p><span class="math display">\[\begin{gather}w_{t+1} = w_t-m_t \\m_t = \gamma m_{t-1} + \eta \nabla g\end{gather}\]</span></p><p>式子中的 m 是指前一刻計算出來的更新值，除了計算當前的 t的梯度外，也會考量到以往累積計算下來的m，每一次的更新值都會受到歷史的因素影響。另外加另一個超參數 <spanclass="math inline">\(\gamma\)</span>，可以自由控制網路受多少比例的動量控制</p><p>而 Momentum最大的好處就是更新速度快，在當前梯度與前一次動量方向相同下，每一次在更新時可以根據前一時刻的方向與數值，加成往下收斂的速度</p><p>而另一方面如果當前梯度與前一次動量方向相反的話，則可以使網路有離開Local minimum 與 plateau的能力，就像一顆從山上滑下來的球，如果遇到小山丘或是小平地，球會選擇保留以往的動量而繼續往同方向滑，甚至有機會越過小山丘繼續滑到Global minimum。</p><p>來看動畫會更清楚，下圖 git 來自 <ahref="https://julien-vitay.net/lecturenotes-neurocomputing/intro.html">Neurocomputing</a></p><p>可以更清楚的了解 SGD 與 Momentum 的差別</p><p><imgsrc="https://julien-vitay.net/lecturenotes-neurocomputing/_images/momentum-sgd.gif" /></p><p>而在 Pytorch 實作中，Momentum 歸類到與 SGD 同一個函式中，Momentum 為SGD 的一個參數，這個參數就是 Momentum 公式中的 <spanclass="math inline">\(\gamma\)</span> 超參數，設為 0 代表傳統 SGD，設為0 ~ 1 之間代表啟用 Momentum 並設定比例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h2 id="reference">Reference</h2><p><ahref="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">Pytorchoptimizer document</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在以前第九章中，有很 ~ 淺的列舉了一些優化器 optimizer，在第十二 ~
十四章中，會更詳細一點去介紹，這些 optimizer
的原理，以及當初提出是要改進什麼事？&lt;/p&gt;
&lt;p&gt;keywords:SGD、Momentum</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(十一)：Pytorch 如何實驗 Backpropagation 之 Pytorch AutoGrad 幫我們做了什麼事？</title>
    <link href="https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%B8%80-%EF%BC%9APytorch-%E5%A6%82%E4%BD%95%E5%AF%A6%E9%A9%97-Backpropagation-%E4%B9%8B-Pytorch-AutoGrad-%E5%B9%AB%E6%88%91%E5%80%91%E5%81%9A%E4%BA%86%E4%BB%80%E9%BA%BC%E4%BA%8B%EF%BC%9F/"/>
    <id>https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%B8%80-%EF%BC%9APytorch-%E5%A6%82%E4%BD%95%E5%AF%A6%E9%A9%97-Backpropagation-%E4%B9%8B-Pytorch-AutoGrad-%E5%B9%AB%E6%88%91%E5%80%91%E5%81%9A%E4%BA%86%E4%BB%80%E9%BA%BC%E4%BA%8B%EF%BC%9F/</id>
    <published>2022-12-28T16:41:11.000Z</published>
    <updated>2022-12-28T16:43:07.883Z</updated>
    
    <content type="html"><![CDATA[<p>本文接續著上一篇 [你所不知道的 Pytorch 大補包(十)：Pytorch 如何實做出Backpropagation 之什麼是 Backpropagation] 繼續更深入的了解 Pytorch的底層</p><p>keywords: AutoGrad <span id="more"></span></p><h2 id="pytorch-autograd-幫我們做了什麼事">pytorch AutoGrad幫我們做了什麼事？</h2><p>以上我們成功的用 numpy手刻了一個超~簡單的神經網路出來，並且訓練它，還取得了 100%正確率的成果，但剛剛 Foward 的函式很簡單，簡單到它的梯度甚至不用到 chainrule 就算得出來，如果今天是一個 100層深的網路，那我們的算式就會變得超長，跟本沒有辨法像剛剛直接用一條算式表達出來</p><p>這個時候幫我們實作 Backpropagation 的 pytorch 的派上用場了，pytorch使用 AutoGrad 自動的幫我們把所有梯度都算出來，並且也做完Backpropagation，在解釋一切程式碼之前，我們再來回頭看看用 pytorch寫出來的程式會多麼的簡潔</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義：輸入 x=2，目標 y=4，變量 w=0</span></span><br><span class="line">x = torch.tensor([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>], dtype=torch.float32)</span><br><span class="line">y = torch.tensor([<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 這個後面有 requires_grad 等等會介紹</span></span><br><span class="line">w = torch.tensor(<span class="number">0</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> w * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義使用 SGD 作用最佳化演算法</span></span><br><span class="line">optimizer = torch.optim.SGD([w], lr=lr)</span><br><span class="line"><span class="comment"># 定義 MSE Loss </span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># (1) Foward Pass 前傳導</span></span><br><span class="line">    y_hat = foward(x)</span><br><span class="line">    <span class="comment"># (1.5) 計算 Loss</span></span><br><span class="line">    l = loss(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (2, 3) 計算 Local Gradient 以及 Backpropagation</span></span><br><span class="line">    l.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (4) 更新權重 w</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># (4.1) 淨空 dw (Gradient) 值</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, w: <span class="subst">&#123;w:<span class="number">.3</span>f&#125;</span>, loss: <span class="subst">&#123;l:<span class="number">.8</span>f&#125;</span>, w.grad: <span class="subst">&#123;w.grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="什麼是-requires_grad">什麼是 requires_grad？</h3><p>一般在建立一個新的 tensor 時，我們會使用 <code>torch.tensor</code>來達成，但是如果 tensor想要實作自動計算梯度的話，我們必需在後面加一個參數 requires_grad</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], dtype=torch.float32)</span><br><span class="line"><span class="comment"># tensor([1., 2., 3., 4.])</span></span><br><span class="line"></span><br><span class="line">w = torch.tensor(<span class="number">0</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># tensor(0., requires_grad=True)    &lt;- 這裡多一個屬性</span></span><br></pre></td></tr></table></figure><p>打開這個 requires_grad 屬性後，pytorch會幫我們打開更多的屬性，而這些屬性是只有在做 Backpropagation時才會用到的，所以平常把它關起來減少記憶體的消耗。還記得前面有提到訓練網路的四大步驟嗎？等等會依照這四個步驟的順序來介紹</p><h3 id="foward-pass-前傳導">Foward Pass 前傳導</h3><p>前傳導其實就是由一堆運算式所組成，輸入一個值，經過這個複雜的運算式後得到一個結果就稱為Foward Pass 前傳導，那如果我們在前傳導的式子中加入 required_grad會發生什麼事情呢？以下用兩個程式來對比</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line">c = a * b</span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/NuB11Wn.png"alt="PyTorch Autograd-A 1.drawio" /></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Foward Pass</span></span><br><span class="line">c = a * b</span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/SvpuwfT.png"alt="PyTorch Autograd-A 2.drawio" /></p><p>可以由上圖分析出幾個重點：</p><ol type="1"><li>打開 requires_grad 後，tensor 的所有運算操作都會畫成一個 Graph</li><li>一個運算當中只要有一個變量 requires_grad=True，未來運算所新增的tensor 一樣會是 requires_grad=True</li><li>有三個新的屬性：grad、grad_fn、is_leaf</li></ol><ul><li><p>grad 值在前傳導時為 None，要等到做 Backpropagation時才會把值填上去</p></li><li><p>grad_fn 是在前傳導時 pytorch自動幫我們加上去的，意思是「對應運算符微分後的算式」pytorch提供了一大堆的 grad_fn 以應付各種微分運算，加速 Backpropagation的流程</p></li><li><p>is_leaf為了要表示這個權重節點是不是在葉節點上，為什麼這個這麼重要呢？因為<strong>pytorch 只會在葉結點上儲存 grad資訊</strong>，目的是為了保留記憶體</p></li></ul><h3 id="backpropagation">Backpropagation</h3><p>前傳導完，記錄了許多數值後，接著就利用這個數值來做Backpropagation，程式以及對應的 Graph 如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Forward Pass </span></span><br><span class="line">c = a * b</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">c.backward()</span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/YlfmW2o.png"alt="PyTorch Autograd-A 4.drawio" /></p><p>可以看到，簡簡單單的一行 <code>c.backward()</code> pytorch竟然幫我們做了這麼多事情…。宏觀上來看這一行指令幫我們算出來 a 的 grad 值3，微觀上來看這一行指令幫我們畫了超多的圖…同時也吃掉了不少記憶體</p><p>首先當呼叫 <code>c.backward()</code> 時，pytroch 會先去尋找grad_fn，接著根據 grad_fn 裡面的微分運算計算 grad，然後放進AccumulateGrad，累計不同次運算的 grad，最後再放到對應節點權重的 grad屬性中</p><p>以數字的例子為：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">假設一開始初始值為 1.0 -&gt;</span><br><span class="line">找到 grad_fn 為 MulBackward -&gt;</span><br><span class="line">計算 dc/da 的偏微分 (dc/db 因為 requires_grad=False 所以不參與計算) -&gt;</span><br><span class="line">dc/da = d(a*b)/da = b = 3.0 -&gt;</span><br><span class="line">用一個累計暫存器存起來 -&gt;</span><br><span class="line">放到 a.grad 中</span><br></pre></td></tr></table></figure><p>再用一個更複雜的例子來舉例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假設 pytorch 的運算變成這個樣子：</span></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Foward Pass </span></span><br><span class="line">c = a * b</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 再定義一個 d</span></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再多一個 Foward Pass</span></span><br><span class="line">e = c * d</span><br><span class="line"></span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">e.backward()</span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/2DCyMTA.png"alt="PyTorch Autograd-Simple 5.drawio" /></p><p>覺得圖片變太複雜嗎 XD，沒關系我們一起從最下面慢慢算上去：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">假設一開始初始值為 1.0 -&gt;</span><br><span class="line">找到 grad_fn 為 MulBackward -&gt;</span><br><span class="line">計算 de/dc、de/dd 的偏微分 -&gt;</span><br><span class="line">de/dc = d(c*d)/dc = d = 6.0 -&gt;</span><br><span class="line">de/dd = d(c*d)/dd = c = 4.0 -&gt;</span><br><span class="line"></span><br><span class="line">de/dc 因為 c 不是葉結點，所以不用寫回 c.grad，直接把值傳給 c.grad_fn，繼續做下一個偏微分 -&gt;</span><br><span class="line">de/dd 因為 d 是葉結點，用一個累計暫存器存起來，再把結果寫回 d.grad -&gt;</span><br><span class="line"></span><br><span class="line">計算 dc/da 的偏微分 (dc/db 因為 requires_grad=False 所以不參與計算) -&gt;</span><br><span class="line">dc/da = d(a*b)/da = b = 3.0 -&gt;</span><br><span class="line">再乘上傳進來的 4，4*3 = 12</span><br><span class="line">用一個累計暫存器存起來 -&gt;</span><br><span class="line">放到 a.grad 中</span><br></pre></td></tr></table></figure><p>我們可以印印看結果是不是正如我們所計算的？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假設 pytorch 的運算變成這個樣子：</span></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Foward Pass </span></span><br><span class="line">c = a * b</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 再定義一個 d</span></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再多一個 Foward Pass</span></span><br><span class="line">e = c * d</span><br><span class="line"></span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">e.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line"><span class="built_in">print</span>(c.grad)</span><br><span class="line"><span class="built_in">print</span>(d.grad)</span><br><span class="line"><span class="built_in">print</span>(e.grad)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a.grad -&gt; 12.0</span><br><span class="line">/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won&#x27;t be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352464346/work/build/aten/src/ATen/core/TensorBody.h:477.)</span><br><span class="line">  return self._grad</span><br><span class="line">c.grad -&gt; None</span><br><span class="line">d.grad -&gt; 6.0</span><br><span class="line">e.grad -&gt; None</span><br></pre></td></tr></table></figure><p>咦…怎麼跟想像中的答案不一樣…，<code>a.grad</code> <code>d.grad</code>都是對的，<code>c.grad</code> <code>e.grad</code>發生了什麼事…？其實剛剛也有提到，在 pytorch 中，<strong>只有葉結點(is_leaf=True) 才會把 .grad存起來</strong>，目的是為了節省不必要的記憶體，而且在圖中也可看到，資料流動flow 如果不是指向葉結點，會直接把值放到下一個 grad_fn 中，不會有AccumulateGrad 把值存到 .grad 中，因此 pytorch才會跳提醒說這個操作不合理，並且回傳 None</p><p>那如果我們真的真的想要得到不是葉結點的 .grad值呢？那我們就要在<strong>前傳導的時候先把它註冊下來</strong>，使用<code>retain_grad()</code> 這個函式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假設 pytorch 的運算變成這個樣子：</span></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Foward Pass </span></span><br><span class="line">c = a * b</span><br><span class="line"><span class="comment"># 用 .retain_grad() 來註冊，告訴 pytorch 要把這個值存起來</span></span><br><span class="line">c.retain_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再定義一個 d</span></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再多一個 Foward Pass</span></span><br><span class="line">e = c * d</span><br><span class="line"><span class="comment"># 用 .retain_grad() 來註冊，告訴 pytorch 要把這個值存起來</span></span><br><span class="line">e.retain_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">e.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;a.grad -&gt; <span class="subst">&#123;a.grad&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;c.grad -&gt; <span class="subst">&#123;c.grad&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;d.grad -&gt; <span class="subst">&#123;d.grad&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;e.grad -&gt; <span class="subst">&#123;e.grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>詳細 <code>.retain_grad()</code>的說明可以看下面這個影片，裡面很詳細的介紹為什麼這樣就可以，以及一個新概念：hook的用法：<ahref="https://www.youtube.com/watch?v=syLFCVYua6Q">https://www.youtube.com/watch?v=syLFCVYua6Q</a></p><p>另外也可以發現，在圖中有一個 AccumulateGrad的方塊，這是用來儲存每一次 Backpropagation 的結果，並把新算出來的 grad與之前的相加存到 .grad 中間，也就是說它不會自動淨空！</p><p>所以通常在程式中我們會手動淨空 .grad 值，以確保每一次訓練時 .grad都是最新的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># (1) Foward Pass 前傳導</span></span><br><span class="line">    y_hat = foward(x)</span><br><span class="line">    <span class="comment"># (1.5) 計算 Loss</span></span><br><span class="line">    l = loss(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (2, 3) 計算 Local Gradient 以及 Backpropagation</span></span><br><span class="line">    l.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (4) 更新權重 w</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># (4.1) 淨空 dw (Gradient) 值    &lt;- 如果不清空 .grad 會累加！</span></span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure><p>至於為什麼要這樣設計，因為如果我們使用的設備記憶體不足，沒辨法一次結太多資料訓練，我們就可以使用Gradient accumulation 的技巧，改成每訓練兩次更新一次參數，變向放大 Batchsize</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i,(image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="comment"># 1. input output</span></span><br><span class="line">    pred = model(image)</span><br><span class="line">    loss = criterion(pred, label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.1 loss 要除以累積的總步數，正規化 loss 的值</span></span><br><span class="line">    loss = loss / accumulation_steps  </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 2.2 計算梯度的值</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 當累積的步數到一定的程度後，梯度中的值也會不斷累加，才會更新網路的參數</span></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % accumulation_steps == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># optimizer the net</span></span><br><span class="line">        optimizer.step()        <span class="comment"># 更新網路參數</span></span><br><span class="line">        optimizer.zero_grad()   <span class="comment"># 清空以前的梯度</span></span><br></pre></td></tr></table></figure><h3 id="要怎麼去掉-requires_grad">要怎麼去掉 requires_grad？</h3><p>從上面的圖來看，只要我們在 tensor 中加入 require_grad 參數，pytorch就會一直記錄追縱未來所有的運算，一直更新那一張大圖，記憶體開銷非常可觀，那我們要怎麼樣把圖中藍藍的那一堆Backpropagation 專用的圖給去掉呢？一共有三個做法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (1) x.requires_grad_(False)</span></span><br><span class="line"><span class="comment"># 這個程式可以 inplace 把 x 的 requires_grad 拿掉</span></span><br><span class="line"><span class="comment"># 在 pytorch 中，所有 xxx_ &lt;- 這個底線的意思代表 inplace 操作的意思，不會回傳任何值</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor(3., requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">x.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor(3.)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (2) x.detach()</span></span><br><span class="line"><span class="comment"># 這個程式一樣可以把 .requires_grad 去掉</span></span><br><span class="line"><span class="comment"># 但是它會建新一個新的且不帶 requires_grad 的 tensor，並回傳</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor(3., requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">y = x.detach()</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(3.)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (3) torch.no_grad()</span></span><br><span class="line"><span class="comment"># 會搭配 with 一起使用，也是最常使用的一個方法</span></span><br><span class="line"><span class="comment"># 在 with 的縮排範圍內，任何 tensor 都不帶 .requires_grad</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor(3., requires_grad=True)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(5.)</span></span><br></pre></td></tr></table></figure><p>最後一個 <code>torch.no_grad()</code>最常看到，通常會在驗證、測試的程式碼中會出現，理由有兩個。</p><ul><li>一、結省記憶體。有時候 nvidia 會噴記憶體不夠，不一定是 Batch size設太大的問題，也有可能是驗證、測試時忘記寫到<code>torch.no_grad()</code> 把那一大堆 Backpropagation的圖都載入了</li><li>二、不會更新參數。因為 <code>torch.no_grad()</code> 把所有Backpropagation剛掉了，排除了所有會更新到參數的因素，因此可以放心的驗證、測試，而不用擔心動到網路的參數</li></ul><p>以上就是全部的內容了！希望看完這篇文章可以更了解 pytorch倒底背後幫我們做了什麼事情！</p><h3 id="reference">Reference</h3><p>本篇文章大量參考了以下兩個 youtube：</p><p><a href="https://www.youtube.com/watch?v=DbeIqrwb_dE">(系列教學影片)PyTorch Tutorial 03 - Gradient Calculation With Autograd</a></p><p><a href="https://www.youtube.com/watch?v=3Kb0QS6z7WA">(系列教學影片)PyTorch Tutorial 04 - Backpropagation - Theory With Example</a></p><p><a href="https://www.youtube.com/watch?v=E-I2DNVzQLg">(系列教學影片)PyTorch Tutorial 05 - Gradient Descent with Autograd andBackpropagation</a></p><p><a href="https://www.youtube.com/watch?v=MswxJw-8PvE">PyTorchAutograd Explained - In-depth Tutorial</a></p><p>官方 Document 永遠是你最好的朋友</p><p><ahref="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">AGENTLE INTRODUCTION TO <code>TORCH.AUTOGRAD</code></a></p><p><a href="https://ithelp.ithome.com.tw/articles/10216440">(iT邦幫忙)Day 2 動態計算圖：PyTorch's autograd (裡面有提到 in-place的問題)</a></p><p><ahref="https://blog.csdn.net/weixin_41417982/article/details/81393917">(Backpropagation參考文章) 神经网络的传播（权重更新）</a></p><p><ahref="https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94">(為什麼我看不到.grad？) Why cant I see .grad of an intermediate variable?</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文接續著上一篇 [你所不知道的 Pytorch 大補包(十)：Pytorch 如何實做出
Backpropagation 之什麼是 Backpropagation] 繼續更深入的了解 Pytorch
的底層&lt;/p&gt;
&lt;p&gt;keywords: AutoGrad</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(十)：Pytorch 如何實做出 Backpropagation 之什麼是 Backpropagation</title>
    <link href="https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81-%EF%BC%9APytorch-%E5%A6%82%E4%BD%95%E5%AF%A6%E9%A9%97-Backpropagation-%E4%B9%8B%E4%BB%80%E9%BA%BC%E6%98%AF-Backpropagation/"/>
    <id>https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81-%EF%BC%9APytorch-%E5%A6%82%E4%BD%95%E5%AF%A6%E9%A9%97-Backpropagation-%E4%B9%8B%E4%BB%80%E9%BA%BC%E6%98%AF-Backpropagation/</id>
    <published>2022-12-28T16:38:57.000Z</published>
    <updated>2022-12-28T16:40:20.577Z</updated>
    
    <content type="html"><![CDATA[<p>常常我們初學 pytroch 的時候都一定會看過下面的程式碼：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">  output = model(dataset)</span><br><span class="line">  loss = criterion(output, target)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># wtf</span></span><br><span class="line">  optimizer = zero_grad()</span><br><span class="line">  loss.backward()</span><br><span class="line">  optimizer.step()</span><br></pre></td></tr></table></figure><p>好不容易跨出第一步，並剛接觸程式碼的你，一看到這坨鬼東西一定心裡有三個問號…(至少我是這樣啦哈哈。</p><p>keywords: Backpropagation <span id="more"></span></p><p>而大部份網路上的教學都會強調：這個就是 Backpropagation喔！也不用太了解它，知道在寫程式時記得要加上它就好了！…</p><p>更進階一點，你是從學校修神經網路相關的課程，也知道 Backpropagation背後的數學原理，甚至還用 mathlab python 手刻了一個陽春Backpropagation，只是當你轉換到 pytorch上來看到程式碼時，不禁覺得…這程式也太簡潔了吧…，只要一行<code>loss.backward()</code> 就可以了，這真的可靠嗎？</p><p>而這篇文章就會從最一開始的脈絡，來慢慢解釋：什麼是Backpropagation、要怎麼用程式來實作 Backpropagation、pytorch倒底幫我們做了什麼？不管你是初心者或是小有經驗的開發者，這些底層冷知識可以幫助你加深對pytorch 的感情喔！</p><h3 id="什麼是-backpropagation">什麼是 Backpropagation？</h3><p>在理解什麼是 Backpropagation之前，先來複習一下訓練一個神經網路一定要經過的四個步驟：</p><ol type="1"><li>Forward Pass 前傳導</li><li>Calculate Gradient 計算梯度</li><li>Backpropagation 後傳導</li><li>Weight update 權重更新</li></ol><p>用一個非常非常簡單的例子來舉例，假設我們要訓練一個可以把輸入資料都都x2 的網路，並且定義以下參數</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># x 定義為輸入資料 = 2</span></span><br><span class="line">x = torch.tensor(<span class="number">2</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y 定義為目標 Ground Truth </span></span><br><span class="line"><span class="comment"># y = x*2 = 2*2 = 4</span></span><br><span class="line">x = torch.tensor(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># w 定義為網路中的一個權重值，初始為 0</span></span><br><span class="line">w = torch.tensor(<span class="number">0</span>, dtype=torch.float32)</span><br></pre></td></tr></table></figure><p>把上面的文字及變數定義換成更白話一點的說法就是：我們今天有一個式子<code>w * x = y</code> 要找到一個適合的 w 值，使得<code>2x = y</code>。</p><p>畫成樹狀圖可以長成如下：</p><p><img src="https://i.imgur.com/Y7vRHHO.png"alt="未命名绘图.drawio" /></p><p>當然在現階段我們可以清楚的一看就知道 <code>w = 2</code>就是答案了，只是在一般的深度學習中，x的多項式可是會達到幾千甚至幾萬的維度，跟本不能用多項式求解的方式來知道答案。那該怎麼辨呢？就使用漸近求解的方式吧！因此才會多一個計算Loss 的步驟，Loss可以得知，網路輸出的結果與真實的結果倒底相差多遠，透過這個相差多遠的資訊，可以進一步得知網路是否有在往正確的方向學習。</p><p>我們可以把最後的結果套上 Mean Square Error(均方誤差)，就是一個相減後平方的公式：<spanclass="math inline">\(\mathcal{L}=(\hat y -y)^2\)</span></p><p>例如當 <code>w = 1</code> 時，我們算出來的 Loss 為 <spanclass="math inline">\((1-2)^2=1\)</span>，可解讀為我們離正確解答的距離還有1 (單位)，而隨著 w 的數值越來越接近 2，Loss 也會越來越小，直到趨近於0。</p><p>因為我們在網路中加入 Loss，對應的運算樹狀圖也要修改如下：</p><p><img src="https://i.imgur.com/vY3QdQ8.png"alt="未命名绘图.drawio" /></p><p>接著我們實際把 xyw 輸入到網路中，並經過一串多項式運算，如同下圖求出Loss 為 16，這個步驟就是 Forward Pass 前傳導</p><p><img src="https://i.imgur.com/ixuxvfW.png"alt="未命名绘图.drawio" /></p><hr /><p>那計算出來的 Loss 是要做什麼的呢？其實這個 Loss除了看網路訓練的好不好之外，還可以用來計算梯度並更新每個節點上的權重。</p><p>什麼是梯度呢？高中數學我們會學到，在一個二維曲線上畫一條切線，就代表它的斜率；如果是在物理上，在v-t圖的一個時間點上找切線斜率，則是代表瞬時速度。只是在深度學習中，我們習慣稱為梯度，英文為Gradient，數學符號為 <span class="math inline">\(\nabla\)</span></p><p>那梯度在深度學習中代表的函意又是什麼呢？代表在多維的空間中，某一點的斜率。以三維空間為例子，三維空間就是一個大曲面，這個曲面有凹有凸，就像一個山脈，一個山脈有山頂、有山谷、有平原、有懸崖…，而梯度類比山脈的例子就相當於是當下等高線的坡度</p><p><img src="https://i.imgur.com/aDZmUmo.jpg" alt="image-20220901184131815" style="zoom: 50%;" /></p><p>那我們算梯度要做什麼…？還記得前面我們有說過 Loss的值是要…越小越好對吧？代表網路預測的結果跟真實的結果距離越近，我們要怎麼知道如何修改w 值才可以使得 Loss最小？這個問句可以用山脈的例子同等於：我們怎麼走才可以下山？甚至也可以說：我們怎麼走才可以最快的到山下？</p><p>答案當然是用滑的阿，有爬過山的都知道上山易下山難，下山時多希望自己有個鋼鐵屁屁可以一口氣滑下山XD，而深度學習也是利用一模一樣的方法：了解哪裡坡度/梯度最大，就可以快速的滑下山，取得最小的Loss，同時也取得預測效果最準的結果</p><hr /><p>而要求得梯度只有一種辨法：微分，更詳細的說是偏微分，我們最想要了解最後網路的<strong>Loss與變量 w 之間的梯度關系</strong>，因此只要求得 loss 對 w的偏微分，就可以知道 w 要怎麼調整效果會最好了，公式如下： <spanclass="math display">\[\nabla g = \frac{d\,\mathrm{loss}}{dw}\]</span> 但仔細看會發現…上面這個式跟本算不出來阿，為什麼呢？如果我們把loss 解壓縮的話： <span class="math display">\[\nabla g=\frac{d\,\mathrm{loss}}{dw}=\frac{d(\hat y-y)}{dw}\]</span> loss 中間完全沒有 w 變量阿，倒底要怎麼對 w做偏微分呢？我們可以先停下腳步來別想要一簇登天直接求得對 w做偏微分，我們可以先算出 LocalGradient，也就是每一個權重先對自己的變量求梯度 (loss 先對 s、s 先對<span class="math inline">\(\hat y\)</span>、<spanclass="math inline">\(\hat y\)</span> 先對 w)： <spanclass="math display">\[\frac{d\,\mathrm{loss}}{ds} = \frac{ds^2}{ds}=2s\\\frac{ds}{d\hat y}=\frac{d(\hat y-y)}{d\hat y}=1\\\frac{d\hat y}{dw}=\frac{d(x\cdot w)}{dw}=x\]</span> 更詳細如下圖：</p><p><img src="https://i.imgur.com/tOz7cAL.png" alt="test" /></p><p>再仔細看看上面的 LocalGradient，咦…好像怎麼有規律？！這不就是傳說中的 chain rule 嗎？ <spanclass="math display">\[\nablag=\frac{d\,\mathrm{loss}}{dw}=\frac{d\,\mathrm{loss}}{ds}\frac{ds}{d\haty}\frac{d\hat y}{dw}\]</span> 也就是說當我們在做 Local Gradient的時候，其實就是在幫我們最感興趣的<strong>loss 對 w的偏微分</strong>在計算它的 chain rule，而這個透過 chain rule一層一層慢慢的找到值的方式，就是 Backpropagation <spanclass="math display">\[\nablag=\frac{d\,\mathrm{loss}}{dw}=\frac{d\,\mathrm{loss}}{ds}\frac{ds}{d\haty}\frac{d\hat y}{dw}=2s\cdot 1 \cdot x=-16\]</span> <img src="https://i.imgur.com/bh1nSlm.png" alt="test" /></p><p>接著我們再將算出來 Backpropagation的值，簡單的利用以下的公式去更新每一個權重，其中 <spanclass="math inline">\(\nabla g\)</span> 為 Backpropagation 的結果、<spanclass="math inline">\(\eta\)</span> 為 learning rate 控制大小用： <spanclass="math display">\[w_{t+1}=w_t-\eta\nabla g\]</span></p><h3 id="要怎麼用程式來實作-backpropagation">要怎麼用程式來實作Backpropagation？</h3><p>以下的例子會回歸最原本的初心，不用高級的 pytorch 工具，而是使用 numpy來達成以上四個基本操作</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義：輸入 x=2，目標 y=4，變量 w=0</span></span><br><span class="line">x = np.array([<span class="number">2</span>], dtype=np.float32)</span><br><span class="line">y = np.array([<span class="number">4</span>], dtype=np.float32)</span><br><span class="line">w = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義網路前傳導的式子</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 MSE 均方差來做為 Loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> ((y_hat - y)**<span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 計算 loss 對變數 w 的偏微分</span></span><br><span class="line"><span class="comment"># 這是是直接把式子展開，直接計算偏微分 (沒有用到 chain rule 的概念)</span></span><br><span class="line"><span class="comment"># dloss/dw = d(w*x - y)^2/dw = 2x (w*x - y)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x, y, y_hat</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(<span class="number">2</span>*x, y_hat-y).mean()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 開始訓練</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Start Training...&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># (1) Foward Pass 前傳導</span></span><br><span class="line">    y_hat = forward(x)</span><br><span class="line">    <span class="comment"># (1.5) 計算 Loss</span></span><br><span class="line">    l = loss(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (2, 3) 計算 Local Gradient 以及 Backpropagation</span></span><br><span class="line">    dw = gradient(x, y, y_hat)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (4) 更新權重 w</span></span><br><span class="line">    w -= lr * dw</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;epoch&#125;</span>, loss: <span class="subst">&#123;l:<span class="number">.8</span>f&#125;</span>, w: <span class="subst">&#123;w:<span class="number">.3</span>f&#125;</span>, dw: <span class="subst">&#123;dw:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 印出來的結果</span><br><span class="line">Start Training...</span><br><span class="line">epoch: 1, loss: 16.00000000, w: 0.160, dw: -16.000</span><br><span class="line">epoch: 2, loss: 13.54240036, w: 0.307, dw: -14.720</span><br><span class="line">epoch: 3, loss: 11.46228790, w: 0.443, dw: -13.542</span><br><span class="line">epoch: 4, loss: 9.70168018, w: 0.567, dw: -12.459</span><br><span class="line">epoch: 5, loss: 8.21150303, w: 0.682, dw: -11.462</span><br><span class="line">epoch: 6, loss: 6.95021534, w: 0.787, dw: -10.545</span><br><span class="line">epoch: 7, loss: 5.88266134, w: 0.884, dw: -9.702</span><br><span class="line">epoch: 8, loss: 4.97908545, w: 0.974, dw: -8.926</span><br><span class="line">epoch: 9, loss: 4.21429777, w: 1.056, dw: -8.212</span><br><span class="line">epoch: 10, loss: 3.56698155, w: 1.131, dw: -7.555  &lt;- w=1.131</span><br></pre></td></tr></table></figure><p>從印出來的結果可以看到，在 epoch=1 的時候，dw也就是我們剛剛算出來的值 -16 是完全正確的，接著 dw 值會往 0 靠近，而Loss 的數值也慢慢降低，以及 w 的值，從 0 慢慢的往答案 2 靠近</p><p>但可能會覺得這個訓練效果也太不好了吧…搞了這麼多數學的東西，結果訓練出來的w 竟然離 2 還很遠！沒關系！這個網路還有很多可以優化的地方：像是增加epoch 數量、或是增加資料集，都可以使網路效果變更好喔！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定義：輸入 x，目標 y，新增資料集至 4 組</span></span><br><span class="line">x = np.array([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>], dtype=np.float32)</span><br><span class="line">y = np.array([<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>], dtype=np.float32)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># 因為資料不只一組，由於 Gradient 不能是一個 Vector (向量) 必需要是一個 Scalar (純量)，所以要取 mean 平均</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x, y, y_hat</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(<span class="number">2</span>*x, y_hat-y).mean()  &lt;- 這裡 mean 的作用</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Start Training...</span><br><span class="line">epoch: 1, loss: 30.00000000, w: 1.200, dw: -120.000</span><br><span class="line">epoch: 2, loss: 4.79999924, w: 1.680, dw: -48.000</span><br><span class="line">epoch: 3, loss: 0.76800019, w: 1.872, dw: -19.200</span><br><span class="line">epoch: 4, loss: 0.12288000, w: 1.949, dw: -7.680</span><br><span class="line">epoch: 5, loss: 0.01966083, w: 1.980, dw: -3.072</span><br><span class="line">epoch: 6, loss: 0.00314570, w: 1.992, dw: -1.229</span><br><span class="line">epoch: 7, loss: 0.00050332, w: 1.997, dw: -0.492</span><br><span class="line">epoch: 8, loss: 0.00008053, w: 1.999, dw: -0.197</span><br><span class="line">epoch: 9, loss: 0.00001288, w: 1.999, dw: -0.079</span><br><span class="line">epoch: 10, loss: 0.00000206, w: 2.000, dw: -0.031  &lt;- YA, w=2 了！</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;常常我們初學 pytroch 的時候都一定會看過下面的程式碼：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;for&lt;/span&gt; epoch &lt;span class=&quot;keyword&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;range&lt;/span&gt;(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;, epochs+&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;):&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  output = model(dataset)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  loss = criterion(output, target)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &lt;span class=&quot;comment&quot;&gt;# wtf&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  optimizer = zero_grad()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  loss.backward()&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  optimizer.step()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;好不容易跨出第一步，並剛接觸程式碼的你，一看到這坨鬼東西一定心裡有三個問號…(至少我是這樣啦哈哈。&lt;/p&gt;
&lt;p&gt;keywords: Backpropagation</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(九)：一些 optimizer 整理</title>
    <link href="https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B9%9D-%EF%BC%9A%E4%B8%80%E4%BA%9B-optimizer-%E6%95%B4%E7%90%86/"/>
    <id>https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B9%9D-%EF%BC%9A%E4%B8%80%E4%BA%9B-optimizer-%E6%95%B4%E7%90%86/</id>
    <published>2022-12-28T16:35:50.000Z</published>
    <updated>2022-12-28T16:37:18.922Z</updated>
    
    <content type="html"><![CDATA[<p>本篇筆記主要參考以下網路文章：<ahref="https://zhuanlan.zhihu.com/p/22252270">https://zhuanlan.zhihu.com/p/22252270</a></p><p>整理了一些常用 optimizer 的數學原理，及其重點特色</p><p>keywords: optimizer <span id="more"></span></p><h2 id="sgd">SGD</h2><ul><li>stochastic gradient descent</li></ul><p><span class="math display">\[g_t = \nabla_{\theta_{t-1}}f(\theta_{t-1}) \\\Delta\theta_t = - \eta * g_t\]</span></p><ul><li><span class="math inline">\(\eta\)</span> 是 learning rate</li><li>SGD 完全依賴目前梯度的斜率大小</li><li>遇到鞍點等地方會不容易達到最優</li><li>且 SGD 整體更新速度慢</li></ul><h2 id="momentum">Momentum</h2><ul><li>模仿物理中的動量</li><li>把之前算出來的梯度大小一起放到這一次的運算</li></ul><p><span class="math display">\[m_t = \mu*m_{t-1} + g_t \\\Delta\theta_t = -\eta * m_t\]</span></p><ul><li>相較於 SGD 更新速度快</li><li>在梯度改變方向的時候，<span class="math inline">\(\mu\)</span>可以減少更新，抑制振盪</li></ul><h2 id="nesterov">Nesterov</h2><ul><li>nesterov 在梯度更新時做一個校正，避免前進太快，同時提高靈敏度，與momentum 有點像</li><li>由公式可以看出 momentum 沒有更改當前梯度 <spanclass="math inline">\(g_t\)</span></li><li>於是在 Nesterov 中就是透過修改 <spanclass="math inline">\(g_t\)</span> 來達到修改的目的</li></ul><p><span class="math display">\[g_t = \nabla_{\theta_{t-1}}f(\theta_{t-1}-\eta*\mu*m_{t-1}) \\m_t = \mu * m_{t-1} + g_t \\\Delta\theta_t = -\eta*m_t\]</span></p><p>雖然 momentum nesterov都是為了增加梯度更新時的彈性，但人工設定還不如用機器自己來學習</p><p>以下介紹機器自己學習的方法</p><h2 id="adagrad">Adagrad</h2><ul><li>是對 learning rate 設定了一項限制</li><li><span class="math inline">\(\epsilon\)</span> 用來保證分非 0</li><li>把 <span class="math inline">\(\eta\)</span> 除上一個值使得</li><li>前期 <span class="math inline">\(g_t\)</span>較小的時候，regularizer 比較大，能夠放大梯度</li><li>後期 <span class="math inline">\(g_t\)</span>較大的時候，regularizer 比較小，能夠約束梯度</li><li>缺點：</li><li>仍要人工設定 learning rate</li><li><span class="math inline">\(\eta\)</span> 設太大的話，會讓regularizer 過於敏感，對梯度改變太大</li></ul><p><span class="math display">\[n_t = n_{t-1} + g^2\\\Delta\theta_t = -\frac{\eta}{\sqrt{n_t+\epsilon}}*g_t\]</span></p><h2 id="adadelta">Adadelta</h2><ul><li>是 Adagrad 的進階版</li><li>只累加固定大小的項</li></ul><p><span class="math display">\[n_t = v*n_{t-1} + (1-v) *g^2_t \\\Delta\theta_t = -\frac{\eta}{\sqrt{n_t+\epsilon}} * g_t\]</span></p><ul><li>在經過作者一系列，近似牛頓迭代法的方法後</li><li>可以實現機器自動學習 learning rate</li></ul><h2 id="rmsprop">RMSprop</h2><ul><li>算是 Adadelta 的變形</li></ul><p><span class="math display">\[E|g^2|_t = \rho * E|g^2|_t-1 + (1 - \rho) * g^2_t \\RMS|g|_t = \sqrt{E|g^2|_t + \epsilon} \\\Delta\theta_t = -\frac{\eta}{RMS|g|_t} * g_t\]</span></p><ul><li>RMS 均方根，作為 learning rate 的約束</li><li>仍然是人工固定的 learning rate</li></ul><h2 id="adam">Adam</h2><ul><li>就是帶有 Momentum 的 RMSprop</li><li>因為 m n 是變數，所以梯度可以動態調整</li></ul><p><span class="math display">\[m_t = \mu * m_{t-1} + (1-\mu)*g_t \\n_t = \mu * n_{t-1} + (1-v)*g_t \\\hat{m}_t = \frac{m_t}{1-\mu^t} \\\hat{n}_t = \frac{n_t}{1-v^t} \\\Delta\theta_t = -\frac{\hat{m}_t}{\sqrt{\hat{n}_t} + \epsilon} * \eta\]</span></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本篇筆記主要參考以下網路文章：&lt;a
href=&quot;https://zhuanlan.zhihu.com/p/22252270&quot;&gt;https://zhuanlan.zhihu.com/p/22252270&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;整理了一些常用 optimizer 的數學原理，及其重點特色&lt;/p&gt;
&lt;p&gt;keywords: optimizer</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(八)：訓練小技巧 DDP 透過多機多卡來訓練模型</title>
    <link href="https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%85%AB-%EF%BC%9A%E8%A8%93%E7%B7%B4%E5%B0%8F%E6%8A%80%E5%B7%A7-DDP-%E9%80%8F%E9%81%8E%E5%A4%9A%E6%A9%9F%E5%A4%9A%E5%8D%A1%E4%BE%86%E8%A8%93%E7%B7%B4%E6%A8%A1%E5%9E%8B/"/>
    <id>https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%85%AB-%EF%BC%9A%E8%A8%93%E7%B7%B4%E5%B0%8F%E6%8A%80%E5%B7%A7-DDP-%E9%80%8F%E9%81%8E%E5%A4%9A%E6%A9%9F%E5%A4%9A%E5%8D%A1%E4%BE%86%E8%A8%93%E7%B7%B4%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-12-28T16:33:39.000Z</published>
    <updated>2022-12-28T16:34:45.668Z</updated>
    
    <content type="html"><![CDATA[<p>DDP 的全文是 Distributed DataParallel，是一種可以透過多機多卡來訓練模型的一種方法，它的本質上就是一個像Map-Reduce 的東西，把訓練資料、Gradient、Loss 等資訊平均分配給每一個GPU，達成多工處理的目的</p><p>DDP 也可以就看成，提高 batch-size 來提高網路效果</p><p>下面我們直接先來看 code 吧：</p><p>keywords: DDP <span id="more"></span></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">################</span></span><br><span class="line"><span class="comment">## main.py文件</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 使用 DDP 最主要 import 的兩個包</span></span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"><span class="comment">### 1. 網路架構 (Module) ### </span></span><br><span class="line"><span class="comment"># 隨便設計的模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToyModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ToyModel, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 假設會用到的資料集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span>():</span></span><br><span class="line">    transform = torchvision.transforms.Compose([</span><br><span class="line">        torchvision.transforms.ToTensor(),</span><br><span class="line">        torchvision.transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">    ])</span><br><span class="line">    my_trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, </span><br><span class="line">        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    <span class="comment"># 我們要給 DataLoader 提供 DPP 的 sampler，使用下面的程式實現</span></span><br><span class="line">    train_sampler = torch.utils.data.distributed.DistributedSampler(my_trainset)</span><br><span class="line">    <span class="comment"># 在 DataLoader 中加入 sampler</span></span><br><span class="line">    <span class="comment"># 這裡的 batch_size 指的是一個 rank 中 (一個程序) 的 batch_size</span></span><br><span class="line">    <span class="comment"># 也就是說總 batch_size 是 batch_size x world_size (總程序數量)</span></span><br><span class="line">    trainloader = torch.utils.data.DataLoader(my_trainset, </span><br><span class="line">        batch_size=<span class="number">16</span>, num_workers=<span class="number">2</span>, sampler=train_sampler)</span><br><span class="line">    <span class="keyword">return</span> trainloader</span><br><span class="line">    </span><br><span class="line"><span class="comment">### 2. 初始化模型、數據、各種配置  ####</span></span><br><span class="line"><span class="comment"># 要從外面手動新增 local_rank 參數</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--local_rank&quot;</span>, default=-<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">FLAGS = parser.parse_args()</span><br><span class="line">local_rank = FLAGS.local_rank</span><br><span class="line"></span><br><span class="line"><span class="comment"># DDP backend 初使化</span></span><br><span class="line">torch.cuda.set_device(local_rank)</span><br><span class="line">dist.init_process_group(backend=<span class="string">&#x27;nccl&#x27;</span>)  <span class="comment"># nccl 由 Nvidia 用 C++ 寫的 Map-Reduce 後端</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 準備資料集</span></span><br><span class="line">trainloader = get_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line">model = ToyModel().to(local_rank)</span><br><span class="line"><span class="comment"># 要 Load 預訓練的模型，需要在建立 DDP 模型之前，且只需要在 rank=0 (主要程序) 上 Load 就可以了</span></span><br><span class="line">ckpt_path = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> dist.get_rank() == <span class="number">0</span> <span class="keyword">and</span> ckpt_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    model.load_state_dict(torch.load(ckpt_path))</span><br><span class="line"><span class="comment"># 建立 DDP 模型 (這一句是精隨 XD)</span></span><br><span class="line">model = DDP(model, device_ids=[local_rank], output_device=local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要在建立 DDP 模型之後，才能設定 optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 Loss function</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss().to(local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 3. 網路訓練  ###</span></span><br><span class="line">model.train()</span><br><span class="line">iterator = tqdm(<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> iterator:</span><br><span class="line">    <span class="comment"># 設定 sampler 的 epoch</span></span><br><span class="line">    <span class="comment"># DistributedSampler 需要利用這個方式統一 shuffle</span></span><br><span class="line">    <span class="comment"># 使每個程序之間的亂數 seed 都是一樣的，使不同程序有相同的 shuffle 效果</span></span><br><span class="line">    trainloader.sampler.set_epoch(epoch)</span><br><span class="line">    <span class="comment"># 後面就與沒有用 DDP 的部份一樣了</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> trainloader:</span><br><span class="line">        data, label = data.to(local_rank), label.to(local_rank)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        prediction = model(data)</span><br><span class="line">        loss = loss_func(prediction, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        iterator.desc = <span class="string">&quot;loss = %0.3f&quot;</span> % loss</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="comment"># DDP:</span></span><br><span class="line">    <span class="comment"># 與原相同，使用 torch.save torch.load 就可以了</span></span><br><span class="line">    <span class="comment"># 要只在 rank=0 上儲存，不然會存到很多遍</span></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&quot;%d.ckpt&quot;</span> % epoch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################</span></span><br><span class="line"><span class="comment">## 在 command line 中執行程式</span></span><br><span class="line"><span class="comment"># 使用 torch.distributed.launch 來啟動 DDP 模式</span></span><br><span class="line"><span class="comment"># 使用 CUDA_VISIBLE_DEVICES，來決定使用哪些 GPU</span></span><br><span class="line"><span class="comment"># CUDA_VISIBLE_DEVICES=&quot;0,1&quot; python -m torch.distributed.launch --nproc_per_node 2 main.py</span></span><br></pre></td></tr></table></figure><h2 id="ring-reduce">Ring-Reduce</h2><p>但是這種有關 Thread 的東西，就不得不請出我們的 Python GIL 啦，PythonGIL 是一個全區鎖，可以看成是使 python 多執行緒效果非常差的兇手</p><p>可見以下網站更詳細的講解：</p><p><ahref="http://cenalulu.github.io/python/gil-in-python/">http://cenalulu.github.io/python/gil-in-python/</a></p><p>而 DDP 為了減少 Python GIL 的限制，因而使用而 Ring-Reduce 架構來使GPU 內互相溝通</p><p><img src="https://i.imgur.com/B3Sslcd.png" /></p><p>每個執行緒都只會接收來自上一個節點，並且把結果只丟給下一個節點，這種「圓圈圈」的做法可以大大減少互相通訊的複雜度(如果假設是每個節點相互連接的話)</p><p>進一步詳細的做法可以參考下面的知乎大神：</p><p><ahref="https://zhuanlan.zhihu.com/p/69797852">https://zhuanlan.zhihu.com/p/69797852</a></p><h2 id="並行計算">並行計算</h2><p><img src="https://i.imgur.com/drp22sg.png" /></p><p>一般來說神經網路的並行模式有一下三種：</p><ol type="1"><li><p>Data Parallelism</p><p>這是最常見的模式，換局話來說就是「增加 Batch-size」</p><p>DP DDP 剛剛講的那些 trick 都是屬於這一種的</p></li><li><p>Model Parallelism</p><p>把模型放在不同 GPU 上，是平行運算 (綠、黃)</p><p>看通訊效率，加速效果可能不明顯</p></li><li><p>Workload Partitioning</p><p>把模型放在不同 GPU 上，是串聯運算 (綠、藍)</p><p>不能加速</p></li></ol><h2 id="ddp-的一些基本名詞">DDP 的一些基本名詞</h2><ul><li><p>group</p><ul><li>程序組，一般只有一個組</li></ul></li><li><p>world size</p><ul><li>表示「全部」的程序總數</li><li>例如有 2 個 server ，每一台每面有 2 張 GPU，world size 為 2x2 =4</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># world size 在不同程序中，得到的值都是相同的</span></span><br><span class="line">torch.distributed.get_world_size()</span><br></pre></td></tr></table></figure></li><li><p>rank</p><ul><li>表示目前的程序編號，0, 1, 2, 3, ...</li><li>其中 rank=0 代表 master 程序</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每個程序有它自己的 rank 編號</span></span><br><span class="line">torch.distributed.get_rank()</span><br></pre></td></tr></table></figure></li><li><p>local rank</p><ul><li>同樣表示目前的程序編號，0, 1, 2, 3, ...</li><li>但特指「一個機器內的 GPU 編號」</li><li>(以 2 個 server ，每一台每面有 2 張 GPU為例，rank：0~3，local_rank：0, 1, 0, 1)</li><li>目的是在執從 torch.distributed.launch 時，機器會自動去分配對應的GPU</li></ul></li></ul><h2 id="ddp-原理">DDP 原理</h2><p>假設我們有 N 張 GPU</p><ul><li>減少 GIL 的限制<ul><li>總共 N 張 GPU 就會有 N 個程序被啟動</li><li>每一個 GPU 都執行同一個模型，參數的數值一開始也是相同的</li></ul></li><li>Ring-Reduce 加速<ul><li>在訓練模型時，使用 Ring-Reduce，彼此交換各自的梯度</li><li>藉此來得到所有運行程序中的梯度</li></ul></li><li>Data Parallelism<ul><li>把每個程序的梯度平均後，各自做 backpropagation 更新權重值</li><li>因為各程序的初始參數、更新梯度是一樣的，所以更新後的參數值也是完全一樣的</li></ul></li></ul><h2 id="ddp-vs-gradient-accumulation">DDP vs Gradient Accumulation</h2><ul><li>上面有提到 DDP 其實也就是「增加 Batch Size」而已</li><li>而 Gradient Accumulation 也是變像的增加 Batch Size</li><li>那兩者有什麼差別呢？</li><li>效能上<ul><li>在沒有 Buffer 參數 (像是 Batch Normalization)下，理論效能是一樣的</li><li>程序數 8 的 DDP 與 Step 8 的 Gradient Accumulation 是一樣的</li><li>(因為 Buffer 參數，理論上要每兩步才更新一次，但因是每個 epoch都會更新的緣故，BN 的分母會有對不上正確數字的問題)</li></ul></li><li>效率上<ul><li>DDP 因使用平行化處理</li><li>會比 Gradient Accumulation 快超多</li></ul></li></ul><h2 id="ddp-調用方式">DDP 調用方式</h2><p>與原本使用 python3 main.py 的使用方不同，需要用torch.distributed.launch 來啟動訓練</p><p>torch.distributed.launch 有幾個參數：</p><ul><li>--nnodes<ul><li>有多少台機器</li></ul></li><li>--node_rank<ul><li>目前是在哪個機器？</li></ul></li><li>--nproc_per_node<ul><li>每個機器有多少個程序</li></ul></li><li>--master_address<ul><li>master (rank=0) 的程序在哪一台 server 上</li></ul></li><li>--master_port<ul><li>要用哪一個 port 進行通訊？</li></ul></li></ul><p>單機下的例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假設只有一台機器</span></span><br><span class="line"><span class="comment"># 且一台機器內有 8 張 GPU</span></span><br><span class="line">python3 -m torch.distributed.launch --nproc_per_node 8 main.py</span><br></pre></td></tr></table></figure><p>多機下的例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假設有兩台機器</span></span><br><span class="line"><span class="comment"># 且每一台機器內有 8 張 GPU</span></span><br><span class="line"><span class="comment"># 需每個機器都執行一次程式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 機器一</span></span><br><span class="line">python3 -m torch.distributed.launch --nnodes=2 --node_rank=0 --nproc_per_node 8 --master_adderss <span class="variable">$address</span> --master_port <span class="variable">$port</span> main.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 機器二</span></span><br><span class="line">python3 -m torch.distributed.launch --nnodes=2 --node_rank=1 --nproc_per_node 8 --master_adderss <span class="variable">$address</span> --master_port <span class="variable">$port</span> main.py</span><br></pre></td></tr></table></figure><p>如果我們要求只使用機器內特定的 GPU 呢？像是機器一共有 8張卡，但只使用 4, 5, 6, 7</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">&quot;4,5,6,7&quot;</span> python -m torch.distributed.launch --nproc_per_node 4 main.py</span><br></pre></td></tr></table></figure><h2 id="reference">Reference</h2><p><ahref="https://zhuanlan.zhihu.com/p/178402798">DDP系列第一篇：入门教程</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;DDP 的全文是 Distributed Data
Parallel，是一種可以透過多機多卡來訓練模型的一種方法，它的本質上就是一個像
Map-Reduce 的東西，把訓練資料、Gradient、Loss 等資訊平均分配給每一個
GPU，達成多工處理的目的&lt;/p&gt;
&lt;p&gt;DDP 也可以就看成，提高 batch-size 來提高網路效果&lt;/p&gt;
&lt;p&gt;下面我們直接先來看 code 吧：&lt;/p&gt;
&lt;p&gt;keywords: DDP</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(七)：訓練小技巧 AMP 混合精度</title>
    <link href="https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B8%83-%EF%BC%9A%E8%A8%93%E7%B7%B4%E5%B0%8F%E6%8A%80%E5%B7%A7-AMP-%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6/"/>
    <id>https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B8%83-%EF%BC%9A%E8%A8%93%E7%B7%B4%E5%B0%8F%E6%8A%80%E5%B7%A7-AMP-%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6/</id>
    <published>2022-12-28T16:30:59.000Z</published>
    <updated>2022-12-28T16:32:33.689Z</updated>
    
    <content type="html"><![CDATA[<p>用一串話簡單解釋什麼是 AMP：</p><p>在 2017 Nvidia提出了用於「混合精度的訓練方法」，是一種可使用不同精度來運算 cuda tensor運算，Nvidia 很貼心的用 python 整理成 apex 套件讓大家方便使用https://github.com/NVIDIA/apex。而在之後 pytorch 1.6 的更新中，在 Nvidia的幫忙下，開發了 torch.cuda.amp 函式 (AMP 全稱 Automatic MixedPrecision)，使得混合精度訓練可以在 pytorch 中直接引入並使用。</p><p>keywords: AMP <span id="more"></span></p><p>相信大家看完一定還是霧颯颯，那接下來依照下列順序介紹AMP，更詳細的了解背後的歷史演進：</p><ul><li>什麼是精度？</li><li>為什麼要混合精度？</li><li>如何使用 AMP？</li></ul><h3 id="什麼是精度">什麼是精度？</h3><p>一般我們在使用 pytorch 時，如果簡單的初始化一個 tensor，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tensor1 = torch.zeros(<span class="number">20</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor.<span class="built_in">type</span>())   <span class="comment"># &#x27;torch.FloatTensor&#x27;</span></span><br><span class="line"></span><br><span class="line">tensor2 = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(tensor.<span class="built_in">type</span>())   <span class="comment"># &#x27;torch.FloatTensor&#x27;</span></span><br></pre></td></tr></table></figure><p>可以看到 pytorch 中，新增預設的精度就是FloatTensor，習慣上中文會稱它叫：單精度浮點運算 (single)</p><p>小小複習一下，通常 float 會用 32 個 bit 來存資料；double稱雙精度浮點則用 64 bit</p><p>而在 pytorch 中一共支援 10 種不同資料型態的 tensor：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.FloatTensor    (32-bit floating point)</span><br><span class="line">torch.DoubleTensor   (64-bit floating point)</span><br><span class="line">torch.HalfTensor     (16-bit floating point 1)</span><br><span class="line">torch.BFloat16Tensor (16-bit floating point 2)</span><br><span class="line">torch.ByteTensor     (8-bit integer (unsigned))</span><br><span class="line">torch.CharTensor     (8-bit integer (signed))</span><br><span class="line">torch.ShortTensor    (16-bit integer (signed))</span><br><span class="line">torch.IntTensor      (32-bit integer (signed))</span><br><span class="line">torch.LongTensor     (64-bit integer (signed))</span><br><span class="line">torch.BoolTensor     (Boolean)</span><br></pre></td></tr></table></figure><p>可以發現在 DoubleTensor 下方多了一個 HalfTensor「半精度浮點」，而這個就是今天的主角，也是為什麼要使用 AMP的最大理由。</p><h3 id="為什麼要混合精度">為什麼要混合精度？</h3><p>剛剛上面介紹各種型態的 Tensor 最後都會整理到 Nvidia GPU中做運算，而在 GPU 負責運算的單元稱 cuda 核心(<strong>C</strong>ompute<strong>U</strong>nified <strong>D</strong>evice<strong>A</strong>rchitecture 統一計算架構)，一個 cuda 核心由一個 ALU(Integer arithmetic logic uint 整數運算單元) 及一個 FPU (Floating pointunit 浮點運算單元) 所組成，也就是說一個 CUDA核心專門來做<strong>乘法</strong>及<strong>加法</strong>，而 cuda核心中還有一個特別的指令：FMA (Fused multiply add)可以用一個指令完成加乘融合的操作。</p><p><img src="https://i.imgur.com/IO8GIwY.png"alt="image-20220820113513902" /></p><p>一般我們在深度學習中最常看見的算式是這個： <spanclass="math display">\[x_{l} = x_{l-1}w+b\]</span> 這種又加又乘的操作藉由 cuda核心的幫忙，可以在不改變精度下，把原本要兩個指令完成的事縮減成一個指令，大輻減少運算時間。以上cuda 預設支援 Float32 的運算，也正好與 pytorch 相符。自 2006 年的 Tesla架構推出以後，cuda 核心就一直內建在 Nvidia GPU 中了。</p><p>不過這時有一個聲音悄悄的跑出來：我們能不能再加速呢？如果還要加速的話有以下兩個地方可以改進：</p><ul><li>設計新的核心，可以硬體加速更高級的運算，例如一個指令完成 Tensor運算</li><li>藉由把浮點數的精度降低，再做乘法，達到減少運算複雜度的加速，但同時又不能失去太多的精度</li></ul><p>如果你是 Nvidia 工程師會怎麼呢？小朋友才選擇嘛 XD當然是兩個都做阿！所以 2017 年年底 Nvidia Volta 架構上提出了新的 Tensor核心單元，完美達成上面兩件事情：在不損失太多精度下，減少整體的運算時間。接下透過以下兩個GIF 動畫可以了解到 Tensor 核心的力量</p><p><img src="https://i.imgur.com/LSa0CvU.gif"alt="1fd55a3c-9362-11eb-a595-1278b449b310" /></p><p><img src="https://i.imgur.com/21VdRyt.gif"alt="5a1ec0e0-7e84-11eb-aca1-aa09f3df2eff" /></p><p>上面兩動畫還隱含了兩個資訊：</p><ul><li>Tensor 核心可以做到使用一個指令完成一個 Tensor 運算</li><li>當資料精度越小時 (FP32 -&gt; FP16 -&gt;INT8)，同一時間下完成的運算量更高</li></ul><p>所以整個又回到最一開始的問題，為什麼要使用「混合精度」？因為更低的精度意味著更快的運算，但為了資料不能丟失太多細節，所以有必要使用高精度運算的還是維持FP32，但是有一些沒那麼重要的運算就可以改使用 FP16，這樣在一個 Tensor運算中，又有 FP32 又有 FP16 的操作，就是混合精度的原由。</p><h3 id="如何使用-amp">如何使用 AMP</h3><p>剛剛上述提到的 FP32 對應 pytorch 中的<code>torch.FloatTensor</code>，而 FP16 則是對應<code>torch.HalfTensor</code>，這兩種不同的精度各自有什麼優缺點呢？</p><p>HalfTensor 的優缺點：</p><p>精度低，運算快，但消失精度的代價是算出來的值失去很多細節，這個現象會導致，overfitting/underfitting的發生。因為在做 Backpropagation時根據數值不斷的往後計算，越算越小，小到超出 FP16 所能表示的最小數值<spanclass="math inline">\(2^{-14}\)</span>，會使得更先前的層參數無法更新</p><p>另一個問題也是因為 FP16 最小的數值間距為 <spanclass="math inline">\(2^{-13}\)</span>如果有小於這個數字的算式都會被當誤差而省略掉了</p><p>因此要如何甚選要什麼運算使用 FP16 來加速可是個大問題，好加在 pytorch已經幫我們整理好了，以下的操作都是可以用 FP16 來加速，因此 pytorch會自動這型態轉換成 HalfTensor 來計算，而其它則維持 FloatTensor：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">__matmul__</span><br><span class="line">addbmm</span><br><span class="line">addmm</span><br><span class="line">addmv</span><br><span class="line">addr</span><br><span class="line">baddbmm</span><br><span class="line">bmm</span><br><span class="line">chain_matmul</span><br><span class="line">conv1d</span><br><span class="line">conv2d</span><br><span class="line">conv3d</span><br><span class="line">conv_transpose1d</span><br><span class="line">conv_transpose2d</span><br><span class="line">conv_transpose3d</span><br><span class="line">linear</span><br><span class="line">matmul</span><br><span class="line">mm</span><br><span class="line">mv</span><br><span class="line">prelu</span><br></pre></td></tr></table></figure><p>那實際上程式碼要怎麼去寫呢？其實也非很簡單，只需引用 torch.cuda.amp包，再進行以下操作就行了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用 amp 中的 autocast 來實現，自動判哪些運算要用 HalfTensor 哪些運算維持原樣用 FloatTensor</span></span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast <span class="keyword">as</span> autocast</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立新 model，預設是 torch.FloatTensor</span></span><br><span class="line">model = Net().cuda()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), ...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 with 關鍵字，把前傳遞 forward 及算 loss</span></span><br><span class="line">    <span class="comment"># 的部份用 autocast() 包起來</span></span><br><span class="line">    <span class="keyword">with</span> autocast():</span><br><span class="line">        output = model(<span class="built_in">input</span>)</span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation 不必用 autocast 包起來</span></span><br><span class="line">    <span class="comment"># 理由是 Backpropagation 會依據 Forward 的資料型態直接沿用來做</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><p>是不是很簡單呢？簡簡單單的一行就可以用 Tensor核心幫你加速訓練/測試的時間，與此同時還有一個好的副作用：顯存下降了！也很合理，因為要存的浮點精度變少了嘛</p><p>不過如果只單純這樣用的話，在訓練時會多發生一個問題，訓練會over/underfitting！，精度的下降果然還是使用在 Backpropagation時，參數傳不到前面去更新了，因此要再使用 amp中的另一個黑科技：GradScaler</p><p>GradScaler 實際精神在於，把網路算出來的 Loss 用一個倍率放大，在Backpropagation 存著 .grad 的值也一並放大，但最後用 optimizer更新參數時還是要把值縮小回原本的大小，這樣子的做法就不會有因為精度損失而導致更新不到前面的參數了</p><p>實驗程式碼的實作方式也不困難，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 利用 amp 中的 autocast 來實現，自動判斷哪些運算要用 HalfTensor 哪些運算維持原樣用 FloatTensor</span></span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast <span class="keyword">as</span> autocast</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立新 model，預設是 torch.FloatTensor</span></span><br><span class="line">model = Net().cuda()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), ...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 with 關鍵字，把前傳遞 forward 及算 loss</span></span><br><span class="line">    <span class="comment"># 的部份用 autocast() 包起來</span></span><br><span class="line">    <span class="keyword">with</span> autocast():</span><br><span class="line">        output = model(<span class="built_in">input</span>)</span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scales loss. 用一定的倍率放大 Loss，並計算出各個 node 的 .grad 值</span></span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 這一步詳細流程見下面</span></span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 準備著，看下一次是否有要做 scaler 放大 Loss</span></span><br><span class="line">        scaler.update()</span><br></pre></td></tr></table></figure><p>這個 scaler放大倍數也是動態調整的，為什麼呢？理應放大倍率越大越好，保留越多的數字，但現實很骨感，如果真放超大會直接overfitting 出現 infs，但是放大太小又會出現 NaNs，所以這個 scaler會自動的去調整放大倍率大小，在不發生仍何 over/underfitting下找到最合適的放大倍率</p><p>以上就是 torch.cuda.amp的完整詳細介紹及用法啦！要再更進階的話還有一個小細節要注意：如果是有使用DDP 訓練的方法，在加入 autocast() 要特別注意</p><p>除了在 train 的 forward 時要加入 autocast() 前文，同時也要記得在 繼承nn.module 的 forward() 函式中，也要加上 autocast() 的前文，或是使用decorator 也可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一：使用 decorator</span></span><br><span class="line">MyModel(nn.Module):</span><br><span class="line"><span class="meta">    @autocast()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 方法一：使用 with 前文</span></span><br><span class="line">MyModel(nn.Module):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">with</span> autocast():</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = MyModel()</span><br><span class="line">dp_model=nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除了訓練 forward 要加，model 中的 forward 也要加</span></span><br><span class="line"><span class="keyword">with</span> autocast():</span><br><span class="line">    output = dp_model(<span class="built_in">input</span>)</span><br><span class="line">    loss = loss_fn(output)</span><br></pre></td></tr></table></figure><p>那實際效果跑起來如何呢？基本上網友們的反應是：一、顯存下降；二、時間變長，咦…等等等，怎麼用了混合精度時間變慢，不是說精度越小速度越快嗎？後來發現原因出現在GradScaler 上面，Loss 及梯度在經過一個 scaler放大縮小一來一回下，增加了不少時間損耗，至於這個功能最後要不要加上去呢…？這個就見人見智囉！</p><h3 id="reference">Reference</h3><p><a href="https://www.zhihu.com/question/451127498">cuda core vstensor core 知乎</a></p><p><ahref="https://www.cnblogs.com/jimchen1218/p/14315008.html">Pytorch自动混合精度(AMP)介绍与使用</a></p><p><ahref="https://zhuanlan.zhihu.com/p/165152789">PyTorch的自动混合精度（AMP）</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;用一串話簡單解釋什麼是 AMP：&lt;/p&gt;
&lt;p&gt;在 2017 Nvidia
提出了用於「混合精度的訓練方法」，是一種可使用不同精度來運算 cuda tensor
運算，Nvidia 很貼心的用 python 整理成 apex 套件讓大家方便使用
https://github.com/NVIDIA/apex。而在之後 pytorch 1.6 的更新中，在 Nvidia
的幫忙下，開發了 torch.cuda.amp 函式 (AMP 全稱 Automatic Mixed
Precision)，使得混合精度訓練可以在 pytorch 中直接引入並使用。&lt;/p&gt;
&lt;p&gt;keywords: AMP</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(六)：訓練小技巧 Gradient accumulation</title>
    <link href="https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%85%AD-%EF%BC%9A%E8%A8%93%E7%B7%B4%E5%B0%8F%E6%8A%80%E5%B7%A7-Gradient-accumulation/"/>
    <id>https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%85%AD-%EF%BC%9A%E8%A8%93%E7%B7%B4%E5%B0%8F%E6%8A%80%E5%B7%A7-Gradient-accumulation/</id>
    <published>2022-12-28T16:26:47.000Z</published>
    <updated>2022-12-28T16:30:15.946Z</updated>
    
    <content type="html"><![CDATA[<p>你是不是常常覺得 GPU顯存不夠用？是不是覺得自己太窮買不起好的顯卡、也租不起好的機台？覺得常常因為東卡西卡關就放棄深度學習？</p><p>沒關系！接下來有幾招可以在預算不太足的情況下，還是可以讓訓練跑得起來！</p><p>keywords: Gradient accumulation <span id="more"></span></p><h2 id="gradient-accumulation">Gradient accumulation</h2><p>第一招叫做 Gradient accumulation</p><p>這是利用 pytorch 每一次在 backpropagation 前都會把梯度清零</p><p>正常一個訓練部份程式會這樣寫：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="comment"># 1. input output</span></span><br><span class="line">    pred = model(image)</span><br><span class="line">    loss = criterion(pred, label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. backward</span></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 把梯度清零</span></span><br><span class="line">    loss.backward()<span class="comment"># backpropagation 計算當前的梯度</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 拫據梯度更新網路參數</span></span><br></pre></td></tr></table></figure><p>而 Gradient accumulation 會這麼寫</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i,(image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="comment"># 1. input output</span></span><br><span class="line">    pred = model(image)</span><br><span class="line">    loss = criterion(pred, label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.1 loss 要除以累積的總步數，正規化 loss 的值</span></span><br><span class="line">    loss = loss / accumulation_steps  </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 2.2 計算梯度的值</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 當累積的步數到一定的程度後，梯度中的值也會不斷累加，才會更新網路的參數</span></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % accumulation_steps == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># optimizer the net</span></span><br><span class="line">        optimizer.step()        <span class="comment"># 更新網路參數</span></span><br><span class="line">        optimizer.zero_grad()   <span class="comment"># 清空以前的梯度</span></span><br></pre></td></tr></table></figure><p>這樣子這的理由是，每次的 epoch不把梯度清零，而是用累加到一定的程度後才會更新網路參數值</p><p>目的是要在不增加 RAM 的條件下，變向增加 batch size (窮人做法阿QQ)</p><p>有兩個要注意的小地方：</p><ol type="1"><li><p>learning rate</p><p>因為變每兩步為一個單位計算梯度了，而 learning rate的設定依舊是以一步為一個單位</p><p>所以要適當的調大一些些</p></li><li><p>batch normalization</p><p>原理同上</p><p>BN 的分母為全部 Batch szie 的值，但因單位的改變，而 BN 卻沒跟上</p><p>所以 BN 的分母的值並非全部的 batch size(但根據下面文章好像又沒差多少…我不是很清楚 XD)</p><p>但可確認的是，效果一定比單純的增加 batch size 來的差一些些</p><p>或是可以調低 BN 的 momentum 參數</p></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;你是不是常常覺得 GPU
顯存不夠用？是不是覺得自己太窮買不起好的顯卡、也租不起好的機台？覺得常常因為東卡西卡關就放棄深度學習？&lt;/p&gt;
&lt;p&gt;沒關系！接下來有幾招可以在預算不太足的情況下，還是可以讓訓練跑得起來！&lt;/p&gt;
&lt;p&gt;keywords: Gradient accumulation</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(五)：網路一層模型 Parameter vs Buffer</title>
    <link href="https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%BA%94-%EF%BC%9A%E7%B6%B2%E8%B7%AF%E4%B8%80%E5%B1%A4%E6%A8%A1%E5%9E%8B-Parameter-vs-Buffer/"/>
    <id>https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%BA%94-%EF%BC%9A%E7%B6%B2%E8%B7%AF%E4%B8%80%E5%B1%A4%E6%A8%A1%E5%9E%8B-Parameter-vs-Buffer/</id>
    <published>2022-12-28T16:18:03.000Z</published>
    <updated>2022-12-28T16:25:31.948Z</updated>
    
    <content type="html"><![CDATA[<p>有時候我們在看別人的論文時會發現：常常會有一些「超參數」的出現，像是ResNet shortcut 進入的權重值等等</p><p>這個時候就可以用 Pytorch 提供的 Parameter 和 buffer來實作，想知道詳細差在哪裡就繼續往下看吧 ~</p><p>keywords: Parameter、buffer <span id="more"></span></p><h2 id="parameter-和-buffer">Parameter 和 buffer</h2><p>有時候我們想要在網路中新增一層或是一個參數時，就可以使用 Parameter或是 buffer</p><ul><li>Parameter 在反向傳播時「會」隨著網路更新權重值</li><li>Buffer 在反向傳播時「不會」隨著網硬更新權重值</li></ul><p>建立方向：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        buffer = torch.randn(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># tensor</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;my_buffer&#x27;</span>, buffer)     <span class="comment"># buffer 的定義方式 (str：定義名字，tensor：傳入權重)</span></span><br><span class="line">        self.param = nn.Parameter(torch.randn(<span class="number">3</span>, <span class="number">3</span>))  <span class="comment"># Parameter 的定義方式 (tensor)</span></span><br><span class="line">        self.register_parameter(<span class="string">&quot;param&quot;</span>, param)       <span class="comment"># 另一種定義 Parameter 的方式 (與上行程式等價)，看你習慣，好處是可自定義名稱</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 可以通过 self.param 和 self.my_buffer 访问</span></span><br><span class="line">        self.my_buffer(x)     <span class="comment"># 使用剛剛定義的 str 名字</span></span><br><span class="line">        self.param(x)</span><br></pre></td></tr></table></figure><p>兩者的共同點就是，在使用 <code>model.state_dict()</code>的方法來保存、讀取網路模型時，都會被存入到 OrderDict 中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">model = MyModel(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># get buffer</span></span><br><span class="line">model = MyModel()</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># get param</span></span><br><span class="line"><span class="keyword">for</span> buffer <span class="keyword">in</span> model.buffers():</span><br><span class="line">    <span class="built_in">print</span>(buffer)</span><br></pre></td></tr></table></figure><p>在 ViT 的 Patch Embedding 中有使用到，用在 reletive positionalencoding 上，因為相對位置編碼不會隨著網路而更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.embs = nn.Embedding(vocab_size, d_model) <span class="comment"># word embedding， 需要 backprop 更新</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pe shape: (0, max_len, d_model)</span></span><br><span class="line">        pe = self._build_position_encoding(max_len, d_model)  </span><br><span class="line">        self.register_buffer(<span class="string">&quot;pe&quot;</span>, pe)  <span class="comment"># position encoding，不需 backprop 更新</span></span><br></pre></td></tr></table></figure><h3 id="reference">reference</h3><p><ahref="https://zhuanlan.zhihu.com/p/89442276">https://zhuanlan.zhihu.com/p/89442276</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;有時候我們在看別人的論文時會發現：常常會有一些「超參數」的出現，像是
ResNet shortcut 進入的權重值等等&lt;/p&gt;
&lt;p&gt;這個時候就可以用 Pytorch 提供的 Parameter 和 buffer
來實作，想知道詳細差在哪裡就繼續往下看吧 ~&lt;/p&gt;
&lt;p&gt;keywords: Parameter、buffer</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(四)：資料擴增、前處理 torchvision.transforms</title>
    <link href="https://mushding.space/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%9B%9B-%EF%BC%9A%E8%B3%87%E6%96%99%E6%93%B4%E5%A2%9E%E3%80%81%E5%89%8D%E8%99%95%E7%90%86-torchvision-transforms/"/>
    <id>https://mushding.space/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%9B%9B-%EF%BC%9A%E8%B3%87%E6%96%99%E6%93%B4%E5%A2%9E%E3%80%81%E5%89%8D%E8%99%95%E7%90%86-torchvision-transforms/</id>
    <published>2022-12-27T06:42:41.000Z</published>
    <updated>2022-12-28T16:22:01.472Z</updated>
    
    <content type="html"><![CDATA[<p>在處理dataset，有時候我們會遇到需要資料前處理、資料擴增…等等對影像處理的步驟：像是希望可以透過旋轉、鏡像做到資料擴增；希望可以利用影像裁切統一輸入影像大小</p><p>而 pytorch 也很貼心的提供給我們一個套件使用：torchvision，torchvision裡面提供了非常多的影像處理方法：像是旋轉、鏡像、裁切</p><p>以下這篇文章整理了大部份常用到的函式：<ahref="https://chih-sheng-huang821.medium.com/03-pytorch-dataaug-a712a7a7f55e">Pytorch提供之torchvisiondata augmentation技巧</a></p><p>keywords: torchvision <span id="more"></span></p><h2 id="如何使用">如何使用？</h2><p>torchvision 所有函式輸入只支持 PIL Image，也是就使用 PIL Image這個套件打開圖片的格式，才會被 torchvision 接受，其它像是 torch.tensor或是 np.array 都是沒有辦法的</p><p>以我們要把影像轉為灰階圖片 -&gt; 然後從 PIL Image 轉成torch.tensor，PIL 的 SOP 如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">    </span><br><span class="line"><span class="comment"># read image with PIL module</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(imagepath)</span><br><span class="line">img = transforms.Grayscale()(img)</span><br><span class="line">img = transforms.ToTensor()(img)</span><br></pre></td></tr></table></figure><p>如果覺得這樣寫太多行的話，torchvision 也有提供 transforms.Compose的方案，可以把很多的 transform 打包在一起，就可以統一呼叫便於管理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義 compose    </span></span><br><span class="line">transfrom = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># read image with PIL module</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(imagepath)</span><br><span class="line"><span class="comment"># 使用 compose</span></span><br><span class="line">img = transform(img)</span><br></pre></td></tr></table></figure><p>而在 transforms.Compose中，我們也可以放入自定義的影像處理函式，假設我們要定義一個「自定義Padding」的處理，程式碼如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義 compose    </span></span><br><span class="line">transfrom = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(),</span><br><span class="line">    <span class="comment"># 自定義的處理函式</span></span><br><span class="line">    SuarePad(),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># read image with PIL module</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(imagepath)</span><br><span class="line"><span class="comment"># 使用 compose</span></span><br><span class="line">img = transform(img)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SquarePad</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, targetW, targetH</span>):</span></span><br><span class="line">        self.targetW = targetW</span><br><span class="line">        self.targetH = targetH</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, image</span>):</span></span><br><span class="line">        <span class="comment"># 2003 308</span></span><br><span class="line">        w, h = image.size</span><br><span class="line">        w_fix = self.targetW - w</span><br><span class="line">        padding = (<span class="number">0</span>, <span class="number">0</span>, w_fix, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> F.pad(image, padding, <span class="number">0</span>, <span class="string">&#x27;constant&#x27;</span>)</span><br></pre></td></tr></table></figure><p>對於上面自定義 class 的補充說明：</p><p><code>__init__</code> 的目的在類似 constructor，只在當一個類別(class) 實作成物件 (object) 時會呼叫</p><p><code>__call__</code> 是可以把 class也模擬有著函式一樣的特色，用傳入參數並呼叫的方式使用</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在處理
dataset，有時候我們會遇到需要資料前處理、資料擴增…等等對影像處理的步驟：像是希望可以透過旋轉、鏡像做到資料擴增；希望可以利用影像裁切統一輸入影像大小&lt;/p&gt;
&lt;p&gt;而 pytorch 也很貼心的提供給我們一個套件使用：torchvision，torchvision
裡面提供了非常多的影像處理方法：像是旋轉、鏡像、裁切&lt;/p&gt;
&lt;p&gt;以下這篇文章整理了大部份常用到的函式：&lt;a
href=&quot;https://chih-sheng-huang821.medium.com/03-pytorch-dataaug-a712a7a7f55e&quot;&gt;Pytorch提供之torchvision
data augmentation技巧&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;keywords: torchvision</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(三)：網路模型 torch.nn.Module</title>
    <link href="https://mushding.space/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B8%89-%EF%BC%9A%E7%B6%B2%E8%B7%AF%E6%A8%A1%E5%9E%8B-torch-nn-Module/"/>
    <id>https://mushding.space/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B8%89-%EF%BC%9A%E7%B6%B2%E8%B7%AF%E6%A8%A1%E5%9E%8B-torch-nn-Module/</id>
    <published>2022-12-27T06:16:02.000Z</published>
    <updated>2022-12-28T16:22:14.476Z</updated>
    
    <content type="html"><![CDATA[<p>設定好訓練SOP、設定好自定義的資料集後，接下來我們要來設計自己的網路模型，會使用到Pytorch 中 torch.nn.Module 這個物件</p><p>keywords: torch.nn.Module <span id="more"></span></p><h2 id="torch.nn.module">torch.nn.Module</h2><ul><li>有三種創建 module 的方法<ul><li>繼承 nn.module 的普通方法</li><li>nn.sequential</li><li>nn.ModuleList</li></ul></li></ul><h3 id="nn.module">nn.Module</h3><ul><li>基本款</li><li>有一個 __init__ 設定各個神經層的設定，命名好後在下一個 forward來使用，通常是放「需要學習的的層」</li><li>另一個 forward來設定各個層的連接以及參數設定，通常放的是「不需要學習的層」，像activate function</li><li>在 pytorch 中 backward 會自動實現，使用的是 Autogard</li><li>以及在 pytorch 中 nn.Module 只支持 mini-batch 的輸入方式 N x C x H xW (1 x 3 x 128 x 128)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(conv2(x))</span><br></pre></td></tr></table></figure><h3 id="nn.sequential">nn.Sequential</h3><ul><li>nn.Sequential的模組是按照順序排列的，需要確保輸出大小與輸入大小一致</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net_seq</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(net2, self).__init__()</span><br><span class="line">        self.seq = nn.Sequential(</span><br><span class="line">              nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">              nn.ReLU(),</span><br><span class="line">              nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">              nn.ReLU()</span><br><span class="line">        )      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.seq(x)</span><br><span class="line">net_seq = net_seq()</span><br><span class="line"><span class="built_in">print</span>(net_seq)</span><br><span class="line"></span><br><span class="line"><span class="comment">#net_seq(</span></span><br><span class="line"><span class="comment">#  (seq): Sequential(</span></span><br><span class="line"><span class="comment">#    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (1): ReLU()</span></span><br><span class="line"><span class="comment">#    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (3): ReLU()</span></span><br><span class="line"><span class="comment">#  )</span></span><br><span class="line"><span class="comment">#)</span></span><br></pre></td></tr></table></figure><ul><li>nn.Sequential 中也採用 OrderedDict 来指定 module 的名字，而非 index(0, 1, 2, ...)</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net_seq</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(net_seq, self).__init__()</span><br><span class="line">        self.seq = nn.Sequential(OrderedDict([</span><br><span class="line">            (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">            (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">            (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">            (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.seq(x)</span><br><span class="line">net_seq = net_seq()</span><br><span class="line"><span class="built_in">print</span>(net_seq)</span><br><span class="line"></span><br><span class="line"><span class="comment">#net_seq(</span></span><br><span class="line"><span class="comment">#  (seq): Sequential(</span></span><br><span class="line"><span class="comment">#    (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (relu1): ReLU()</span></span><br><span class="line"><span class="comment">#    (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (relu2): ReLU()</span></span><br><span class="line"><span class="comment">#  )</span></span><br><span class="line"><span class="comment">#)</span></span><br></pre></td></tr></table></figure><h3 id="nn.modulelist">nn.ModuleList</h3><ul><li>nn.ModuleList 也是一個存不同 module 的 list，可任意得把 nn.Module加到 list 中</li><li>與 python 的 list 操作相同，可以 extend append...</li><li>但它會自動把 module 的 parameters 自動加入網路中</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net_modlist</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(net_modlist, self).__init__()</span><br><span class="line">        self.modlist = nn.ModuleList([</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">20</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modlist:</span><br><span class="line">            x = m(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net_modlist = net_modlist()</span><br><span class="line"><span class="built_in">print</span>(net_modlist)</span><br><span class="line"><span class="comment">#net_modlist(</span></span><br><span class="line"><span class="comment">#  (modlist): ModuleList(</span></span><br><span class="line"><span class="comment">#    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (1): ReLU()</span></span><br><span class="line"><span class="comment">#    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (3): ReLU()</span></span><br><span class="line"><span class="comment">#  )</span></span><br><span class="line"><span class="comment">#)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net_modlist.parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(param.data), param.size())</span><br><span class="line"><span class="comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([20, 1, 5, 5])</span></span><br><span class="line"><span class="comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([20])</span></span><br><span class="line"><span class="comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([64, 20, 5, 5])</span></span><br><span class="line"><span class="comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([64])</span></span><br></pre></td></tr></table></figure><h3 id="nn.sequential-vs-nn.modulelist">nn.Sequential vsnn.ModuleList</h3><ul><li>nn.Sequential 內部自動實現 forward 所以不用再一個一個加，但nn.ModuleList 沒有，任需一個一個加入</li><li>且 nn.Module 中沒有一定的順序，可用 index 來指定</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不在 nn.Module 的方法</span></span><br><span class="line">seq = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(seq)</span><br><span class="line"><span class="comment"># Sequential(</span></span><br><span class="line"><span class="comment">#   (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (1): ReLU()</span></span><br><span class="line"><span class="comment">#   (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (3): ReLU()</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># nn.Sequential</span></span><br><span class="line"><span class="comment"># 繼承 nn.Module 的方法，就要寫出 forward </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net1</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(net1, self).__init__()</span><br><span class="line">        self.seq = nn.Sequential(</span><br><span class="line">             nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">             nn.ReLU(),</span><br><span class="line">             nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">             nn.ReLU()</span><br><span class="line">        )      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.seq(x)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># nn.ModuleList 的方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net2</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(net2, self).__init__()</span><br><span class="line">      self.modlist = nn.ModuleList([</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">      ])</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 注意：只能按照下面利用 for 的方式</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">       <span class="keyword">for</span> m <span class="keyword">in</span> self.modlist:</span><br><span class="line">           x = m(x)</span><br><span class="line">       <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="reference">Reference</h3><ul><li>https://blog.csdn.net/u012609509/article/details/81203436</li><li>https://zhuanlan.zhihu.com/p/75206669</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;設定好訓練
SOP、設定好自定義的資料集後，接下來我們要來設計自己的網路模型，會使用到
Pytorch 中 torch.nn.Module 這個物件&lt;/p&gt;
&lt;p&gt;keywords: torch.nn.Module</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(二)：Dataset DataLoader</title>
    <link href="https://mushding.space/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%BA%8C-%EF%BC%9ADataset-DataLoader/"/>
    <id>https://mushding.space/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%BA%8C-%EF%BC%9ADataset-DataLoader/</id>
    <published>2022-12-27T05:59:29.000Z</published>
    <updated>2022-12-28T16:22:19.727Z</updated>
    
    <content type="html"><![CDATA[<p>如果今天開發需求不是像 Mnist這樣，別人已經幫你準備好的資料集，而是自己的影像資料集，那要怎麼放進DataLoader 裡面訓練呢？</p><p>keywords: DataLoader <span id="more"></span></p><h2 id="dataset-dataloader">Dataset DataLoader</h2><ul><li>使用繼承 Dataset 可以自定義 data，再放進 DataLoader 中</li><li>一個 Dataset 繼承後要 override 的 function 如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">customDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        <span class="comment"># Initialize paths, transforms, and so on</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        <span class="comment"># 1. Read from file (using numpy.fromfile, PIL.Image.open)</span></span><br><span class="line">        <span class="comment"># 2. Preprocess the data (torchvision.Transform).</span></span><br><span class="line">        <span class="comment"># 3. Return the data (e.g. image and label)</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        <span class="comment"># Indicate the total size of the dataset</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br></pre></td></tr></table></figure><ul><li>__init__ 負責初使化 path、img list、label list、transform</li><li>__getitem__ 負責讀取圖片，並做 transform，回傳 img 以及 label<ul><li>回傳值也可以回傳不只兩個，也可根據需求回傳想要的資料</li></ul></li><li>__len__ 回傳 imgs 的長度 len(self.imgs)</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;如果今天開發需求不是像 Mnist
這樣，別人已經幫你準備好的資料集，而是自己的影像資料集，那要怎麼放進
DataLoader 裡面訓練呢？&lt;/p&gt;
&lt;p&gt;keywords: DataLoader</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>你所不知道的 Pytorch 大補包(一)：從官方 mnist source code 來學習 Pytorch</title>
    <link href="https://mushding.space/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B8%80-%EF%BC%9A%E5%BE%9E%E5%AE%98%E6%96%B9-mnist-source-code-%E4%BE%86%E5%AD%B8%E7%BF%92-Pytorch/"/>
    <id>https://mushding.space/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B8%80-%EF%BC%9A%E5%BE%9E%E5%AE%98%E6%96%B9-mnist-source-code-%E4%BE%86%E5%AD%B8%E7%BF%92-Pytorch/</id>
    <published>2022-12-27T05:45:48.000Z</published>
    <updated>2023-01-08T02:41:35.953Z</updated>
    
    <content type="html"><![CDATA[<p>Pytorch已經成為深度學習的主流框架了，以下系列是我從大學時期，一路學習 Pytorch筆記整理下來的心得，後來再經整理整理決定放到網路上給大家參考，希望可以幫助到更多從0 開始想自學的人 ~</p><p>整個系列會由淺到深，一路從最基本的 SOP，到 Pytorch底層的實作邏輯，以下先以深度學習界的 Hallo World mnist 來開始學習</p><p>keywords: mnist <span id="more"></span></p><ul><li>首先來看看 mnist 在 pytorch 上實作的程式</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> StepLR</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.dropout1 = nn.Dropout(<span class="number">0.25</span>)</span><br><span class="line">        self.dropout2 = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">9216</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)</span><br><span class="line">        x = self.dropout1(x)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.dropout2(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        output = F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model, device, train_loader, optimizer, epoch, log_interval, is_dry_run</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, batch_idx * <span class="built_in">len</span>(data), <span class="built_in">len</span>(train_loader.dataset),</span><br><span class="line">                <span class="number">100.</span> * batch_idx / <span class="built_in">len</span>(train_loader), loss.item()</span><br><span class="line">            ))</span><br><span class="line">        <span class="keyword">if</span> is_dry_run:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">model, device, test_loader</span>):</span></span><br><span class="line">    <span class="comment"># evaluate mode</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># stop gradient calculation &amp; decrease GPU processing</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            output = model(data)</span><br><span class="line">            test_loss += F.nll_loss(output, target, reduction=<span class="string">&#x27;sum&#x27;</span>).item()</span><br><span class="line">            pred = output.argmax(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss, correct, <span class="built_in">len</span>(test_loader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_loader.dataset)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="comment"># variables</span></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    epochs = <span class="number">14</span></span><br><span class="line">    learning_rate = <span class="number">1</span></span><br><span class="line">    log_interval = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># is variables</span></span><br><span class="line">    is_save_model = <span class="literal">True</span></span><br><span class="line">    is_dry_run = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># is use cuda?</span></span><br><span class="line">    use_cuda = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set seed</span></span><br><span class="line">    torch.manual_seed(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set device</span></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set train/test dict</span></span><br><span class="line">    train_kwargs = &#123;<span class="string">&#x27;batch_size&#x27;</span>: batch_size&#125;</span><br><span class="line">    test_kwargs = &#123;<span class="string">&#x27;batch_size&#x27;</span>: batch_size&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if use cuda?</span></span><br><span class="line">    <span class="keyword">if</span> use_cuda:</span><br><span class="line">        cuda_kwargs = &#123;</span><br><span class="line">            <span class="string">&#x27;num_workers&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">&#x27;pin_memory&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">            <span class="string">&#x27;shuffle&#x27;</span>: <span class="literal">True</span></span><br><span class="line">        &#125;</span><br><span class="line">        train_kwargs.update(cuda_kwargs)</span><br><span class="line">        test_kwargs.update(cuda_kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mnist transforms</span></span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load mnist set</span></span><br><span class="line">    train_dataset = datasets.MNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">        train=<span class="literal">True</span>,</span><br><span class="line">        transform=transform,</span><br><span class="line">        download=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    test_dataset = datasets.MNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">        train=<span class="literal">False</span>,</span><br><span class="line">        transform=transform,</span><br><span class="line">        download=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set loader</span></span><br><span class="line">    train_loader = Data.DataLoader(train_dataset, **train_kwargs)</span><br><span class="line">    test_loader = Data.DataLoader(test_dataset, **test_kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># start init net</span></span><br><span class="line">    model = Net().to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set optimizer</span></span><br><span class="line">    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set lr decrease scheduler</span></span><br><span class="line">    scheduler = StepLR(optimizer, step_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># start training for epoch</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">        train(model, device, train_loader, optimizer, epoch, log_interval, is_dry_run)</span><br><span class="line">        test(model, device, test_loader)</span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_save_model:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&quot;mnist_cnn.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><ul><li><p>一個 pytorch 一定會有下面幾個部份</p></li><li><p>device</p></li><li><p>DataLoader</p></li><li><p>optimizer</p></li><li><p>scheduler (*)</p></li><li><p>init &amp; define Net class</p><ul><li>__init__</li><li>forward</li></ul></li><li><p>for epoch -&gt;</p><ul><li>train</li><li>test</li><li>save model</li></ul></li></ul><h3 id="cuda-的設置">cuda 的設置</h3><ul><li>可以根據狀況來使用 "cpu" 或者是 "gpu"</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># is use cuda?</span></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="comment"># set seed</span></span><br><span class="line">torch.manual_seed(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set device</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="seed-的設置">seed 的設置</h3><ul><li>在初使化參數時是選擇用隨機的方式，所以要選擇用哪一個 seed？</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set seed</span></span><br><span class="line">torch.manual_seed(<span class="number">5000</span>)</span><br></pre></td></tr></table></figure><h3 id="mnist-的設置">MNIST 的設置</h3><ul><li>使用 torchvision 來用 MNIST<ul><li>root = 儲存的地方</li><li>train = 設定是否訓練集</li><li>transform = 是否要做資料前處理轉換</li><li>download = 是否要下載</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load mnist set</span></span><br><span class="line">train_dataset = datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=transform,</span><br><span class="line">    download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">test_dataset = datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=transform,</span><br><span class="line">    download=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="dataloader-的設置">DataLoader 的設置</h3><ul><li>在 MNIST 的例子中<ul><li>dataset = 要輸進去的 data</li><li>batch_size = 一次要多少個 batch</li><li>shuffle = 每個 epoch 是否要洗牌</li><li>num_workers = 一次有多少 CPU 執行緒來處理</li><li>pin_memory = 會使用 GPU 處理</li><li>collate_fn = 是怎麼處理樣本的，可以自定義來實現自己想要的功能</li><li>drop_last = 如果總資料除以 batch 大小有餘數的話，還乘下不滿 batch的資料，要不要丟棄</li></ul></li><li>在以下的例子中 ** 代表把 dictionary 裡的資料按照 key: value的方式拿出來</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cuda_kwargs = &#123;</span><br><span class="line">    <span class="string">&#x27;num_workers&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;pin_memory&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&#x27;shuffle&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: batch_size</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># set loader</span></span><br><span class="line">train_loader = Data.DataLoader(train_dataset, **train_kwargs)</span><br><span class="line">test_loader = Data.DataLoader(test_dataset, **test_kwargs)</span><br></pre></td></tr></table></figure><h3 id="optimizer">optimizer</h3><ul><li>在 pytorch 有很多 optimizer 可以用</li><li>所有的 optimizer 都有 step()<ul><li>當計算好 loss 之後就用來更新所有的參數</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="scheduler">scheduler</h3><ul><li>是用來管理 learning rate，合理的 learning rate可以快速收斂，隨著訓練的進行 training rate 應該要越來越小</li><li>而 pytroch 提供了 6 種方式來使用</li></ul><h4 id="steplr">StepLR</h4><ul><li>等間隔的調整 learning rate</li><li>step 以一個 epoch 為一個單位</li></ul><h4 id="multisteplr">MultiStepLR</h4><ul><li>不一定要等間隔，按設定的間隔調整 learning rate</li></ul><h4 id="exponentiallr">ExponentialLR</h4><ul><li>按指數來調整 learning rate</li><li>lr = lr * gamma ** epoch</li></ul><h4 id="cosineannealinglr">CosineAnnealingLR</h4><ul><li>以 cosine 為週期，在每個週期最大的時候富新設置 learning rate</li></ul><h4 id="reducelronplateau">ReduceLROnPlateau</h4><ul><li>當某個指標不再變化(下降或升高)，就調整 learning rate</li></ul><h4 id="lambdalr">LambdaLR</h4><ul><li>每一組 epoch 就是一個學習的策略</li><li>每個 epoch 裡都是酪 lambda function</li></ul><h3 id="model-save-load">model save &amp; load</h3><h4 id="state_dist">state_dist</h4><ul><li>是比較推的做法，只存模型裡面的參數，可以加速載入的速度</li><li>使用 torch.save() 保存 state_dict()</li><li>記住一定要用 model.eval() 來固定 drop以及歸一化層，不然每次的結果都不一樣</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><h4 id="整個-model">整個 model</h4><ul><li>比較直觀的做法，但因為是整個 model 比較大</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, PATH)</span><br><span class="line">model = torch.load(PATH)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><h4 id="checkpoint">Checkpoint</h4><ul><li>除了 model 的 state_dict() 之外，有時也可以同時存一些其它的參數</li><li>像是 epoch 數，optimizer 也會有 state_dict() 的值，loss 值等等…</li><li>值都是用 dict 來存，所以要存取參數也要用 checkpoint['something']的方法</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line">            <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">            <span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line">            <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">            <span class="string">&#x27;loss&#x27;</span>: loss,</span><br><span class="line">            ...</span><br><span class="line">            &#125;, PATH)</span><br><span class="line">            </span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line"> </span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">loss = checkpoint[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line"> </span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><h3 id="batch-裡的設置">batch 裡的設置</h3><ul><li>optimizer.zero_grad()<ul><li>初始化梯度為 0</li></ul></li><li>output = Net(model)<ul><li>也就是求出 forward propagation</li></ul></li><li>loss = critertion(output, target)<ul><li>算出 loss</li></ul></li><li>loss.backward()<ul><li>也就是求出 backward propagation</li></ul></li><li>optimizer.step()<ul><li>更新參數</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">output = Net(model)</span><br><span class="line">loss = critertion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h3 id="loss-function">loss function</h3><ul><li>常見的有以下幾種</li></ul><h4 id="log_softmax">log_softmax</h4><ul><li>就是 log 和 softmax 一起做</li><li>而 softmax 的公式如下：<ul><li>把每個值做 exponential ，再除以全部 exponential 加總的值</li><li>值會在 0 ~ 1 之間</li></ul></li></ul><p><span class="math display">\[softmax(x_i) = \frac{exp(x_i)}{\Sigma exp(x_i)}\]</span></p><ul><li>那 log_softmax 就只是再加上個 log</li><li>值為 <span class="math inline">\(-\infin\)</span> ~ 0</li></ul><p><span class="math display">\[softmax(x_i) = log(\frac{exp(x_i)}{\Sigma exp(x_i)})\]</span></p><ul><li>dim = 0 代表列總合為 1</li><li>dim = 1 代表行總合為 1</li></ul><h4 id="nll_loss">nll_loss</h4><ul><li>negative log likelihood loss</li><li>把 softmax 的結果中每一個 label 的數值拿出來<br /></li><li>取絕對值相加求平均就是 nll_loss</li></ul><h4 id="cross_entropy">cross_entropy</h4><ul><li><p>在計算兩個向量的相似度，計算期望向量與實際向量的相似度</p></li><li><p>與內積相同，內積也可以看做在做相似度</p></li><li><p>在二元的世界中公式可以表示成： <span class="math display">\[y . p(f(x)) + (1-y) . (1 - p(f(x)))\]</span></p></li><li><p>接著要取 log</p></li></ul><p><span class="math display">\[y . ln(p(f(x))) + (1-y) . ln(1 - p(f(x)))\]</span></p><ul><li>而因為算出來是機率，越大越相似</li><li>所以加個負號才能代表 loss 的精神 -&gt; 越小越好</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;Pytorch
已經成為深度學習的主流框架了，以下系列是我從大學時期，一路學習 Pytorch
筆記整理下來的心得，後來再經整理整理決定放到網路上給大家參考，希望可以幫助到更多從
0 開始想自學的人 ~&lt;/p&gt;
&lt;p&gt;整個系列會由淺到深，一路從最基本的 SOP，到 Pytorch
底層的實作邏輯，以下先以深度學習界的 Hallo World mnist 來開始學習&lt;/p&gt;
&lt;p&gt;keywords: mnist</summary>
    
    
    
    <category term="Pytorch 大補包" scheme="https://mushding.space/categories/Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85/"/>
    
    
    <category term="Pytorch" scheme="https://mushding.space/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Marp 踩坑心得</title>
    <link href="https://mushding.space/2022/12/27/Marp-%E8%B8%A9%E5%9D%91%E5%BF%83%E5%BE%97/"/>
    <id>https://mushding.space/2022/12/27/Marp-%E8%B8%A9%E5%9D%91%E5%BF%83%E5%BE%97/</id>
    <published>2022-12-27T04:31:48.000Z</published>
    <updated>2022-12-28T16:06:15.824Z</updated>
    
    <content type="html"><![CDATA[<p>還在為著 PPT要調圖片調一整天而困擾嗎？還在為著不知道要怎麼排版比較好嗎？推薦Marp，它超好用的！</p><p>keywords: Marp <span id="more"></span></p><h2 id="安裝">安裝</h2><ul><li>在 vscode 上面下載 Marp for VS Code 套件</li><li><img src="https://i.imgur.com/PfQWrZg.png"alt="image-20221004210811691" /></li></ul><h2 id="自定義-theme">自定義 Theme</h2><ul><li><p>下載別人寫好的 css 檔案</p></li><li><p>將目標投影片的 .md 檔與 css 檔案放在同一個資料夾底下</p></li><li><p>並使用 vscode 開起對應資料夾</p></li><li><p>按下 <code>ctrl + ,</code>進入設定，在搜尋列輸入：<code>marp: theme</code></p></li><li><p>按下新增項目，輸入 css 的檔案路徑 (記得要加 ./)</p></li><li><p><img src="https://i.imgur.com/Hglmg6p.png"alt="image-20221004211013113" /></p></li><li><p>享受別人的成果吧！</p></li><li><div class="sourceCode" id="cb1"><preclass="sourceCode yaml"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="at">  ---</span></span><span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">marp</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span><span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">theme</span><span class="kw">:</span><span class="at"> olive</span></span><span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="at">  ---</span></span></code></pre></div></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;還在為著 PPT
要調圖片調一整天而困擾嗎？還在為著不知道要怎麼排版比較好嗎？推薦
Marp，它超好用的！&lt;/p&gt;
&lt;p&gt;keywords: Marp</summary>
    
    
    
    <category term="雜開發心得" scheme="https://mushding.space/categories/%E9%9B%9C%E9%96%8B%E7%99%BC%E5%BF%83%E5%BE%97/"/>
    
    
    <category term="Marp" scheme="https://mushding.space/tags/Marp/"/>
    
  </entry>
  
  <entry>
    <title>什麼是 python Pass by Assignment？</title>
    <link href="https://mushding.space/2022/12/27/%E4%BB%80%E9%BA%BC%E6%98%AF-python-Pass-by-Assignment%EF%BC%9F/"/>
    <id>https://mushding.space/2022/12/27/%E4%BB%80%E9%BA%BC%E6%98%AF-python-Pass-by-Assignment%EF%BC%9F/</id>
    <published>2022-12-27T04:06:11.000Z</published>
    <updated>2023-08-24T07:21:35.855Z</updated>
    
    <content type="html"><![CDATA[<p>這幾天在刷題寫 leetcode，用最方便的 python 語言來刷，在寫到 <ahref="https://leetcode.com/problems/combination-sum/">39. CombinationSum</a> 這題的時候，為了要滿足遞迴的條件，所以在遞迴函式中加入當前答案<code>res=list()</code> 當做參數，結果發現…這個 List面試的數值怎麼一直變來變去啦…明明我沒有去動到它的阿…</p><p>keywords: Pass by Assignment <span id="more"></span></p><p>以下是犯人 code：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.ans = <span class="built_in">list</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combinationSum</span>(<span class="params">self, candidates: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span></span><br><span class="line">        </span><br><span class="line">        res = <span class="built_in">list</span>()</span><br><span class="line">        res.append(candidates[<span class="number">0</span>])</span><br><span class="line">        self.findSum(res, candidates, target)</span><br><span class="line"></span><br><span class="line">        candidates = candidates[<span class="number">1</span>:]</span><br><span class="line">        res.pop()</span><br><span class="line">        res.append(candidates[<span class="number">0</span>])</span><br><span class="line">        self.findSum(res, candidates, target)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.ans</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findSum</span>(<span class="params">self, res, candidates, target</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> candidates:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(res) == target:</span><br><span class="line">            self.ans.append(res)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(res) &gt; target:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        res.append(candidates[<span class="number">0</span>])</span><br><span class="line">        self.findSum(res, candidates, target)</span><br><span class="line"></span><br><span class="line">        candidates = candidates[<span class="number">1</span>:]</span><br><span class="line">        res.pop()</span><br><span class="line">        res.append(candidates[<span class="number">0</span>])</span><br><span class="line">        self.findSum(res, candidates, target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>發生事情當下我心想：該不會是發生了 call by reference吧…，所以特別花了一點時間來研究一下，python 中傳值的方式倒底是怎麼傳</p><h3 id="call-by-value-vs-call-by-reference">Call by Value vs Call byReference</h3><p>在開始講 python 是怎麼傳值前，我們要先來了解在 python中是如何定義資料型態的，以下的例子我會舉 JS 當成是另一個語言來做比較</p><p>在 JS (以及一部份語言) 中所有的資料型態分為兩種：primitive type 以及object type</p><p>所謂的 primitive type (中文翻原始型別)，在 JS 中有以下類別：</p><ul><li>String</li><li>Number (int, float, ...)</li><li>Boolean</li><li>Null</li><li>Undefined</li></ul><p>而大部份你看得到的「其它」類別，都是 object type，也就是都是由一個object 定義出來的</p><ul><li>Object</li><li>Function</li><li>Array</li><li>Set</li></ul><p>而在 JS 中：primitive type 全部都是 call byvalue，也就是傳進函式前，complier會先幫你在記憶體中新增一塊不同位置，但是值相同</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">call_by_value</span>(<span class="params">x</span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(x);    <span class="comment">// 5 </span></span><br><span class="line">  x = <span class="number">1</span>;</span><br><span class="line">  <span class="built_in">console</span>.log(x);    <span class="comment">// 1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> x = <span class="number">5</span>;</span><br><span class="line"><span class="built_in">console</span>.log(x);      <span class="comment">// 5 (進入函數前)</span></span><br><span class="line">call_by_value(x);</span><br><span class="line"><span class="built_in">console</span>.log(x);      <span class="comment">// 5 (雖然進入函數後 x 有更動到，但因在 function 內是其它記憶體，所以外部值沒變)</span></span><br></pre></td></tr></table></figure><p>其它的 Object type 則是 call byreference，也就是不管是在函式內外，變數指向的記憶體位置都是一樣的，所以在call_by_reference() 函式內的改動，會影響到函式外的變數：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> person = &#123;</span><br><span class="line">  <span class="attr">name</span>: <span class="string">&#x27;John&#x27;</span>,</span><br><span class="line">  <span class="attr">age</span>: <span class="number">25</span>,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">call_by_reference</span>(<span class="params">obj</span>) </span>&#123;</span><br><span class="line">  obj.age += <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">console</span>.log(preson);    <span class="comment">// &#123; name: &#x27;John&#x27;, age: 25 &#125;</span></span><br><span class="line">increaseAge(person);</span><br><span class="line"><span class="built_in">console</span>.log(person);    <span class="comment">// &#123; name: &#x27;John&#x27;, age: 26 &#125;</span></span><br></pre></td></tr></table></figure><h3 id="mutable-vs-immutable">Mutable vs Immutable</h3><p>而在 python 中比較不一樣的是，python 中所有的型別都是一個物件(object)，每一個資料都會有一個 __class__ 屬性來看看是由哪一個 class所生成的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(x.__class__)   <span class="comment"># &lt;class &#x27;int&#x27;&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x))       <span class="comment"># &lt;class &#x27;int&#x27;&gt;     // 與上面的程式等價</span></span><br></pre></td></tr></table></figure><p>同時也可以使用 id 來看看這個變數是放在記憶中哪一個位置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(x))         <span class="comment"># 4342041840</span></span><br></pre></td></tr></table></figure><p>因為每一個型別都是一個物件，所以不會像其它語言的分法一樣，在 python中是透過是不是 mutable 來區分的。根據網路上別人留言的定義 mutable的意思是</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// a mutable object is an object that can be changed</span><br><span class="line">// while an immutable object can&#x27;t be changed</span><br></pre></td></tr></table></figure><p>下表是 python 中常見型別是不是 mutable/immutable 的表格。<ahref="https://medium.com/@meghamohan/mutable-and-immutable-side-of-python-c2145cf72747">圖片參考自Mutable vs Immutable Objects in Python</a></p><p><img src="https://i.imgur.com/UPYZbbs.png" alt="image-20221206110751263" style="zoom:50%;" /></p><p>接下來詳細介紹這兩個的差別：</p><p>immutable 物件一旦被創造出來，它的值就永遠不可以再更改，像是int、float、string、bool、<strong>tuple</strong>。用下面簡單的例子來舉例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(x))    <span class="comment"># 4334161232</span></span><br><span class="line"></span><br><span class="line">x = <span class="number">200</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(x))    <span class="comment"># 4334161264</span></span><br></pre></td></tr></table></figure><p>可以發現因為 x 是 int 型別，是屬於 immutable，一旦值發生改變 python是直接會再找一塊新的記憶體來存 x 變數，而非修改原本記憶體的值</p><p>不知道大家看到這邊的時候有沒有覺得很奇怪，為什麼 python要這麼沒有效率的一直新增記憶體空間阿？我們不訪試著想想看，如果今天是在 C中我們重複定義了一個變數會發生什麼事：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> x = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">200</span>;    <span class="comment">// error: redefinition of ‘x’</span></span><br></pre></td></tr></table></figure><p>它會噴重複定義的錯，因為 x所在的記憶體位置已經被使用了，當我們想再定義一次時，complier會提醒我們不能這麼做。但是有沒有想過，為什麼 python 可以這麼做呢？python雖然少了型別 (int) 的部份，但還是可以達成下面程式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">100</span></span><br><span class="line">x = <span class="number">200</span></span><br></pre></td></tr></table></figure><p>其實我們之所以可以在 python 裡面執行這種操作，就是因為每當使用<code>=</code> 去 assign 一個變數時，python都會在記憶體中新增一塊位置存放它，也就是說其實這兩個 x根本是不一樣的東西，而這也是 immutable 的精神所在：值絕對不會被更改</p><p>如果用圖片的方式來表達的話，python 中的執行方式，就會如下圖所式：</p><p><img src="https://i.imgur.com/Mu0Wonv.png" alt="image-20221227121240682" style="zoom:67%;" /></p><p>到這邊就引出這篇文章最重要的想法：這種每當有 assignment發生時，immutable 型別所指向的記憶體都會改變，這種方法在 <ahref="https://docs.python.org/3/faq/programming.html#how-do-i-write-a-function-with-output-parameters-call-by-reference">pythonofficial document</a> 中稱作：<strong>Pass by Assignment</strong>。</p><p>我們現在來看看如果把 Pass by Assignment 的想法加上 function會發生什麼事，以下範例我們將 immutable 變數當成參數傳進 function：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">x, y</span>):</span></span><br><span class="line">  x = <span class="number">5</span></span><br><span class="line">  y = y + <span class="number">1</span></span><br><span class="line">  <span class="built_in">print</span>(x, y)    <span class="comment"># 4 24</span></span><br><span class="line">  </span><br><span class="line">a = <span class="number">10</span></span><br><span class="line">b = <span class="number">20</span></span><br><span class="line"><span class="built_in">print</span>(a, b)      <span class="comment"># 10 20</span></span><br><span class="line"></span><br><span class="line">fun(a, b)</span><br><span class="line"><span class="built_in">print</span>(a, b)      <span class="comment"># 10 20</span></span><br></pre></td></tr></table></figure><p>當 a b immutable 變數傳進 function 時 python會像其它的語言一樣，新增一塊記憶體並且 copy變數的值到這個記憶體中，不管在 function 中的任何操作都不會影響到外面 a,b 變數。</p><p>也就是說 python 的 immutable 型別在 Pass by Assignment中，很像在其它語中稱作的 Call by Value，記憶體操作如下圖表示：</p><p><img src="https://i.imgur.com/HMMzAbJ.png" alt="image-20221227121526401" style="zoom:50%;" /></p><p>接下來是換 mutable 的部份。mutable型別的有：list、set、dict，這些型別如同 mutable的意義一樣：是可以在宣告後修改的，也就是在同一個記憶體位置中修改存的值，我們用以下的範例來看看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">my_list = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(my_list))     <span class="comment"># 4337133824</span></span><br><span class="line"></span><br><span class="line">my_list[<span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(my_list))     <span class="comment"># 4337133824</span></span><br><span class="line"></span><br><span class="line">my_list.append(<span class="number">900</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(my_list))     <span class="comment"># 4337133824</span></span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/oL5rrrg.png" alt="image-20221227121550340" style="zoom:50%;" /></p><p>可以發現不管我們對 my_list 做：修改值、append 等操作，id(my_list)都是不會變的，而正是 mutable的主要表現：可以在相同記憶體下修改其中的值</p><p>那如果 mutable 型別遇上 function會發生什麼事呢？看看下面程式的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">x, y</span>):</span></span><br><span class="line">  x.append(<span class="number">30</span>)</span><br><span class="line">  y[<span class="string">&#x27;id&#x27;</span>] = <span class="number">10</span></span><br><span class="line">  <span class="built_in">print</span>(x, y)    <span class="comment"># [10, 20, 30]</span></span><br><span class="line">   <span class="comment"># &#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;age&#x27;: 16, &#x27;id&#x27;: 10&#125;</span></span><br><span class="line">  </span><br><span class="line">a = [<span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line">b = &#123;</span><br><span class="line">  <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;John&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;age&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">print</span>(a, b)      <span class="comment"># [10, 20]</span></span><br><span class="line"> <span class="comment"># &#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;age&#x27;: 16&#125;</span></span><br><span class="line"></span><br><span class="line">fun(a, b)</span><br><span class="line"><span class="built_in">print</span>(a, b)      <span class="comment"># [10, 20, 30]</span></span><br><span class="line">   <span class="comment"># &#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;age&#x27;: 16, &#x27;id&#x27;: 10&#125;</span></span><br></pre></td></tr></table></figure><p>可以發現，當 a b 傳到 function 後，當 function 內部的 x y修改值後，外部的 a b 同時也會一起修改</p><p>當 a b mutable 變數傳進 function 時，python 會將 function 內的變數 x指向 a 所指的記憶體位置，使得在 function 內修改值時，function外也會同時被修改 (因為就是同一個東西)</p><p>也就是說 python 的 mutable 型別在 Pass by Assignment中，很像在其它語言中稱作的 Call by Reference，記憶體操作如下圖表示：</p><p><img src="https://i.imgur.com/gFS3AVQ.png" alt="image-20221227121609941" style="zoom:50%;" /></p><p>但是與正常 Call by Reference 不一樣的是，如果我們在 function 內是用assignment 重新給定一個 mutable 變數值時，python 會像 Call by Value一樣重新找一塊新的記憶體放，而 function 內外的值互不相影響，如下圖：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">x</span>):</span></span><br><span class="line">  x = [<span class="number">0</span>]</span><br><span class="line">  <span class="built_in">print</span>(<span class="built_in">id</span>(x)) <span class="comment"># 4337133860</span></span><br><span class="line">  </span><br><span class="line">a = [<span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line"><span class="built_in">print</span>(a)      <span class="comment"># [10, 20]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))  <span class="comment"># 4337133824</span></span><br><span class="line"></span><br><span class="line">fun(a)</span><br><span class="line"><span class="built_in">print</span>(a)      <span class="comment"># [10, 20]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))  <span class="comment"># 4337133824</span></span><br></pre></td></tr></table></figure><p><img src="https://i.imgur.com/nQ3pNSh.png" alt="image-20221227121638838" style="zoom:50%;" /></p><p>可以發現當 <code>x = [0]</code> 後 python竟然是重新找一個記憶體去存放，所以當然也不會動到 a 裡面的值，而正是python Call by Assignment 最要留意的一個點，它並非「完全的」Call byReference 喔 ~</p><h3 id="reference">Reference</h3><p><ahref="https://medium.com/@jobboy0101/js%E5%9F%BA%E7%A4%8E-primitive-type-v-s-object-types-f88f7c16f225">JS基礎：Primitivetype v.s Object types</a></p><p><ahref="https://www.maxlist.xyz/2021/01/26/python-immutable-mutable-objects/">(圖片主要參考來源)[Python 基礎教學] 什麼是 Immutable &amp; Mutable objects</a></p><p><ahref="https://luka.tw/Python/%E5%9F%BA%E7%A4%8E%E6%95%99%E5%AD%B8/past/2021-09-21-is-python-call-by-sharing-122a4bf5a956/">(推薦說得很清楚！)【 Python 教學 】什麼是 Pass By Assignment？</a></p><p><ahref="https://docs.python.org/3/faq/programming.html#how-do-i-write-a-function-with-output-parameters-call-by-reference">官方Call by Assignment Document</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;這幾天在刷題寫 leetcode，用最方便的 python 語言來刷，在寫到 &lt;a
href=&quot;https://leetcode.com/problems/combination-sum/&quot;&gt;39. Combination
Sum&lt;/a&gt; 這題的時候，為了要滿足遞迴的條件，所以在遞迴函式中加入當前答案
&lt;code&gt;res=list()&lt;/code&gt; 當做參數，結果發現…這個 List
面試的數值怎麼一直變來變去啦…明明我沒有去動到它的阿…&lt;/p&gt;
&lt;p&gt;keywords: Pass by Assignment</summary>
    
    
    
    <category term="雜開發心得" scheme="https://mushding.space/categories/%E9%9B%9C%E9%96%8B%E7%99%BC%E5%BF%83%E5%BE%97/"/>
    
    
    <category term="python" scheme="https://mushding.space/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>如何在 docker container 中 matplotlib 顯示中文？</title>
    <link href="https://mushding.space/2022/12/27/%E5%A6%82%E4%BD%95%E5%9C%A8-docker-container-%E4%B8%AD-matplotlib-%E9%A1%AF%E7%A4%BA%E4%B8%AD%E6%96%87%EF%BC%9F/"/>
    <id>https://mushding.space/2022/12/27/%E5%A6%82%E4%BD%95%E5%9C%A8-docker-container-%E4%B8%AD-matplotlib-%E9%A1%AF%E7%A4%BA%E4%B8%AD%E6%96%87%EF%BC%9F/</id>
    <published>2022-12-27T03:58:39.000Z</published>
    <updated>2022-12-27T04:03:04.194Z</updated>
    
    <content type="html"><![CDATA[<p>一般來說 matplotlib 在產生 figure時，所套用的字體並未包含中文，所以如果要在 figure中顯示中文，我們勢必要特別指定一個字體給它</p><p>keywords: docker、matplotlib、中文 <span id="more"></span></p><h3 id="下載字體">下載字體</h3><p>首先要來下載喜歡的字體，選一個自己喜歡的就可以了，這裡我是選 google開源的 Noto 繁中字體</p><p><ahref="https://fonts.google.com/noto/specimen/Noto+Sans+TC">https://fonts.google.com/noto/specimen/Noto+Sans+TC</a></p><p>將下載後的檔案解壓縮，選一個自己喜歡的字體組細，這邊我是選<code>NotoSansTC-Medium.otf</code></p><h3 id="加到-matplotlib-裡面">加到 matplotlib 裡面</h3><p>進入到 docker container 中 (使用 vscode ssh container 或是指令 dockerexec -it ... 都可以)，到 container 的根目錄中 <code>/</code></p><p>找到並進入以下路徑：<code>/opt/conda/lib/python3.7/site-packages/matplotlib</code>。上述路徑是matplotlib 存放在 docker container 中的位置</p><p>接著再找到以下資料夾：<code>mpl-data/fonts/ttf/</code>，這是字體存放的地方，把剛剛下載好的<code>NotoSansTC-Medium.otf</code> 上傳至這個資料夾中</p><h3 id="修改-matplotlib-設定檔">修改 matplotlib 設定檔</h3><p>在剛剛的 <code>mpl-data</code>資料夾中，找到一個名稱叫：<code>matplotlibrc</code> 的設定檔，打開它</p><p>找到一下程式，把以下兩行的註解拿掉，並在 font.serif 的第一個<code>,</code> 前加入剛剛上傳的字體名稱</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">font.family:</span>  <span class="string">sans-serif</span>    <span class="comment"># &lt;- 拿掉註解</span></span><br><span class="line"><span class="comment">#font.style:   normal</span></span><br><span class="line"><span class="comment">#font.variant: normal</span></span><br><span class="line"><span class="comment">#font.weight:  normal</span></span><br><span class="line"><span class="comment">#font.stretch: normal</span></span><br><span class="line"><span class="comment">#font.size:    10.0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 加入上傳字體名稱</span></span><br><span class="line"><span class="attr">font.serif:</span>     <span class="string">NotoSansTC-Medium,</span> <span class="string">DejaVu</span> <span class="string">Serif,</span> <span class="string">Bitstream</span> <span class="string">Vera</span> <span class="string">Serif,</span> <span class="string">Computer</span> <span class="string">Modern</span> <span class="string">Roman,</span> <span class="string">New</span> <span class="string">Century</span> <span class="string">Schoolbook,</span> <span class="string">Century</span> <span class="string">Schoolbook</span> <span class="string">L,</span> <span class="string">Utopia,</span> <span class="string">ITC</span> <span class="string">Bookman,</span> <span class="string">Bookman,</span> <span class="string">Nimbus</span> <span class="string">Roman</span> <span class="string">No9</span> <span class="string">L,</span> <span class="string">Times</span> <span class="string">New</span> <span class="string">Roman,</span> <span class="string">Times,</span> <span class="string">Palatino,</span> <span class="string">Charter,</span> <span class="string">serif</span>    <span class="comment"># &lt;- 拿掉註解</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#font.sans-serif: DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif</span></span><br></pre></td></tr></table></figure><h3 id="修改-python-程式">修改 python 程式</h3><p>接著回到程式中，我們要在建立一個 figure物件後，調整一些字體設定，使用 <code>plt.rcParams</code>來修改字體及字體大小</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create font</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt font setting</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = FONT_SIZE</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;NotoSansTC-Medium&#x27;</span>]</span><br></pre></td></tr></table></figure><p>接著就可以跑原本寫的程式啦 ~ ……嗎？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(<span class="string">&#x27;save.png&#x27;</span>)</span><br></pre></td></tr></table></figure><p>這個時候會發現…跑了上面存圖片的程式，生出來的中文字還是顯示不出來……為什麼呢？</p><h3 id="加到-cache-中">加到 cache 中</h3><p>原來 matplotlib 會在 <code>/root/.cache/matplotlib</code> 中新增cache，所有的設定優先會從邊尋找，所以我們剛剛這麼大費周章的修改，結果對matplotlib 來講跟本沒差…</p><p>所以現在我們要手動修改 cache 中的檔案，打開<code>/root/.cache/matplotlib/fontlist-v330.json</code> 檔案</p><p>在程式的最下面新增這一些東西：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">    ...</span><br><span class="line">...</span><br><span class="line">&#123;</span><br><span class="line">      <span class="attr">&quot;fname&quot;</span>: <span class="string">&quot;fonts/ttf/NotoSansTC-Medium.otf&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;NotoSansTC-Medium&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;style&quot;</span>: <span class="string">&quot;italic&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;variant&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;weight&quot;</span>: <span class="number">400</span>,</span><br><span class="line">      <span class="attr">&quot;stretch&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;size&quot;</span>: <span class="string">&quot;scalable&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;__class__&quot;</span>: <span class="string">&quot;FontEntry&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">&#123;</span><br><span class="line">      <span class="attr">&quot;fname&quot;</span>: <span class="string">&quot;/usr/share/fonts/truetype/dejavu/NotoSansTC-Medium.otf&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;NotoSansTC-Medium&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;style&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;variant&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;weight&quot;</span>: <span class="number">400</span>,</span><br><span class="line">      <span class="attr">&quot;stretch&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;size&quot;</span>: <span class="string">&quot;scalable&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;__class__&quot;</span>: <span class="string">&quot;FontEntry&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;__class__&quot;</span>: <span class="string">&quot;FontManager&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="重新整理">重新整理</h3><p>最後最後，也是最重要也最容易忘記的一步，就是 <strong>重開container！</strong>，剛剛新增了這麼多東西如果不給它重新整理一下，這個設定是不會生效的！</p><p>下 <code>docker container restart</code> 重起 container後就大功告成啦！！！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;一般來說 matplotlib 在產生 figure
時，所套用的字體並未包含中文，所以如果要在 figure
中顯示中文，我們勢必要特別指定一個字體給它&lt;/p&gt;
&lt;p&gt;keywords: docker、matplotlib、中文</summary>
    
    
    
    <category term="雜開發心得" scheme="https://mushding.space/categories/%E9%9B%9C%E9%96%8B%E7%99%BC%E5%BF%83%E5%BE%97/"/>
    
    
    <category term="docker" scheme="https://mushding.space/tags/docker/"/>
    
    <category term="matplotlib" scheme="https://mushding.space/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Global-Local Temporal Representations For Video Person Re-Identification - 融合 Dilated 與 Self-Attention 的空間時間注意力</title>
    <link href="https://mushding.space/2022/07/18/Global-Local-Temporal-Representations-For-Video-Person-Re-Identification-%E8%9E%8D%E5%90%88-Dilated-%E8%88%87-Self-Attention-%E7%9A%84%E7%A9%BA%E9%96%93%E6%99%82%E9%96%93%E6%B3%A8%E6%84%8F%E5%8A%9B/"/>
    <id>https://mushding.space/2022/07/18/Global-Local-Temporal-Representations-For-Video-Person-Re-Identification-%E8%9E%8D%E5%90%88-Dilated-%E8%88%87-Self-Attention-%E7%9A%84%E7%A9%BA%E9%96%93%E6%99%82%E9%96%93%E6%B3%A8%E6%84%8F%E5%8A%9B/</id>
    <published>2022-07-18T13:32:08.000Z</published>
    <updated>2022-07-18T15:03:50.534Z</updated>
    
    <content type="html"><![CDATA[<p>本篇論文目標同樣為：在一影片序列中，找出不同幀影像間的注意力，來區分重要與不重要的特徵幀，藉此來強化網路的結果</p><p><ahref="https://arxiv.org/abs/1908.10049">https://arxiv.org/abs/1908.10049</a></p><p>keywords: Dilated Convolution、Self-Attention、GLTR <span id="more"></span></p><h2 id="introduction">Introduction</h2><p>作者認為在一個影片序列中有兩個重要特徵：short-term temporal短期的關連性，目標在相鄰幀中找到相似的行人；long-term temporal長期的關連性，目標在兩較遠的幀中找出關連，使得可以解決行人遮擋或影片雜訊等問題</p><p>為了達成上述目標，作者提出了 Global-Local Temporal Representation(GLTR) 架構，其中包含了兩個子架構，Dilated Temporal Pyramid (DTP)架構來達成 short-term temporal 短期的關連性； Temporal Self-Attention(TSA) 架構來達成 long-term temporal 長期的關連性，藉著結合：Dilated Conv以及 Self-Attention 作者在結果上取得了不錯的成績 (MARS 87.02% Rank-1Accuracy)</p><p>作者提到在這之前有人使用 3D Conv的方法來解決影片資料的問題，作者認為這樣子的方法有幾個缺點：運算時間大、沒有很有邏輯的去分析空間中的注意力</p><h2 id="網路架構">網路架構</h2><h3 id="backbone">Backbone</h3><p>作者使用 ResNet-50 作為主架構，先將影片拆分出所有的幀，將二維的影像<span class="math inline">\(H\times W\times d\)</span>先經骨幹網路學習，再把結果 reshape 成類似三維的 <spanclass="math inline">\(H\times W\times d\times T\)</span></p><p><img src="https://i.imgur.com/kII3QqO.png"alt="image-20220718143905285" /></p><p>詳細的做法為：一幀影像大小為 <span class="math inline">\(H\timesW\times d\)</span> ，一共有 N 個幀 <span class="math inline">\(N \timesH\times W\times d\)</span> ，再加上 Batch，最後再 reshape 一下得到 <spanclass="math inline">\(BN\times D\times H\times W\)</span>的輸入表示，這個維度可以理解為把 Batch 與幀數視為相同一個維度，Batch假設是 10，影片假設有 10 幀，則一次放進網路的二維影像總數就是 10x10 =100 張。最後再把維度 reshape 回原 <span class="math inline">\(B\timesN\times D\times H\times W\)</span>。利用這個方法就不會因多一個維度需要3D Conv 了。</p><h3 id="dilated-temporal-pyramid-convolution">Dilated Temporal PyramidConvolution</h3><p>作者在架構中引入 Dilated Convolution 擴張卷積，利用其 rate <spanclass="math inline">\(r\)</span>的特色，可以在不改變解釋度、不增加運算量的前提下，增加網路的 receptivefield 視野。當 <span class="math inline">\(r\)</span>越大除了可看為視野越大外也可理解為「兩相鄰幀時間隔離變遠」，越大越long-term</p><p>同時也引入了 FPN 金字塔網路的概念，設計不同的 rate 最後用 concat融合在一起，也就是把 short-term 與 long-term合併特徵，使得網路有更豐富的資訊</p><h3 id="temporal-self-attention">Temporal Self Attention</h3><p>將剛剛得到不同視野 (時間長短) 的特徵圖做Self-Attention，作者設計的很剛好，FPN 的金字塔層數是 3 剛好對應Self-Attention 要切成 QKV 三份。三個不同視野 (時間長短)的特徵圖彼此做重要度分析，最後經一 average pooling 得到最後的結果</p><h2 id="實驗結果">實驗結果</h2><p>DTP TSA 的一些 Ablation study，可發現兩個都加上效果最好</p><p><img src="https://i.imgur.com/maZMfUA.png"alt="image-20220718212848428" /></p><p>SOTA 表，其中 STA 為上一篇的論文架構名稱</p><p><img src="https://i.imgur.com/FLP2FaR.png"alt="image-20220718213016057" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本篇論文目標同樣為：在一影片序列中，找出不同幀影像間的注意力，來區分重要與不重要的特徵幀，藉此來強化網路的結果&lt;/p&gt;
&lt;p&gt;&lt;a
href=&quot;https://arxiv.org/abs/1908.10049&quot;&gt;https://arxiv.org/abs/1908.10049&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;keywords: Dilated Convolution、Self-Attention、GLTR</summary>
    
    
    
    
    <category term="3D image" scheme="https://mushding.space/tags/3D-image/"/>
    
    <category term="Attention" scheme="https://mushding.space/tags/Attention/"/>
    
  </entry>
  
</feed>
