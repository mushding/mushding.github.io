<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Single Image Super-Resolution via a Holistic Attention Network</title>
    <url>/2021/06/26/Single%20Image%20Super-Resolution%20via%20a%20Holistic%20Attention%20Network/</url>
    <content><![CDATA[<p>在 super resolution 超高解析度影像問題中，此篇作者認為 LR 至 HR
小細節常常會「平滑化」的原因是，在超深的網路中，各個 layer
之間的資訊並不流通，因而提出了 HAN 架構。</p>
<p>keywords: HAN, LAM, CSAM</p>
<span id="more"></span>
<h1
id="single-image-super-resolution-via-a-holistic-attention-network">Single
Image Super-Resolution via a Holistic Attention Network</h1>
<h2 id="abstract">Abstract</h2>
<p>作者認為 channel attention 把每一個 channel
都分別來開單獨計算，而乎略掉了各 channel 之間的關聯性。</p>
<p>為了解決這個問題，作者提出了 HAN (Holistic attention 全局
attention)，由LAM 如 CSAM 所組成</p>
<p>LAM (Layer attention module)，可以去找各 layer 間的垂直關系</p>
<p>CSAM (Channel-Spatial attention module)，嗯…就是 Spatial attention ?
去找各特徵圖上重要的像素自</p>
<h2 id="introduction">Introduction</h2>
<ul>
<li>Single image super-resolution (SISR) 就是指單一影像變高解析度
<ul>
<li>給定一個 LR low-resolution 生成一張 HR high resolution
影像，以上問題程為 SR super-resolution 問題</li>
</ul></li>
<li>SRCNN 則是 SR 領域的開山始祖
<ul>
<li>現今大部份成功的 SR model 都是建立在 CNN 上，且使用很深的網路以及
Residual</li>
<li>超深的網路好處是，在尋找 LR 與 HR
之間複雜的對應關系非常厲害，而多虧了 Residual
的幫忙，太深的網路才不會發生梯度消失的問題</li>
</ul></li>
<li>作者發現在 LR
圖上的細節部份，像素間常常會變平滑掉，作者認為是因為乎略掉中間特徵層之間的關系所導致
<ul>
<li>雖然在有些地方用上了 channel attention 但還是乎略掉 feature 與
feature 之間的關系</li>
<li>channel attention 不能計算出各 layer
之間的權重，尤其是在淺層網路中的資訊很容易因網路深度而慢慢消失，雖然在設計中會有一個
long skip connetion
使淺層資訊得以流動到下層去，但這會使重要的下層資訊與上層資訊權重相同
(越深的網路應該越重要才對)</li>
</ul></li>
<li>作者提出了 HAN Holistic attention network
<ul>
<li>包含了 LAM 以及 CSAM</li>
<li>LAM 在尋找 multi-scale layers 之間的關系</li>
<li>CSAM 則找 channel spatial 之間的關系</li>
</ul></li>
</ul>
<h2 id="related-work">Related Work</h2>
<ul>
<li>作者說 SR 領域有兩種做法
<ul>
<li>一是傳統演算法</li>
<li>二是使用 CNN</li>
</ul></li>
<li>SRCNN -&gt; DRCN -&gt; DRRN -&gt; LapSR ....</li>
</ul>
<h2 id="han">HAN</h2>
<p><img src="https://i.imgur.com/dvcu3qi.png" alt="image-20210625135046099"  /></p>
<p>作者 backbone 的部份使用的是 RCAN，RCAN 的特色就是使用到了 RIR
(Residual in Residual) 一共包含了兩個 skip connection 一個 long skip
一個 short ，目的就是為了能使各 layer
之間的訊息能更有效的流動，不會因為深度太深的問題導致梯度消失…，以及在 RG
(Residual Group) 裡加上了 CA (channel attention) ，嗯…這個 attention
有沒有幫助嗎…是有到一點點啦</p>
<h3 id="網路架構">網路架構</h3>
<p>整個網路架構如下： 與 RCAN 相同，有兩個 skip connection
，不同的地方在於，在每個 RG 的 output 層拉出了一條線連接到 LAM
去，去尋找各 Layer 的重要性，有用的 layer 會被加強，多餘的則會被壓制，而
CSAM 的部份作者只有做最後一層，是在效果與正確率所做出的選擇
(當然可以每一層都做啦…就很慢就是了)</p>
<p>https://zhuanlan.zhihu.com/p/65469586</p>
<p><img src="https://i.imgur.com/TiuFecI.png"
alt="image-20210625161718604" /></p>
<p>最後把 LAM CSAM 與 long skip connection 三個相加，再經由一個 Upsample
層，這邊使用的是 sub-pixel conv，又稱作 pixel shuffle，如果要將原圖放大
3 倍，我們會先需要生出 3^2 個特徵圖，全部是經過 conv
轉換，最後把這些特徵圖按照順序放到原 pixel 中 (從 1 個像素變成 9
個像素)</p>
<p><img src="https://i.imgur.com/2XUg1us.png"
alt="image-20210625145232838" /></p>
<h3 id="loss-funtion">loss funtion</h3>
<p>Loss function 的部份為了與 RCAN 做比較，與原論文同樣是使用 L1
loss，把原圖 (SR) 與生成的高解析圖 (HR) 像素相減求平均</p>
<h3 id="lam">LAM</h3>
<p>以下介紹 LAM <img src="https://i.imgur.com/c4oUpQk.png"
alt="image-20210625150435548" /></p>
<p>LAM 的想法與 self-attention 有些類似，一樣特徵圖分為三分，兩分做
correlation ，得出的結果做線性加權，只是乘出來的 feature 是 NxN
，這個就是本篇論文最大的特色，是找出一個 layer 之間的 correlation
matrix，以下是用數學式子來表達</p>
<p><span class="math inline">\(\delta\)</span> 為 softmax <span
class="math inline">\(\varphi\)</span> 為 reshape T 為轉至</p>
<p><span class="math display">\[
\begin{gather}
w_{j,i} = \delta (\varphi(FG)_i \cdot (\varphi(FG))^T_i)\\
i, j=1,2,...N,\\
\end{gather}
\]</span></p>
<p>結果會用線性加權的方式回原圖，最後與 short cut
來的原圖相加，作者多設計了一個 <span
class="math inline">\(\alpha\)</span> 原始為 0
，是通過機器自己去學習出來的，也可以代表一個 layer 的重要性</p>
<p><span class="math display">\[
F_{L_j}=\alpha\sum^N_{i=1}w_{i,j}FG+FG_j
\]</span></p>
<h3 id="csam">CSAM</h3>
<p><img src="https://i.imgur.com/bUIMvHo.png" alt="image-20210625155307457" style="zoom:67%;" /></p>
<p>與傳統的方法不同，作者為了增加 channel 與 spatial
之間的相關性，直接把特徵圖做 3 維卷積，直接把 channel spatial
看成一個大整體，最後與 self-attention 相同，與自己做 element-wise
product ，最後加上原圖得到最後結果，作者認為使用 3 線卷積可以使 CSAM
學到 inter-channel 還有 intra-channel 之間的關系，也就是層與層，與
spatial 的綜合關系</p>
]]></content>
      <categories>
        <category>論文</category>
      </categories>
      <tags>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title>手把手 hexo 從零開始教學(零)</title>
    <url>/2021/06/27/%E6%89%8B%E6%8A%8A%E6%89%8B%20hexo%20%E5%BE%9E%E9%9B%B6%E9%96%8B%E5%A7%8B%E6%95%99%E5%AD%B8(%E9%9B%B6)/</url>
    <content><![CDATA[<p>因為疫情期間基本上跟本不能去實驗室，都整天待在家裡面，變成說就算到了暑假也沒什麼感覺
w
，反正每天過的生活也差不多就那樣。為了可以更能提起精神看看論文、寫寫筆記，決定用
hexo 來架一個自己的網站，上面來放一些論文筆記什麼的…，以下是 hexo
的一樣心得。</p>
<p>keywords: hexo <span id="more"></span></p>
<h1 id="手把手-hexo-從零開始教學零">手把手 hexo 從零開始教學(零)</h1>
<h2 id="hexo-常用指令">hexo 常用指令</h2>
<p>目前有三個最常用到指令 * 生成 public 檔 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo generate</span><br><span class="line">hexo g</span><br></pre></td></tr></table></figure> *
開起一個簡單的 server <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo server</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure> * 把 public 檔刪掉 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure>
都常我都會用 npm 把它們合在一起，這樣開發的時候 debug 比較方便一些
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo s --debug</span><br></pre></td></tr></table></figure></p>
<h2 id="hexo-上使用-mathjax-踩雷">hexo 上使用 mathjax 踩雷</h2>
<p>hexo 上面初始的 markdown 編釋器是
marked，但是它只能處理一些文本上的需求，如果在筆記中需要用到 mathjax
等數學符號，就要把 marked 換掉。</p>
<p>網路上有推薦說有兩個套件可以換，分別是 * hexo-renderer-pandoc *
hexo-renderer-kramed</p>
<p>兩個都對 mathjax 的支援度很高，但根據 nexT 官網的描述，以及我個人花了
2 個小時 google 的心得…，pandoc 比較推薦，kramed 就是會發生一些神奇的
bug，什麼 $$ 顯示不出來阿…</p>
<p><a
href="https://github.com/theme-next/hexo-theme-next/blob/master/docs/zh-CN/MATH.md">nexT
官網的解釋</a></p>
<p>總之以下推薦使用 pandoc 做為編釋以及一些細節的小修改：</p>
<ul>
<li>首先把 marked 移除，不然 pandoc 會跟它版本衝突 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br></pre></td></tr></table></figure></li>
<li>接著安裝 pandoc <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo</span><br></pre></td></tr></table></figure></li>
<li>接著在 nexT 的 _config.yml 中把 math 的地方 enable
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">math:</span><br><span class="line">  ...</span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true</span><br></pre></td></tr></table></figure></li>
<li>最後重新整理一下 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo s --debug</span><br></pre></td></tr></table></figure></li>
</ul>
<p>就大功告成啦啦，可以來試試看 inlineMath 跟 displayMath 有沒有成功</p>
<p><span class="math inline">\(a = b + c\)</span></p>
<p><span class="math display">\[
\begin{gathered}
    a = b + c \\
    i = 1, 2
\end{gathered}
\]</span></p>
<h2 id="調教-pandoc">調教 pandoc</h2>
<p>裝完 pandoc 後會發現一些圖片註解的地方會跑出來兩邊，嗯…對我來說有點煩
w，所以我上網找了一下發現只要在 hexo 的 _config.yml
加入一些設定就可以了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pandoc:</span><br><span class="line">  extensions:</span><br><span class="line">    - &#x27;-implicit_figures&#x27;</span><br></pre></td></tr></table></figure>
<p>還有一個問題，就是也不知道為什麼 pandoc 的 $$ 數學公式加上 \\
不會換行…，後來一查才知道，原來 mathjax 有提供多功能的數學公式排版 w
，要把那個加上去才行</p>
<p>以下列出如果要使用換行公式的做法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$$</span><br><span class="line">\begin&#123;gathered&#125;</span><br><span class="line">    ...</span><br><span class="line">\end&#123;gathered&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>
<p>以上就大功告成啦，得到一個排板漂亮，又可以顯示 mathjax 的編釋器啦</p>
<h2 id="hexo-新增文章介紹">hexo 新增文章介紹</h2>
<p>在 hexo 中所有的文章都是存在 source
這個資料夾底下，當我們要新增一個文章的時候，可以打下面這個指令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo new [layout] title</span><br></pre></td></tr></table></figure>
<p>其中這個 layout 預設是 post，而 title 就是文章的標題</p>
<p>那這個 layout 其實對應的是在 scaffolds 裡面的 .md 檔，它會去找對應的
layout 去生成出預設排版</p>
<p>像我就會把 post
改成以下的樣子，這樣每次新增一個文章的時候，就不用再多花時間打字了：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: &#123;&#123; title &#125;&#125;</span><br><span class="line">date: &#123;&#123; date &#125;&#125;</span><br><span class="line">tags:</span><br><span class="line">categories: </span><br><span class="line">mathjax: false</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">&lt;!--more--&gt;</span><br></pre></td></tr></table></figure>
<p>此外在 hexo 的 _config.yml
中也有底下這一行，更改檔案的名稱，在這邊我加上新增時間，可以更好的來管理文章。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">new_post_name: :title.md</span><br></pre></td></tr></table></figure>
<p>可以改成 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">new_post_name: :year-:month-:day-:title.md</span><br></pre></td></tr></table></figure></p>
<h2 id="hexo-首頁-discription">hexo 首頁 discription</h2>
<p>hexo 首頁會預設把你文章整坨貼上來，我們可以加上一些 discription
來簡單介紹一下文章概要，精簡版面</p>
<p>有兩種做法：</p>
<ol type="1">
<li>是用 discription</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">discription: </span><br><span class="line">---</span><br></pre></td></tr></table></figure>
<ol start="2" type="1">
<li>用 <code>&lt;!--more--&gt;</code></li>
</ol>
<h2 id="hexo-next-設定背影透明化">hexo nexT 設定背影、透明化</h2>
<p>https://hackmd.io/<span class="citation"
data-cites="Heidi-Liu/hexo-theme">@Heidi-Liu/hexo-theme</span>
https://blog.csdn.net/qq_43414603/article/details/104113198</p>
]]></content>
      <categories>
        <category>hexo 心得筆記</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>手把手 hexo 從零開始教學(一)</title>
    <url>/2021/06/30/%E6%89%8B%E6%8A%8A%E6%89%8B-hexo-%E5%BE%9E%E9%9B%B6%E9%96%8B%E5%A7%8B%E6%95%99%E5%AD%B8/</url>
    <content><![CDATA[<p>最近為了想要在家要面自己用 resperry pi
做一個網頁伺服來放個人筆記，所以花了一些時間研究了 hexo
的一些設定，以及踩了一些雷 w，以下用很簡單的內容來快速架好一個 hexo
的網頁。</p>
<p>keywords: hexo <span id="more"></span></p>
<h1 id="手把手-hexo-從零開始教學一">手把手 hexo 從零開始教學(一)</h1>
<h2 id="初使化-hexo">初使化 hexo</h2>
<p>首先我們要先下載 hexo <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure> 接著初使化 hexo 依照以下指令輸入
<code>&lt;your_hexo_dir\</code> 的部份填入資料夾的名稱 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir &lt;your_hexo_dir&gt;</span><br><span class="line">hexo init &lt;your_hexo_dir&gt;</span><br><span class="line">cd &lt;your_hexo_dir&gt;</span><br></pre></td></tr></table></figure>
這樣一個簡單的 hexo 就創立好啦</p>
<h2 id="hexo-常用指令">hexo 常用指令</h2>
<p>我們可以透過 hexo 的一些常用指令來開起 server * 生成 public 檔
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo generate</span><br><span class="line">hexo g</span><br></pre></td></tr></table></figure> * 開起一個簡單的 server <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo server</span><br><span class="line">hexo s</span><br></pre></td></tr></table></figure> * 把 public 檔刪掉
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean</span><br></pre></td></tr></table></figure> 都常我都會用 npm 把它們合在一起，這樣開發的時候 debug
比較方便一些 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo s --debug</span><br></pre></td></tr></table></figure> 接著 hexo 的網頁就會開在 <a
href="http://localhost:4000">localhost:4000</a> 的地方囉</p>
<h2 id="安裝-設定-next-主題">安裝 &amp; 設定 next 主題</h2>
<p>在 hexo 中裡面有各式各樣的主題可以選擇，在這裡我用 next
主題做試範，也因為 next
主題非常多人在使用，而且在許多第三方服務都已經包好了，使用起來資源非常多，非常方便。</p>
<p>首先先下載 next <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd &lt;your_hexo_dir&gt;</span><br><span class="line">git clone https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure></p>
<p>接著到 <code>hexo _config.yaml</code> 裡面去把 theme 修改成 next
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure></p>
<p>下載完就會發現在 theme 資料夾中多了一個 next
的新資料夾，在裡面會有一個 _config.yaml
設定檔，等等我們就要來修改裡面的值，讓我們的網頁更漂亮囉。</p>
<h3 id="修改-icon">修改 icon</h3>
<p>在 <code>theme/next/source/images</code>
中，把想要新增的圖片放上來，並且修改對應的路徑 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">favicon:</span></span><br><span class="line">  <span class="attr">small:</span> <span class="string">/images/camal.png</span></span><br><span class="line">  <span class="attr">medium:</span> <span class="string">/images/camal.png</span></span><br><span class="line">  <span class="attr">apple_touch_icon:</span> <span class="string">/images/apple-touch-icon-next.png</span></span><br><span class="line">  <span class="attr">safari_pinned_tab:</span> <span class="string">/images/camal.png</span></span><br></pre></td></tr></table></figure></p>
<h3 id="修改-footer">修改 footer</h3>
<p>footer 是網頁最底下的那一行，我們可以把它的 icon
以及一些文字修改：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">footer:</span></span><br><span class="line">  <span class="comment"># Specify the date when the site was setup. If not defined, current year will be used.</span></span><br><span class="line">  <span class="comment">#since: 2015</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Icon between year and copyright info.</span></span><br><span class="line">  <span class="attr">icon:</span></span><br><span class="line">    <span class="comment"># Icon name in Font Awesome. See: https://fontawesome.com/icons</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">fas</span> <span class="string">fa-dragon</span></span><br><span class="line">    <span class="comment"># If you want to animate the icon, set it to true.</span></span><br><span class="line">    <span class="attr">animated:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># Change the color of icon, using Hex Code.</span></span><br><span class="line">    <span class="attr">color:</span> <span class="string">&quot;#111111&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># If not defined, `author` from Hexo `_config.yml` will be used.</span></span><br><span class="line">  <span class="attr">copyright:</span> <span class="string">&quot;若要轉載文章，麻煩請保留原作者名稱與原始連結。&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Powered by Hexo &amp; NexT</span></span><br><span class="line">  <span class="attr">powered:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h3 id="設定-schemes">設定 schemes</h3>
<p>我自己比較喜歡這一個 w，把它註解掉就可以了 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Schemes</span></span><br><span class="line"><span class="comment"># scheme: Muse</span></span><br><span class="line"><span class="comment">#scheme: Mist</span></span><br><span class="line"><span class="comment">#scheme: Pisces</span></span><br><span class="line"><span class="attr">scheme:</span> <span class="string">Gemini</span></span><br></pre></td></tr></table></figure></p>
<h3 id="menu-網站分頁連結">menu &amp; 網站分頁連結</h3>
<p>這個東西會顯示在網頁的最左手邊
(如果沒設定到其它地方的話)，如果有要新增分頁，要先到 source
資料新增一個名字對應的資料夾，接著到 <code>theme _config.yaml</code>
設定 menu。設定格式為：<code>分頁連結 || 圖示</code></p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">menu:</span></span><br><span class="line">  <span class="attr">home:</span> <span class="string">/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-home</span></span><br><span class="line">  <span class="attr">about:</span> <span class="string">/about/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-user</span></span><br><span class="line">  <span class="attr">demo:</span> <span class="string">/demo/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-laptop-code</span></span><br><span class="line">  <span class="attr">tags:</span> <span class="string">/tags/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-tags</span></span><br><span class="line">  <span class="attr">categories:</span> <span class="string">/categories/</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-th</span></span><br><span class="line">  <span class="comment">#archives: /archives/ || fa fa-archive</span></span><br><span class="line">  <span class="comment">#schedule: /schedule/ || fa fa-calendar</span></span><br><span class="line">  <span class="comment">#sitemap: /sitemap.xml || fa fa-sitemap</span></span><br><span class="line">  <span class="comment">#commonweal: /404/ || fa fa-heartbeat</span></span><br></pre></td></tr></table></figure>
<h3 id="設定社交平台">設定社交平台</h3>
<p>這個就看個人想要放什麼囉 w</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">social:</span></span><br><span class="line">  <span class="attr">GitHub:</span> <span class="string">https://github.com/mushding</span> <span class="string">||</span> <span class="string">fab</span> <span class="string">fa-github</span></span><br><span class="line">  <span class="attr">E-Mail:</span> <span class="string">mailto:ajy1005464@gmail.com</span> <span class="string">||</span> <span class="string">fa</span> <span class="string">fa-envelope</span></span><br><span class="line">  <span class="comment">#Weibo: https://weibo.com/yourname || fab fa-weibo</span></span><br><span class="line">  <span class="comment">#Google: https://plus.google.com/yourname || fab fa-google</span></span><br><span class="line">  <span class="comment">#Twitter: https://twitter.com/yourname || fab fa-twitter</span></span><br><span class="line">  <span class="comment">#FB Page: https://www.facebook.com/yourname || fab fa-facebook</span></span><br><span class="line">  <span class="comment">#StackOverflow: https://stackoverflow.com/yourname || fab fa-stack-overflow</span></span><br><span class="line">  <span class="comment">#YouTube: https://youtube.com/yourname || fab fa-youtube</span></span><br><span class="line">  <span class="attr">Instagram:</span> <span class="string">https://instagram.com/mushding</span> <span class="string">||</span> <span class="string">fab</span> <span class="string">fa-instagram</span></span><br><span class="line">  <span class="comment">#Skype: skype:yourname?call|chat || fab fa-skype</span></span><br></pre></td></tr></table></figure>
<h3 id="mathjax">mathjax</h3>
<p>在另一個筆記中有更詳細的設定教學 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">math:</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">mathjax:</span></span><br><span class="line">    <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">mhchem:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure></p>
<h3 id="avatar-頭像設定">avatar 頭像設定</h3>
<p>在左邊的欄位可以加上自己的像片，在 <code>theme _config.yaml</code>
找到 avatar，而圖片的位置在 <code>theme/next/source/images</code> 中</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">avatar:</span></span><br><span class="line">  <span class="comment"># Replace the default image and set the url here.</span></span><br><span class="line">  <span class="attr">url:</span> <span class="string">/images/avatar.png</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be dispalyed in circle.</span></span><br><span class="line">  <span class="attr">rounded:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># If true, the avatar will be rotated with the cursor.</span></span><br><span class="line">  <span class="attr">rotated:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
<h3 id="其它第三方設定">其它第三方設定</h3>
<p>next 提供超多第三方可以設定的，在這邊我只使用了兩個</p>
<p>pjax 可以實現轉換分頁不重新加載 disqus 則是網頁底下的討論欄
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">pjax:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">disqus:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">shortname:</span> <span class="string">mushding-website</span></span><br><span class="line">  <span class="attr">count:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></p>
<h3 id="設定背景透明度">設定背景、透明度</h3>
<p>如果你想讓你網頁變得更文青的話，可以來修改這兩項設定</p>
<p>找到 在 <code>theme _config.yaml</code> 中找到
<code>custom_file_path</code>，並且把 style 的地方註解拿掉
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">custom_file_path:</span></span><br><span class="line">  <span class="comment">#head: source/_data/head.swig</span></span><br><span class="line">  <span class="comment">#header: source/_data/header.swig</span></span><br><span class="line">  <span class="comment">#sidebar: source/_data/sidebar.swig</span></span><br><span class="line">  <span class="comment">#postMeta: source/_data/post-meta.swig</span></span><br><span class="line">  <span class="comment">#postBodyEnd: source/_data/post-body-end.swig</span></span><br><span class="line">  <span class="comment">#footer: source/_data/footer.swig</span></span><br><span class="line">  <span class="comment">#bodyEnd: source/_data/body-end.swig</span></span><br><span class="line">  <span class="comment">#variable: source/_data/variables.styl</span></span><br><span class="line">  <span class="comment">#mixin: source/_data/mixins.styl</span></span><br><span class="line">  <span class="attr">style:</span> <span class="string">source/_data/styles.styl</span></span><br></pre></td></tr></table></figure></p>
<p>接著在 hexo 的 source 資料下新增
<code>source/_data/styles.styl</code> 檔案，在裡面填上：</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">body</span> &#123;</span><br><span class="line">    <span class="attribute">background</span>: <span class="built_in">url</span>(<span class="string">/images/bg-2.jpeg</span>);</span><br><span class="line">    <span class="attribute">background-repeat</span>: no-repeat;</span><br><span class="line">    <span class="attribute">background-attachment</span>: fixed;</span><br><span class="line">    <span class="attribute">background-size</span>: cover;</span><br><span class="line">    <span class="attribute">background-position</span>:<span class="number">50%</span> <span class="number">50%</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>透明度則是到 <code>themes/next/source/css/_variables/base.styl</code>
中</p>
<p>找到 <code>content-bg-color</code> ，且改成 rgba 的格式
<code>rgba(255, 255, 255, 0.8)</code> 就可以囉</p>
<h2 id="新增-sitemap">新增 sitemap</h2>
<p>我們可以透過 Google Search Console 來使得我們網站可以被 Google
搜尋到，增加流量。下載並生成：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-sitemap --save</span><br><span class="line">hexo d -g</span><br></pre></td></tr></table></figure>
<p>會在 public 資料夾生成一個 sitemap.xml 檔，接著去 Google Search
Console 提交就可以囉</p>
<p>會需要在 DNS 中 (我是用 cloudflare 代管) 新增一筆 TXT
Record，長這樣：</p>
<p><img src="https://i.imgur.com/TL3ul5l.png"
alt="image-20210709110129707" /></p>
<p>等一下就可以回到 Google 看看囉！</p>
<h2 id="新增本地搜尋">新增本地搜尋</h2>
<p>打開 <code>themes\next\_config.yml</code> 後，尋找
<code>local_search</code>，把 false 改成 true 就可以了</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">local_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># If auto, trigger search by changing input.</span></span><br><span class="line">  <span class="comment"># If manual, trigger search by pressing enter key or search button.</span></span><br><span class="line">  <span class="attr">trigger:</span> <span class="string">auto</span></span><br><span class="line">  <span class="comment"># Show top n results per article, show all results by setting to -1</span></span><br><span class="line">  <span class="attr">top_n_per_article:</span> <span class="number">1</span></span><br><span class="line">  <span class="comment"># Unescape html strings to the readable one.</span></span><br><span class="line">  <span class="attr">unescape:</span> <span class="literal">true</span></span><br><span class="line">  <span class="comment"># Preload the search data when the page loads.</span></span><br><span class="line">  <span class="attr">preload:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hexo 心得筆記</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>手把手 hexo 從零開始教學(二)</title>
    <url>/2021/07/02/%E6%89%8B%E6%8A%8A%E6%89%8B-hexo-%E5%BE%9E%E9%9B%B6%E9%96%8B%E5%A7%8B%E6%95%99%E5%AD%B8-%E4%BA%8C/</url>
    <content><![CDATA[<p>繼上一次我們成功的在本地端 <code>localhost</code>
做出一個靜態的網頁，在這一次的教學中，我會介紹如何把靜態網頁使用 docker
包起來佈署，以及利用域名、DNS
真的架出一個屬於個人的網站。於其它網路上的教學文不同的是，因為考慮到未來我有可能要加上動態網頁的部份，所以我沒有使用
gitpage 去做。</p>
<p>keywords: hexo, docker, DNS, cloudflare <span id="more"></span></p>
<h2 id="生成靜態資料">生成靜態資料</h2>
<p>所謂的靜態資料就是指把 js, css
經一連串編釋，變成一個最小單位的一個檔案，可以把它想像成一個程式碼都以經最佳化的濃縮檔。而這個資料夾在習慣上會取名叫
public 或者是 build
(這些都可以在設定中更改)，而我們只需要把這份檔案，不管用什麼方法把它部署上去，我們的網站就搞定了。</p>
<p>在 hexo 中，生成靜態資料的指令是：<code>hexo generate</code> 或簡寫為
<code>hexo g</code>。執行就可以看到工作目錄底下多了一份
<code>public</code> 檔，而那個就是接下來的重點了。</p>
<h2 id="docker">docker</h2>
<p>我就不在這裡多說什麼是 docker，但我為了想要移植方便因此多用一個
docker。</p>
<p>在工作目錄底下新增一個 <code>Dockerfile</code> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch Dockerfile</span><br></pre></td></tr></table></figure>
在裡面新增以下的程式： <figure class="highlight docker"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> nginx:stable-alpine</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> public /usr/share/nginx/html</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">80</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;nginx&quot;</span>, <span class="string">&quot;-g&quot;</span>, <span class="string">&quot;daemon off;&quot;</span>]</span></span><br></pre></td></tr></table></figure> 在這裡我使用 nginx
這個反向代理套件。以下是 Dockerfile 內容簡單描述： 1. 會先去 Dockerhub
上下載對應的 image 2. 把剛剛生成出來的 public 複製到 nginx
底下的資料夾，nginx 會自己去偵測它 3. 把 Docker 中的 80 port 打開 (80
port 是 http 的 port) 4. 起動 nginx</p>
<p>最後執行以下指令，等它跑了一陣子後就會建立好一個
container，並且執行喔 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker build</span><br><span class="line">docker run </span><br></pre></td></tr></table></figure></p>
<h2 id="nginx">nginx</h2>
<p>後來我還在外面再加上一層 nginx
來管理未來可能會新增前後端的需求。以下是 nginx 的 Dockerfile 內容
<figure class="highlight docker"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> nginx </span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> ./default.conf /etc/nginx/conf.d/default.conf</span></span><br></pre></td></tr></table></figure> 然後新增一個 default.conf 檔案，裡面是來設定 nginx 的。
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  listen 80;</span><br><span class="line">  listen 443;</span><br><span class="line"></span><br><span class="line">  location / &#123;</span><br><span class="line">    proxy_pass         http://frontend;</span><br><span class="line">    proxy_redirect     off;</span><br><span class="line">    proxy_set_header   Host $host;</span><br><span class="line">    proxy_set_header   X-Real-IP $remote_addr;</span><br><span class="line">    proxy_set_header   X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    proxy_set_header   X-Forwarded-Host $server_name;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>listen 80、listen 443 是來監聽這些 port 到主機上的，location
則是未來管理網址來達成重新導向的。記得要在 proxy_pass 的地方打上 Docker
中對應的 bridge 名稱，這樣 Docker 的前後端才可互相溝通。</p>
<h2 id="docker-compose">docker-compose</h2>
<p>因為我們有太多 Dockerfile 啦，還是用 docker-compose
來好好整理統一一下。</p>
<p>在最上一層資料夾新增一個 <code>docker-compose.yaml</code> 檔
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch docker-compose.yaml</span><br></pre></td></tr></table></figure> 在裡面新增對應的程式碼： <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3&#x27;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">frontend:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./hexo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&quot;hexoapp:1.0.0&quot;</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">unless-stopped</span></span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">sub-etha</span></span><br><span class="line">    <span class="attr">volumes:</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">./hexo/public:/usr/share/nginx/html</span></span><br><span class="line">  <span class="attr">revproxy:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&quot;reverseproxy:1.0.0&quot;</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">unless-stopped</span></span><br><span class="line">    <span class="attr">networks:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">sub-etha</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">443</span><span class="string">:443</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">80</span><span class="string">:80</span></span><br><span class="line">    <span class="attr">depends_on:</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">frontend</span></span><br><span class="line"><span class="attr">networks:</span></span><br><span class="line">  <span class="attr">sub-etha:</span></span><br><span class="line">    <span class="attr">driver:</span> <span class="string">bridge</span></span><br></pre></td></tr></table></figure></p>
<p>接著輸入： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose up --build</span><br></pre></td></tr></table></figure> 就會看到全部的 Dockerfile
都被我們串接在一起啦，這個時候再回到 <code>localhost</code>
可以看到我們的網頁有成功運行，(這個時候你可能會想：阿這不就跟剛剛一模一樣，同樣的網址阿效果也相同，為什麼我們還繞了一大圈用這麼多東西呢？)。其實這些準備，都是為了讓我們未來不管是擴充，或是移植到其它伺服器上時更加方便的事前準備喔。</p>
<p>接下來 build 好 container 後，我們就可以讓我們的網頁上線啦啦啦。</p>
<h2 id="域名-dns">域名 &amp; DNS</h2>
<h3 id="porkbun">porkbun</h3>
<p>現在可以看到我們的網址都是一串神秘的 ip 位置，或是
localhost，這是因為我們是把網站架在自己的電腦上的，自己的電腦連上自己架的網站非常合理，但是如果今天你的目標是要讓全世界的人都可以看到你認真經營的網站的話，就必須要有三個條件啦</p>
<ol type="1">
<li>一個固定 ip</li>
<li>一個域名</li>
<li>一個機器</li>
</ol>
<p>通常在家裡面使用的網路都是「浮動網路」ip
位置每格一段時間就變，一旦變了就沒有人可以存取到你的網站了，而要一個固定
ip 解決方法有很多，可以打電話到 ISP (中華電信…) 去尋問，或是用 PPPoE
也可以。(或是有錢的話可以去 azure 或是 GCP 要一個也可以 w)</p>
<p>首先先去買域名，這裡我是向 <a href="https://porkbun.com/">porkbun</a>
買，這裡的網域又大又…阿是又多又便宜。</p>
<p>選好一個自己喜歡的名字後，接著選副域名，通常越有名的就會越貴，像是
.com .org 阿，那越不有名或看起來就像詐騙網站的 ww 就會非常便宜，像是
.xyz ...那就看自己的需求囉。porkbun
第一年會有優惠，從第二年開始要續約的話才會錢一點。</p>
<p>買好後就會看到下列畫面： <img src="https://i.imgur.com/wSdKZOE.png"
alt="image-20210702152427067" /></p>
<p>一共有兩個重點：DNS 以及 DNS record，DNS 是 Domain Name Server
的縮寫，負責把 ip 位置轉換成剛剛買的網域，DNS record
則是一個轉換的設定。</p>
<p>接下來我會介紹 cloudflare ，一個免費而且非常好用的 DNS
代管伺服器，它可以免費提供 Whois 服務 (查不到你的 ip 位置)，以及擋下一些
DDoS 攻擊。</p>
<h3 id="cloudflare">cloudflare</h3>
<p>到 <a href="https://dash.cloudflare.com/">cloudflare</a>
中，先註冊一個帳號，接著新增一個網域，他會要叫你先驗證 DNS，也就是設把
DNS 改成 cloudflare，回到 porkbun，點開 Authoratative nameservice
把裡面的東西全刪了，貼上 cloudflare 提供的 DNS。接下來 porkbun
會說這個設定可以要等最多 48 小時才會生效，因為 DNS cache
的問題，(但我個人的經驗差不多 1 個小時就差不多了 w)</p>
<p><img src="https://i.imgur.com/toa02RC.png"
alt="image-20210702154142658" /></p>
<p>一但 cloudflare 驗證完後會寄一封通知信，接著 porkbun 的 DNS record
設定就會換移到 cloudflare 上啦。</p>
<p>回到 cloudflare ，點開 DNS，選擇 A (就是指單純轉換的意思)，Name
寫買的網域名稱，Content 則是 ipv4 的位置，也可以加上 www
的設定，就可以一併把 <code>www.website.com</code> 導向到
<code>website.com</code>。</p>
<p><img src="https://i.imgur.com/yPq8RPZ.png"
alt="image-20210702154406198" /></p>
<p>恭禧啦！設定完成了！可以直接利用買的網域名稱連上自己架的網站伺服器了！</p>
<h2 id="homepage-雷">homepage 雷</h2>
<p>如果架好後發現，咦奇怪，我的網站怎麼 js css
跑掉了，排版變的怪怪的，這極有可能是因為 homepage 設定的問題，因為在
<code>hexo generate</code> 因為我們把 public
資料拿到其它地方去，網頁的「首頁」位置跑掉了，在 package.json
最上面加上這一行就可以解決問題了。</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="comment">// &quot;homepage&quot;: &quot;&lt;你的網頁網域&gt;&quot;</span></span><br><span class="line"><span class="string">&quot;homepage&quot;</span>: <span class="string">&quot;https://mushding.space/&quot;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hexo 心得筆記</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title>使用深度學習在 super resolution 整理 (二)</title>
    <url>/2021/07/06/%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E5%9C%A8-super-resolution-%E6%95%B4%E7%90%86-%E4%BA%8C/</url>
    <content><![CDATA[<p>繼上一篇的文章，本篇文章將會重點式的整理各個網路所要解決的問題，以及提出的改進方法。</p>
<p>keywords: SRCNN, FSRCNN, DRRN, EDSR <span id="more"></span></p>
<h2 id="srcnn">SRCNN</h2>
<p>這是 SR 領域的開山第一篇論文，架構非常簡單，只包含了三層 CNN</p>
<p><img src="https://i.imgur.com/CrLEPCq.png"
alt="image-20210706143720694" /></p>
<p>SRCNN 首先把 LR 使用 bicubic
將圖片放大至目標大小，接著作者提出了三步驟：Patch extention、Non-linear
mapping、Reconstruction。簡單來說就是經過：找特徵、組合特徵、upsampling
三個步驟，而這三個步驟也是 SR 最重要的核心想法。</p>
<p>卷積層使用的 kernel size 大小分為9x9，1x1 和 5x5。用
Timofte資料集，和 ImageNet pre-train。使用 MSE 作為 loss function。</p>
<h2 id="fsrcnn">FSRCNN</h2>
<p>主要是針對 SRCNN 改進了三點：一、在最後一層使用了 upsampling，解決了
SRCNN 輸入圖片還要經過 bicubic 放大的問題。二、改變卷積 kernel size
有更快的效果。三、共享 mapping 層，使如果要更改放大倍率的話可以直接 fine
tune。</p>
<p>FSRCNN 不用先把圖片放大，也把 kernel size 變小，速度上加速了不少。
<img src="https://i.imgur.com/G0J1tLA.png"
alt="image-20210706145159922" /></p>
<p>FSRCNN 可分為五個步驟：一、找特徵，與 SRCNN 一樣只是 kernel size
變小了。二、降維，經過一個 1x1 conv
把特徵圖減少，使得網路速度加快。三、mapping，也有做特徵圖組合的部份。四、Expanding。作者發現如果特徵圖太少的話，upsampling
的效果不太好，所以多加上了個 1x1 conv 增維。五、deconv。FSRCNN 的
upsampling 層使用的是 deconv。</p>
<h2 id="espcn">ESPCN</h2>
<p>此篇作者認為如果在高解析的圖片上做 upsampling
會增加計算複雜度，所以提出一個新方法可以直接在低解析的圖 upsampling
至高解析度 <img src="https://i.imgur.com/VFFYWCu.png"
alt="image-20210706145659734" /></p>
<p>作者提出 sub-pixel convolutional layer 又稱 pixel
shuffle，可參考上一篇文章</p>
<h2 id="vdsr">VDSR</h2>
<p>在介紹 VDSR 之前，先來介紹 ResNet 對 SR
領域的影響，這篇作者提到，其實低解析度與高解析圖片之間的差異度是非常小的，全部一起訓練會學到很多不必要的資訊，如果可以只學習「高解析與低解析之間的差」，那效率一定會提高。因此
VDSR 引入了 ResNet 的觀念，加上了殘差。</p>
<p><img src="https://i.imgur.com/InBFF4e.png"
alt="image-20210706151158349" /></p>
<p>VDSR 主要有四個貢獻：一、加深網路的層數到 20
層，為了能提取更多的特徵。二、同時也因為加入了 Residual
的概念，網路不會出現梯度消失/爆炸的問題。三、在每次 conv
之前都會對圖片做 padding 補 0，可以使每次 conv
完的圖片不會慢慢縮小，而是維持原大小，作者有在文後實驗補充 padding
的效果會比較好。四、將不同解析度倍數的圖片放在一起訓練，這樣 model
就可以一次處理不同的解析度問題了。</p>
<h2 id="drcn">DRCN</h2>
<p>使用了 RNN 的概念去提取特徵 <img
src="https://i.imgur.com/qBcuVIz.png"
alt="image-20210706152238830" /></p>
<p>作者把網路分為三個部份，提取特徵、特徵組合、upsampling，最大的特包就是特徵組合用一個
RNN 來實作，彼此共享權重，但我個人認為這個就只是神經元固定的好多層的
conv 而已</p>
<p><img src="https://i.imgur.com/coc8uMP.png"
alt="image-20210706152501440" /></p>
<p>作者還在 RNN 每一層中加上一個 Recursive-Supervision
為了來解決梯度消失/爆炸的問題，與 Residual 的概念相近。</p>
<h2 id="drrn">DRRN</h2>
<p>作者提出了新的概念：局部 vs 全局，概據 Residual
加入的長度來作為劃分： <img src="https://i.imgur.com/1QJbC3P.png"
alt="image-20210706152939520" /></p>
<p>VDSR 在各個地方都加上了 Residual
不管是小區域，或是大區域，作者認為這樣殘差的學習可以更全面。</p>
<p>與前面其它的架構做比較：VDSR 是全局殘差學習。DRCN 是全局殘差學習 +
每一個權重的殘差學習。DRRN是多路徑模式的局部殘差學習 + 全局殘差學習 +
每一個權重的殘差學習</p>
<h2 id="lapsrn">LapSRN</h2>
<p>作者總結了一下之前論文所遇到的問題：一、圖片先預先做放大，增加計算時間開銷，而且作者認為
deconv 或是 sub-pixel conv
在學習上的架構都過於簡單，在低解析度到高解析之間的 mapping
效果並不好。二、使用 L2 當做 loss 會有細節平滑化的問題
(smooth)。三、當要 upsampling 成很大的圖片時，例如直接放大 8
倍，效果一定不會很好。因此作者提山 LapSRN
，一個慢慢增加圖片大小的做法。</p>
<p><img src="https://i.imgur.com/DsfRAju.png"
alt="image-20210706154107539" /></p>
<p>LapSRN 一次只會放大 2 倍，如果要把圖片放大 8 倍的話，就會經過 3
次的運算。</p>
<p>LapSRN 網路架構分為兩部份： 一是 Feature Extraction
Branch，負責先找出「此放大倍率」下的特徵，再經一個 deconv
放大倍率，後接兩個 conv
層，一個用於繼續放大圖片特徵，一個用於計算出不同倍率間的殘差。 二是
Image Reconstruciton Branch，會先把 LR 做 deconv 得到 2
倍放大的圖片，接著與在 Feature Extraction Branch
計算出的殘差數值相加，就會最後結果。</p>
<p>而 LapSRN 的 loss funtion 設計為：</p>
<p><span class="math display">\[
L(\hat{y}; y;\theta) = \frac{1}{N}
\sum^N_{i=1}\sum^L_{s=1}\rho((\hat{y}^i_s - x^i_s) - r^i_s)
\]</span></p>
<p>其中， <span class="math inline">\(\rho\)</span> 叫作 Charbonnier
的懲罰函數 <span class="math inline">\(\sqrt{x^2 +
\varepsilon^2}\)</span> ( <span class="math inline">\(L1\)</span> 的變形
)， <span class="math inline">\(\varepsilon\)</span> 大小設為
0.001。<span class="math inline">\(x\)</span> 表示低解析度圖，y
表示高解析度圖，r 表示殘差，s 表示對應的放大級數。N 表示訓練的 batch
size 大小，L表示網路一共有多少放大層數。</p>
<p>可以看到這個 loss funtion
在每一個不同放大倍率下對有對應的計算，因此每一級都有一個
loss，訓練時就是要把每一級的 loss 和減少。</p>
<h2 id="srdensenet">SRDenseNet</h2>
<p>因為 DenseNet 是 CVPR 2017 Best Paper，所以就把 DenseNet 拿到 SR
領域來做啦</p>
<p><img src="https://i.imgur.com/LCajMLV.png"
alt="image-20210706155926611" /></p>
<p>SRDenseNet 一共分為四個部份：一、經過一個 conv
學到低解析的特徵。二、經過一大堆的 Dense Block
學習到高解析的特徵。三、經過一個 deconv upsampling。四、最後再經過一個
conv 得到最終的影像</p>
<p>這篇論文的作一共做了三種不同的實驗：一、只把 Dense Block
最後一層拿來用。二、把最後一層以及低解析度的那一層拿來用。三、每一個
Dense Block 以及低解析度的都拿來用。經實驗得知效果是
3&gt;2&gt;1，而計算量也是
3&gt;2&gt;1。經過這個實驗可以證明另一個有趣的事情，就是各個不同值置的
Residual 是有互補的關系的，也就是說低解析高解析是可互相提供訊息的</p>
<h2 id="srgansrresnet">SRGAN(SRResNet)</h2>
<p>使用了 GAN 來做 SR 的問題，而 Generator Network 則是使用叫
SRResNet，基本上就是 ResNet</p>
<p><img src="https://i.imgur.com/1c2FsEi.png"
alt="image-20210706160800084" /></p>
<p>使用 GAN 來解決問題，基本上與傳統 GAN 沒什麼大不同，loss function
也就是 content loss + adversarial loss。</p>
<p>adversarial loss 為 GAN 的 Discriminator 的 loss content loss 為
Generator 的 Loss，可使用 MSE 或是預訓練的 VGG loss</p>
<h2 id="edsr">EDSR</h2>
<p><img src="https://i.imgur.com/nlM5Cz1.png"
alt="image-20210706161259395" /></p>
<p>EDSR 的最大貢獻是把 Residual 中的 BN 給去掉了，本篇作者認為 CNN
中的分類、偵測是屬於「高層」應用，而 SR 屬於「低層」應用，直接把
Residual 搬過來用是不合適的，作者認為 BN 耗費時間及記憶體，所以如果把 BN
拿掉的話，就可以疊加「更多」層進去了。</p>
<p>並且使用分批訓練，先預訓練低解析度的網路，再把低解析度的網路參數初始化下一階的訓練，實驗證明這樣做的效率好很多，神奇的是效果也提升了。</p>
<p>下一個是同論文的架構 MDSR</p>
<p><img src="https://i.imgur.com/81kYY7P.png"
alt="image-20210706162159565" /></p>
<p>在網路前面加上了不同解析度預訓練好的模型來減少不同倍數輸入圖片之間的差異。</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/31664818</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title>四稜溫泉一日遊</title>
    <url>/2021/07/06/%E5%9B%9B%E7%A8%9C%E6%BA%AB%E6%B3%89%E4%B8%80%E6%97%A5%E9%81%8A/</url>
    <content><![CDATA[<p>趁著全國三級警戒還沒結束，大家都還不太敢出門玩的時候，決定去一趟秘境走一走
(當然要選人最少的景點囉)。這一次出去玩的地方是在台七線上的一個天然野溪溫泉，叫做四稜溫泉，有著可以直接在溪邊泡溫泉的獨特感受。</p>
<p>keywords: 四稜溫泉 <span id="more"></span></p>
<p>四稜溫泉位在台七線上 58.5k 處，一個做叫「四稜」的地方</p>
<p>58.5k 左右的路景照 <img src="https://i.imgur.com/WYRkp74.jpg"
alt="image-20210706110710239" /></p>
<p>在入口的地方會有一個警告牌那就是登山口啦 <img
src="https://i.imgur.com/4mECr0k.jpg"
alt="image-20210706111227175" /></p>
<p>接著就是一路的…下山！我本來以為一路上會像爬山一樣，會慢慢往上爬，結果不是，這個小徑像直接切到溪谷一樣，超極無敵陡的，有點小危險
<img src="https://i.imgur.com/pOn9JlP.jpg"
alt="image-20210706111601057" /></p>
<p>途中經過一棵古老神木，樹桿的直徑超極大的！不過看來它並有撐得起歲月的侵蝕qq，但突然在一片森林中出現一棵參天古木，而且還有一種螺旋狀，真的很有靈性的感覺呢
<img src="https://i.imgur.com/CLmgoQL.jpg"
alt="image-20210706112015553" /></p>
<p>最後爬下山友們熱心架的梯子後… <img
src="https://i.imgur.com/NrfGb9n.png"
alt="image-20210706111852651" /></p>
<p>就到達四稜溫泉啦啦，這就是我們今天的主角四稜溫泉，真的就完全座落在溪邊呢，而且還有人用石頭圍出一個澡池來
ww，(不過感覺只要颱風一來就不見了…) <img
src="https://i.imgur.com/Zv2K3wb.jpg"
alt="image-20210706112539976" /></p>
<p>這條溪在 google
地圖上面寫說是三光溪，這條溪的水真的超乾淨的啦，而且超極冰
w，很難想像過河泡溫泉全身泡在溪水裡的感覺 w。 <img
src="https://i.imgur.com/FOZLX57.jpg" alt="image-20210706112639429" />
<img src="https://i.imgur.com/AXZ8PhA.jpg"
alt="image-20210706112653770" /> <img
src="https://i.imgur.com/x8BfM3p.jpg"
alt="image-20210706112825331" /></p>
<p>後來就在河邊的礫石上開烤 w <img src="https://i.imgur.com/R3RVWXn.jpg"
alt="image-20210706113257225" /></p>
<p>還利用了大自然的力量，來冰可樂 w <img
src="https://i.imgur.com/p132y8B.jpg"
alt="image-20210706113329076" /></p>
<p>接著就到過河到對岸去泡溫泉啦，要抓緊山友們拉出來的繩索，水流其實有點急有點危險，真的要蹲低踩好重心一步一步慢慢過，可惜的是因為要過河所以就沒拍泡溫泉的相片了
qq <img src="https://i.imgur.com/We30FgF.jpg"
alt="image-20210706113756176" /></p>
<p>後來就打包打包回家去囉！當然要把垃圾收拾乾淨囉，好好保護大自然 <img
src="https://i.imgur.com/WYUSll5.jpg"
alt="image-20210706113952439" /></p>
<p>後來在回去的路上竟然遇到一個來露營的老外，不過這又是另一段故事了
w</p>
<p>回到停車的地方不知不覺也已經 7
點多了，太陽都快下山啦，終於可以回家好好休息了，剛爬上山見到文明的感覺真好
w。 <img src="https://i.imgur.com/Ru9A3uw.png"
alt="image-20210706114933704" /></p>
<p>心得：四稜溫泉是我人生第一個野溪溫泉，覺得這個秘境的感覺真的很有 fu
，真的很像身在異世界中一樣
w，不過到溪邊的路實在真的不好走，需要一定的裝備再去會比較安全一些，(我嘛…應該不會再第二遍了吧
w，下山好傷膝蓋，也可能是我不太會爬山…)</p>
]]></content>
      <categories>
        <category>遊記</category>
      </categories>
      <tags>
        <tag>四稜溫泉</tag>
      </tags>
  </entry>
  <entry>
    <title>使用深度學習在 super resolution 整理 (一)</title>
    <url>/2021/07/04/%E4%BD%BF%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E5%9C%A8-super-resolution-%E6%87%89%E7%94%A8%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<p>因為實驗室最近有人在報 SR
領域相關的論文，於是我也來研究研究一下，倒底深度學習在 SR
發展到什麼地步，以及目前最新的技術是什麼。以下這篇文章會從一些基本的
loss funtion、metrics 開始講起，接著會講從 2016
一直到現在論文倒底改進了哪些地方。這個系列我會分兩篇文章來說，首先先來看看
SR 領域的概論。</p>
<p>keywords: super resolution <span id="more"></span></p>
<h2 id="問題定義-problem-definition">問題定義 Problem Definition</h2>
<p>在 SR 領域中，我們所要探討的問題是要把 LR 影像 (low resolution)
轉換成 HR 影像 (high resolution)。</p>
<p>通常在實作中實驗順序會先把高解析度 ground truth 經過模糊化得到
LR。如以下的公式：(<span class="math inline">\(I_x\)</span> 為 LR，<span
class="math inline">\(I_y\)</span> 為 ground truth，D 為模糊公式，<span
class="math inline">\(\theta\)</span> 為參數)</p>
<p><span class="math display">\[
I_x = D(I_y; \theta)
\]</span></p>
<p>接著會經過一個網路，最後生成一張 HR 圖片，公式如下：(<span
class="math inline">\(I_x\)</span> 為 LR，<span
class="math inline">\(I&#39;y\)</span> 為生成的 HR，F
為一個神經網路，<span class="math inline">\(\alpha\)</span> 為參數)</p>
<p><span class="math display">\[
I&#39;_y = F(I_x; \alpha)
\]</span></p>
<p>最後一步把生成出的 HR (<span class="math inline">\(I&#39;_y\)</span>)
與 ground truth (<span class="math inline">\(I_y\)</span>)
做比對，或是做 loss funtion
就可以知道網路生成的圖片與原圖相不相近了，可用公式描述如下：(<span
class="math inline">\(\lambda\Phi(\theta)\)</span> 為正歸項)</p>
<p><span class="math display">\[
\theta&#39; = argmin_\theta(I_y, I&#39;_y) + \lambda\Phi(\theta)
\]</span></p>
<h2 id="評估方法-image-quality-assessment">評估方法 Image Quality
Assessment</h2>
<p>要如何用客觀、數學的方式來評斷兩張高解析度圖片相不相近呢？(ground
truth vs HR)，有以下三種方法：PSNR，MSE，SSIM。</p>
<h3 id="mse">MSE</h3>
<p>也就是 mean square error，均方差的意思，把兩張圖 pixel py pixel
把每個像素的誤差開平方相加，公式如下：<span class="math inline">\(I_i -
I&#39;_i\)</span> 為兩圖相減。</p>
<p><span class="math display">\[
MSE=\frac{1}{N}\sum^N_{i=1}(I_i - I&#39;_i)^2
\]</span></p>
<h3 id="psnr">PSNR</h3>
<p>又稱峰值訊噪比，與 MSE 有相關，做法是把圖片最高的 pixel
除以圖片全部的均方差，數字越大越好，最後再因為人眼的關系再取
log，使得數值變化縮小，通常峰值訊噪比值在 30dB 到 50dB 之間，越接近 50dB
越好。公式如下：(<span class="math inline">\(L\)</span>
為圖片中最大的像素值)</p>
<p><span class="math display">\[
PSNR=10*log(\frac{L^2}{MSE})
\]</span></p>
<h3 id="ssim">SSIM</h3>
<p>改進 PSNR 數值太大，人眼反而不準的問題，與 PSNR
一樣，數值越大越相似，SSIM 由以下三項定義出來： *
兩張影像灰階平均值的差異，沒有一張亮一張暗 (由平均來看) *
兩張影像的顏色種類分佈 (由標準差來看) * 兩張影像一致性的變化
(由共變異數來看)</p>
<p>公式如下：(c 的目的是避免除 0)</p>
<p><span class="math display">\[
\begin{gathered}
l(f, g)=\frac{2\mu_f\mu_g+c_1}{\mu_f^2+\mu_g^2+c_1}
\end{gathered}
\]</span></p>
<p><span class="math display">\[
\begin{gathered}
c(f, g)=\frac{2\sigma_f\sigma_g+c_2}{\sigma_f^2+\sigma_g^2+c_2}
\end{gathered}
\]</span></p>
<p><span class="math display">\[
\begin{gathered}
s(f, g)=\frac{2\sigma_fg+c_3}{\sigma_f\sigma_g+c_3}
\end{gathered}
\]</span></p>
<h2 id="上採樣方法-upsampling-methods">上採樣方法 Upsampling
Methods</h2>
<p>在 SR 領域中最重要的就是 upsampling，目的是要把經過 CNN
截取出的特徵圖，慢慢的放大，使特徵可以放近 LR 中，回恢成 HR。而
upsampling 的方法有好幾種，目前有也研究目標在朝向更好的 upsampling
方法，因為目前主流的 upsampling 都會有細節模糊等缺點。</p>
<h3 id="bicubic">bicubic</h3>
<p>中文可叫做雙三次插值，所謂的插值就是在已知的兩數之間再找出一個新值。bicubic
的計算量大，相比其它的插值法效果比較好，詳細公式參考底下網站。
https://www.codenong.com/cs106567714/</p>
<h3 id="deconv-反卷積">deconv 反卷積</h3>
<p>反卷積就是卷積的相反，把原圖的像素之間補上 0
，這樣在做完反卷積時圖片就會放大了。反卷積的效果不好，在細節的處理上也很鋸齒。</p>
<h3 id="sub-pixel-convolution">sub-pixel convolution</h3>
<p>又叫做 pixel shuffle，它的核心思想就是如果今天要把圖片放大 3
倍，我們就要將原圖的 channel 數量從 1 -&gt; 9，經過幾層 conv
的訓練後，將每一層的 channel 有規律的放回原圖，就可得到比原圖大的圖片了
<img src="https://i.imgur.com/QTsyr9h.png"
alt="image-20210706142249603" /></p>
<p>接下來會來講講從 2016 開始的 SRCNN 一路到現在的網路改進重點。</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>super resolution</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP 與 CV 的結合：self attention 以及 Transformer</title>
    <url>/2021/07/07/NLP-%E8%88%87-CV-%E7%9A%84%E7%B5%90%E5%90%88%EF%BC%9Aself-attention-%E4%BB%A5%E5%8F%8A-Transformer/</url>
    <content><![CDATA[<p>2020 是個 Transformer 在 CV
界大放異彩的一年，在大學時期不知為何的學了一堆 NLP
領域的東西、但是因著興趣研究所選擇念 CV
的我，一聽到這個消息我有點小開心阿，竟然有一天可以把我學到的這兩個東西結合在一起，真是太神奇啦啦。於是打算在未來研究所試試看往這個方向研究…。這篇是
Transformer 系列文的第一篇，會來先了解最基本也是一切的開始：self
attention 以及 Transformer，這兩個開山始祖。</p>
<p>keywords: self attention, Transformer <span id="more"></span></p>
<h2 id="transformer-架構初看">Transformer 架構初看</h2>
<p>這系列的重點是在 Transformer 上，所以我們先來看看 Transformer
的架構長什麼樣，再來一步一步拆解其中的區塊。(一關一關來過 w)</p>
<p><img src="https://i.imgur.com/gLA1fus.png"
alt="image-20210707231927724" /></p>
<h2 id="self-attention">self attention</h2>
<p>在研究所上課期間，有聽到老師介紹 self attention GAN
這篇論文，因此我對 self attention 的第一印象是來自 CV 的觀念，attention
是用來尋找圖片中重要的像素值，並且加以放大。但是在 NLP 領域中，self
attention 的觀念有那麼一點點的不同，但基本的大觀念是相通的，以下是我對在
NLP 中 self attention 意義的理解：</p>
<p>self attention 的誕生是為了解決 RNN 不太能「平行處理」的問題
(parallel problem)，什麼是平行處理呢？通常在 RNN 中對定一個 input
會對應一個 output，接著把 output 當做是下一個值的
input，接著重複以上的動作。可以發現一個問題我們沒有辨法一次性的把所有
input 一口氣放到 RNN 中，一口氣生出一串 output，而這個就是 self
attention 所以解決的問題。</p>
<p><img src="https://i.imgur.com/LwKMHeF.png"
alt="image-20210707163517881" /></p>
<p>self attention 提供的解法最核心的想法是：用算的！把每一個 input 與
input 之間的關系都算一遍！在 NLP 中 self attention 拆成的三個 vector
都有它對應的名字：query、key、value。</p>
<ul>
<li>query 指的是 -&gt; 要去與其它配對的</li>
<li>key 指的是 -&gt; 被配對的</li>
<li>value 指的是 -&gt; 放大縮小配對關系</li>
</ul>
<p>以下是計算步驟 <img src="https://i.imgur.com/wvp9Wv1.png"
alt="image-20210707163950107" /></p>
<ul>
<li>第一步：
<ul>
<li>將 query 和 key 計算相似度得到一個共變異數矩陣</li>
<li>可以是內積，cosine 相似度，MLP</li>
</ul></li>
<li>第二步：
<ul>
<li>使用 softmax 把權重歸一化</li>
</ul></li>
<li>第三步：
<ul>
<li>不像 SE 是直接把值乘回原圖</li>
<li>這邊的做法是使用「加權求和」</li>
<li>把 attention map 中的每一行，與原圖的每一行做線性組合</li>
</ul></li>
</ul>
<p>與 CV 的不太相同，CV 做 self attention 是要加強特徵圖中重要的地方，而
NLP 中做 self attention 是為了可以得到一個類似 RNN
提取特徵的網路架構。</p>
<h2 id="multi-head-self-attention">Multi-head Self attention</h2>
<p>在 Transtormer 中使用的 Self attention 是 Multi-head Self
attention，它的觀念也很簡單，就是把 query、key、value 再多用一個矩陣分為
<span class="math inline">\(q1, q2, k1, k2, v1, v2\)</span>，因此最後的
<span class="math inline">\(b\)</span> 會是兩個結果，如下圖：</p>
<p><img src="https://i.imgur.com/WJbnu3a.png"
alt="image-20210707165016203" /></p>
<p>而最後的結果 <span class="math inline">\(b\)</span> 會把 <span
class="math inline">\(b1, b2\)</span> 維度相加，再經一個調整維度的 <span
class="math inline">\(W\)</span> 使回複成與輸入相同的維度。如下圖：</p>
<p><img src="https://i.imgur.com/i6mAG9s.png"
alt="image-20210707165317159" /></p>
<p>使用 Multi-head Self attention 最直覺得差異就是多了一個 <span
class="math inline">\(b\)</span> 在這裡稱為一個 head，每多一個 head
等同於多一個訓練不同側重點的 attention，例如 2 個 head
的話，可能一個訓練是全域訊息，一個訓練是局部訊息，越多的 head
線性組合的空間也就越大。</p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>在 self attention 中會發現一個問題，就是當輸入字串是「A 打了 B」與「B
打了 A」機器會把它們當成是同一個輸入，因為 self attention 並沒有考慮
sequence 之間的順序。因此我們在輸入前要加上一個與 <span
class="math inline">\(a^i\)</span> 同維度的 <span
class="math inline">\(e^i\)</span>，而這個 <span
class="math inline">\(e^i\)</span> 就代表位置資料，在原論文中 <span
class="math inline">\(e^i\)</span> 是人工設計的，不是學習出來的。</p>
<p>那會有一個小問題，為什麼 <span class="math inline">\(a^i\)</span> 與
<span class="math inline">\(e^i\)</span>
之間是相加呢？這邊提出一個想法，假設有一個 one hot encoding <span
class="math inline">\(p^i\)</span> ，它會與最原使的輸入 <span
class="math inline">\(x^i\)</span> 相加，一同乘以 <span
class="math inline">\(W\)</span> 矩陣，根據線性代數的原理 <span
class="math inline">\(W\)</span> 可看作 <span class="math inline">\(W^I,
W^P\)</span> 的組合，公式如下：</p>
<p><span class="math display">\[
\begin{gathered}
W \cdot x^i_p = [W^I, W^P] \cdot  \begin{bmatrix}x^i\\p^i\end{bmatrix} =
\\
W^I \cdot x^i + W^P \cdot p^i = \\
a^i + e^i
\end{gathered}
\]</span></p>
<p>而乘開後得到 <span class="math inline">\(W^I \cdot x^i + W^P \cdot
p^i\)</span> ，但其實 <span class="math inline">\(W^I \cdot x^i\)</span>
就是 <span class="math inline">\(a^i\)</span> ，<span
class="math inline">\(W^P \cdot p^i\)</span> 就是 <span
class="math inline">\(e^i\)</span>，得證是可以直接相加的。</p>
<p><img src="https://i.imgur.com/UHazpwA.png"
alt="image-20210707233810364" /></p>
<p>那又一個問題來了，<span class="math inline">\(e^i\)</span>
倒底是怎麼設計的呢？它如果用圖畫出來會長這個樣子…</p>
<p><img src="https://i.imgur.com/tGK6dQb.png"
alt="image-20210707235902004" /></p>
<p>嗯…看不懂 ww，不過它是根據一個神奇的公式所生成出來的，叫做
Sinusoidal，以下以 <span class="math inline">\(PE\)</span> (Position
Embedding) 代稱：</p>
<p><span class="math display">\[
\begin{gathered}
PE_{(pos, 2i)} = sin(pos/10000^{2i/d_model}) \\
PE_{(pos, 2i + 1)} = cos(pos/10000^{2i/d_model})
\end{gathered}
\]</span></p>
<p><span class="math inline">\(pos\)</span> 代表輸入值在 sequence
中的位置，舉個例子：當 <span class="math inline">\(pos\)</span> 為 1
時，對應的 Positinal Encoding 可以寫成：</p>
<p><span class="math display">\[
PE(1) =
[sin(1/10000^{0/512}),cos(1/10000^{0/512}),sin(1/10000^{2/512}),cos(1/10000^{2/512}),...]
\]</span></p>
<p>至於為什麼是 10000 嘛…沒人知道
w，總之這個奇怪的式子可以有以下的好處：</p>
<ul>
<li>使每一個位置都有唯一的 Positional Encoding</li>
<li>當輸入是長度會變動時，是可以單單修改公式中的 <span
class="math inline">\(i\)</span> 來達成目的</li>
<li>因為是三角函數的關系，可以讓 model 容易計算出相對的位置
(角和公式)</li>
</ul>
<p>當然還有更多奇奇怪怪的編碼方式…</p>
<p><img src="https://i.imgur.com/tsn4mMZ.png"
alt="image-20210708001248746" /></p>
<h2 id="transformer">Transformer</h2>
<p>接下來就到重頭戲啦，我們終於可以來仔細看看裡面倒底藏了什麼東東。一共分成兩半，左半稱為
Encoder，右半稱為 Decoder。每一個「綠色」的 Block 都可以重複 N
遍，Encoder 的資料會送給 Decoder。接下來細解說各個部份：</p>
<p><img src="https://i.imgur.com/gLA1fus.png"
alt="image-20210707231927724" /></p>
<h3 id="encoder">Encoder</h3>
<p>先來說說左半邊的 Encoder，Encoder 的前半段 Multi-Head Attention
就是上面提到的 Attention，比較不一樣的地方是有拉一條 Residual
直接與結果相加，(這個部份與 self attention GAN 觀念相同)。Add &amp; Norm
則是兩個東西的合稱，Add 就是 Residual，而 Norm 則是做完 Residual
後會經過一個 Normalization，而這裡選用的是 Layer Norm，與 Batch Norm
不同的是，Layer 看重的是 channel 與 channel 之間的標準化</p>
<p><img src="https://i.imgur.com/ASydltz.png"
alt="image-20210708002338795" /></p>
<p>接著把結果再放進一個 FFN 中做進一步訓練，並且也加上了 Residual 及
Layer Norm。以上為 Encoder 的整體架構。</p>
<h3 id="decoder">Decoder</h3>
<p>我們慢慢由下往上講起，Decoder 中一共有兩個 Attention。</p>
<p>Decoder 的輸入就比較有趣一點了，與 RNN 相同，Decoder
的輸入為每一個時間點產生的結果合，也就是說，在 <span
class="math inline">\(t-1\)</span> 的 output 就是在 <span
class="math inline">\(t\)</span> 的 input 。</p>
<p>也因為這樣，在 Decoder 的「第一個」 self attention，換了個名字：叫做
Masked Multi-Head Self Attention，其實道理也很簡單：因為 Decoder 的
input 是隨著時間變化了增加的，因此在做 attention
的時候我們不能像時空旅人一樣，直接預知到未來的輸出一起做運算。解決的方法就是在
query 乘上 key 後多乘上一個 Mask ，這個 Mask 負責把後面的值給蓋住，不讓
attention 算到它。(先做 Masked 再做 Softmax)，下圖為 Masked Multi-Head
Self Attention 的流程圖：</p>
<p><img src="https://i.imgur.com/ECf9hPH.png"
alt="image-20210708003720186" /></p>
<p>Decoder 的輸入第一個字符會是一個
&lt;Begin&gt;，而輸出最後一個字節會是一個 &lt;End&gt;，下圖解釋 Decoder
的 input 以及 output 以及它是「依序」生出結果來的。</p>
<p><img src="https://i.imgur.com/RYXM1sz.png"
alt="image-20210708004319820" /></p>
<p>值得注意的是 Decoder 的訓練與測式的方法不同：
<strong>測試時：</strong> 如果 RNN 一樣，上一個時間點的 output
就為下一個時間點的 input，接著照著 Transformer
的架構走，一個一個的生成出結果。 <strong>訓練時：</strong>
這裡就比較特別了，用了一個叫做 Teacher Forcing
的方法，簡單來說就是直接把 Ground Truth 當成輸入，直接去訓練
Decoder，因為是直接輸入「整串」GT，所以可以平行化加速。(但依然會被 Mask
給蓋掉後面的值 w，不然真的就是時空旅人了)</p>
<p>「第二個」Attention，也有變化，它其實不能稱為完全的
Attention，「第二個」Attention 的 Query 來自 Decoder，Key Value 來自
Encoder (仔細看看圖)</p>
<p>最後經過 Softmax 得到 one Hot
encoding，預測出下一個時間點的字詞。</p>
<p>以下就是 Transformer 的完整架構啦啦，比較特別是它的 Seq2Seq
的感覺吧，輸入是 Sequence 輸出也是 Sequence，這種東西要怎麼放到 CV
中去實作呢…？所以接下來要來討論 DETR 這篇論文，他成功的把 Transformer
放進 Object detection 的問題應用中。</p>
<h2 id="reference">Reference</h2>
<p>為什麼要使用 LayerNorm https://www.zhihu.com/question/395811291</p>
<p>為什麼要提出 scaled dot product attention
https://blog.csdn.net/qq_37430422/article/details/105042303</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP 與 CV 的結合：End-to-End Object Detection with Transformers DETR</title>
    <url>/2021/07/08/NLP-%E8%88%87-CV-%E7%9A%84%E7%B5%90%E5%90%88%EF%BC%9AEnd-to-End-Object-Detection-with-Transformers-DETR/</url>
    <content><![CDATA[<p>本篇文章要來看看 Facebook 是怎麼把 Transformer 運用在 Object
Detection 上，也因為這篇論文的成功，CV 界吹起了一陣 Transformer 熱…</p>
<p><a
href="https://arxiv.org/pdf/2005.12872.pdf">https://arxiv.org/pdf/2005.12872.pdf</a></p>
<p>keywords: DETR <span id="more"></span></p>
<h2 id="abstract">Abstract</h2>
<p>DETR 為 Detection Tranformers 的簡寫，這篇論文提出了一個 end-to-end
且 based on Tranformer 的方法來解決 Object
Detection，而最後的準確率以及運行時間與改良後的 Faster R-CNN 相當。</p>
<h3 id="特色">特色</h3>
<p>下圖為 DETR 的架構圖： DETR 架構分為兩個主要的部份：CNN 以及
Transformer。 <img src="https://i.imgur.com/ViOBka4.png"
alt="image-20210708144748759" /></p>
<p>由於因為使用了 Transformer，因此作者把 Object Detection
的問題看成一個 set prediction problem，並且訓練時要求 predict set 與
ground truth set 間的bipartite matching。看不懂嗎 ww
沒關系以下詳細介紹各個名詞意思。</p>
<h3 id="set-prediction-problem">set prediction problem</h3>
<p>有一些 Set，彼此之間做 matching 的問題，通常會包含兩種 Set：predict
set 與 ground truth set。 <img src="https://i.imgur.com/B2nWwSK.png"
alt="image-20210708163216274" /></p>
<h3 id="bipartite-matching">bipartite matching</h3>
<p>而 bipartite matching 是 set prediction problem 中的一種特例：Set
數為 2，且所有的對應關系皆為「一對一關系」，如下圖所示：</p>
<p><img src="https://i.imgur.com/R7ObEmW.png"
alt="image-20210708165853197" /></p>
<p>比較特別的地方是如果已經沒對應的話，例如上圖的
2，那麼它的對應關系就會是 <span
class="math inline">\(\emptyset\)</span>，在 Object Detection
中就代表背景。之後會來詳細介紹例子來進一步解釋…</p>
<h3 id="no-anchor-no-nms-no-receptive-field">No anchor, No NMS, No
receptive field</h3>
<p>因為使用 set prediction 使得 DETR 有以下的特色：</p>
<ul>
<li>不用 NMS 因為所有的集合關系為「一對一」，不像以前 anchor based
的方法會有多對一的問題</li>
<li>整體的網路架構非常簡單，不需要因為領域的不同而做對應的細調</li>
</ul>
<h2 id="細看網路架構圖">細看網路架構圖</h2>
<p><img src="https://i.imgur.com/unsItRh.png"
alt="image-20210708171653143" /></p>
<p>DETR
可分為四個部份：backbone、encoder、decoder、FFN，以下分別解釋：</p>
<h3 id="backbone">backbone</h3>
<p>處理的問題非常簡單，輸入為圖片，輸出則為 <span
class="math inline">\((B , C , H , W)\)</span>
的特徵圖。負責找出特徵用的，在 DETR 中會把特徵圖壓縮成 <span
class="math inline">\((B , 2048 , H/32 , W/32)\)</span> 張，也就是放大 5
倍，特徵圖數量為 2048。</p>
<p>接著經過一個 1x1 conv 降維減少運算量使 <span class="math inline">\((B
, 2048 , H/32 , W/32)\)</span> 變成 <span class="math inline">\((B , 256
, H/32 , W/32)\)</span></p>
<p>但因為要把特徵圖放進 Transformer 的原因，我們要轉換維度 (從 3d 變成
2d)，有點像把圖片用 sequence 來表示的感覺。把<span
class="math inline">\((B , 256 , H/32 , W/32)\)</span> 變成 <span
class="math inline">\((B , 256 , (H/32 \cdot W/32))\)</span></p>
<p>原論文中使用的是 ResNet-50 或 ResNet-101</p>
<h3 id="encoder">encoder</h3>
<p><strong>1. Positional Encoding</strong> 把 backbone
產生的特徵圖，先加上 positional encoding，再放進 encoder，其中
positional encoding
也有做修改，變成二維的編碼了，為了符合圖片是二維的關系。公式改為以下：</p>
<p><span class="math display">\[
\begin{gathered}
PE_{(pos_x,2i)} = sin(pox_x/10000^{2i/128})\\
PE_{(pos_x,2i+1)} = cos(pox_x/10000^{2i/128})\\
PE_{(pos_y,2i)} = sin(pox_y/10000^{2i/128})\\
PE_{(pos_y,2i+1)} = cos(pox_y/10000^{2i/128})
\end{gathered}
\]</span></p>
<p>小細節的地方是原本特徵數 256 的部份，會平分一半給 x 軸的編碼，一半給
y 軸的編碼，所以各是 128。</p>
<p>把生成的 Positional Encoding 加上 CNN 生成的特徵圖就是 Encoding
的輸入了。如下圖所示：</p>
<p><img src="https://i.imgur.com/X6K8Cf7.png"
alt="image-20210708180309801" /></p>
<p><strong>2. Encoder</strong> 底下是 encoder 與 decoder 的架構：</p>
<p><img src="https://i.imgur.com/J90hZNT.png"
alt="image-20210708172344944" /></p>
<p>總結與原 Transformer 不一樣的地方：</p>
<ul>
<li>Positional Encoding 改成可考慮二維的編碼</li>
<li>且每一個 Block 的輸入都要加上 Positional Encoding
(原始是只加再最一開始而已)</li>
<li>且 Positional Encoding 只與 Query、Key 相加，不與 Value 相加</li>
</ul>
<p>最後的輸出維度為 <span class="math inline">\((B, 256,
HW)\)</span>，且會把結果送給 Decoder。</p>
<h3 id="decoder">Decoder</h3>
<p>Decoder 的變化更大了，他的 input 是一個叫做 Object query
的東東，通常維度設為 <span class="math inline">\((N, b,
256)\)</span>，而這個 <span class="math inline">\(N\)</span> 在原來
Transformer 代表輸出句子的長度，在這裡指的是「要生出多少個 BBox」，這個
<span class="math inline">\(N\)</span> 設越大越好，越大的 <span
class="math inline">\(N\)</span> 可以有更多的 BBox
組合可能性，同時付出的計算代價也沒有很大。(因為 Object query
是一個矩陣，其中一維變大而已)</p>
<p>在這裡 Object query 擔任的是一個類似 Positional Encoding
的角色，它會與第一個 self attention 的 query key 相加，與第二個 self
attention 的 query 相加。只不過它是一個可以自我學習的 Positional
Encoding，不像前一個是人工設定的，可以理解為 Object query 在學習這 100
個 BBox 之間的全局關系。</p>
<p><img src="https://i.imgur.com/J90hZNT.png"
alt="image-20210708172344944" /></p>
<h3 id="ffn">FFN</h3>
<p>最後 FFN 的地方會分成兩個不同維度的輸出</p>
<ul>
<li>一個是維度 <span class="math inline">\((B, 100, class + 1)\)</span>
的分類輸出</li>
<li>一個是維度 <span class="math inline">\((B, 100, 4)\)</span> 的 BBox
輸出，4 分別代表的是 <span class="math inline">\((c_x, c_y, w,
h)\)</span></li>
</ul>
<h2 id="loss-function">Loss function</h2>
<p>到目前為止我們已經得到了兩個結果：一共 N 個 BBox set
以及預測分類結果。那接下來我們來看 Loss
function，問題來了，這些輸出都是無序的阿，完全不知道哪一個 BBox
對應到那一個 Class，在這篇論文使用了一個經典的演算法 <strong>Hungarian
Algorithm 匈牙利演算法</strong>，可以來專門解決一對一分配問題。</p>
<h3 id="hungarian-algorithm-匈牙利演算法">Hungarian Algorithm
匈牙利演算法</h3>
<p>匈牙利演算法是一個專門來解決指派問題，假設今天有三位工人以及三份工作，每一位工人作工作都有不同的成本，今天在<strong>每一個工作都被分配到的前提下</strong>，找出一個成本最小的組合。例如：</p>
<p><img src="https://i.imgur.com/D5Z4QeY.png"
alt="image-20210708215043817" /></p>
<p>可以發現 -&gt;
讓吉姆清潔浴室、史提夫打掃地板、艾倫清洗窗戶時，可以達到最小成本
$6，匈牙利演算法就是在解決這個問題，詳細算法不在這多做說明，有興趣可參考維基百科
<a
href="https://zh.wikipedia.org/wiki/%E5%8C%88%E7%89%99%E5%88%A9%E7%AE%97%E6%B3%95">維基百科</a></p>
<p>對應到我們的例子中，工人就是 100 個 BBox，而工作就是圖中的 Ground
truth 類別。假設有一張圖中有 Dog、Horse、Car，我們的矩陣就是一個 <span
class="math inline">\(100 \cdot 3\)</span> 的矩陣，如下圖：</p>
<p><img src="https://i.imgur.com/mk6a73F.png"
alt="image-20210709011410151" /></p>
<p>選擇一個矩陣中值總合為最小的組合，即為 BBox
對應的分類別了，其中所有對應不到的就算在空集合中，也就是背景類別。有個小地方要注意(這是我個人的理解)，在背景
Background
的部份在第一個出來的版本是會選擇出一個框來框它的，而會在後來的調整中把屬於背景的
BBox 去掉。如原論文下圖最後一步表示：綠色的框不見了。</p>
<p><img src="https://i.imgur.com/ViOBka4.png"
alt="image-20210708144748759" /></p>
<h3 id="loss-定義">Loss 定義</h3>
<p>那矩陣中所代表的值就是我們的 Loss 啦，在 DETR 中 Loss
定義為以下：首先是「匈牙利演算法」 -&gt; 總合為最小 Loss
的數學定義：</p>
<p><span class="math display">\[
\hat{\sigma} = arg\underset{\sigma\in\sum_N}{min}
\sum^N_iL_{match}(y_i,\hat{y}_{\sigma(i)})
\]</span></p>
<p>意思為某一真值 <span class="math inline">\(y_i\)</span> 以及 一預測值
<span class="math inline">\(\hat{y}_{\sigma(i)}\)</span>
的所有可能的排列，經過 <span class="math inline">\(L_{match}\)</span> 使
<span class="math inline">\(y_i\)</span> 與 <span
class="math inline">\(\hat{y}_{\sigma(i)}\)</span> 的距離為最小。</p>
<p>而 <span class="math inline">\(L_{match}\)</span>
也就是上圖矩陣中的數值一共包含兩個部份：</p>
<ul>
<li>class 分類的 cross entropy loss</li>
<li>BBox 的 loss</li>
</ul>
<p><span class="math display">\[
\mathcal{L}_{Hungarian}(y,\hat{y}) =
\sum^N_{i=1}[-log\hat{p}_{\hat{\sigma}(i)}(c_i)+\mathcal{L}_{box}(b_i,\hat{b}_{\hat{\sigma}}(i))]
\]</span></p>
<p>BBox 的 loss 又包含兩個部份：</p>
<ul>
<li>L1 loss</li>
<li>GIoU</li>
</ul>
<p>其中的 <span class="math inline">\(\lambda_{iou}
\lambda_{L1}\)</span> 為超參數，可調整，代表 BBox Loss 所佔的比重</p>
<p><span class="math display">\[
\mathcal{L}_{box}(b_i,\hat{b}_{\hat{\sigma}}(i)) =
\lambda_{iou}\mathcal{L}_{iou}(b_i,\hat{b}_{\hat{\sigma}}(i)) +
\lambda_{L1}||b_i-\hat{b}_{\hat{\sigma}}(i)||
\]</span></p>
<h2 id="模型訓練方法">模型訓練方法</h2>
<p>所以模型的訓練就可以用白話解釋成： 已知圖上有 Car Dog Horse
三類別，先使用匈牙利算法算出 Loss 最低的組合，再把這個組合與 GT 的 BBox
計算出 Loss，接著 backpropagation 回模型訓練</p>
<h2 id="object-query-詳解以及-decoder-詳解">Object query 詳解以及
Decoder 詳解</h2>
<p>看完了架構，我們來回頭看 Object query，倒底這個 <span
class="math inline">\((100,B,256)\)</span> 的向量做了什麼事，而 Decoder
中的 query key value 又為什麼這樣設計呢？</p>
<p><strong>Object query (query)</strong> 我們可以把 Object query
看成是有 100 個格子，每個格子有 256 維的向量，每個格子中的 256
維包含了某個類別的訊息，例如 Car
的位置、編碼特徵等等，可理解為這個格子就是在找 Car 的，所以稱為不同
Object 的訊息</p>
<p><strong>Key Value</strong> 而 Key 和 Value 則是從 Encoder
而來，是經過 Encoder 找出的「圖像全局訊息」，嗯…就是一個綜合特徵感。把
Query 與 Key 計算像是在尋找「某個位置附近有沒有 Car
(Object)」，而如果有就經 Value 加權輸出，如果沒有…就什麼也沒有啦 w
就輸出為 0</p>
<p><strong>最後</strong> 最後會發現如果與 Fast R-CNN 比較的話，其實
Object query 與 anchor 非常像，只是這個 Object query 的維度為 <span
class="math inline">\((100,B,256)\)</span>
非常高，優點為能夠通過訓練來尋找，且因維度高能表示的特徵也多，缺點為維度太高，訓練時間長，不好訓練。</p>
<h2 id="experiments">Experiments</h2>
<p>與 Faster RCNN 對比，在效果上不相上下。</p>
<p><img src="https://i.imgur.com/z1Effh9.png"
alt="image-20210709021214812" /></p>
<p>缺點：</p>
<ul>
<li>沒有引入 FPN 所以在小物件上效果不好</li>
<li>訓練時間真的太久啦</li>
</ul>
<h2 id="結論">結論</h2>
<p>這篇 DETR 可說是 Transform
熱門的先趨，用了非常多的概念，希望能把圖片表示像是 sequence
一樣的來訓練。</p>
<p>使用 Transformer
的好處是可以學習到更多的特徵點，並且輸入輸入概念全部不一樣，全都是變成
Sequence 了，有點像 Seq2Seq 那樣，也因此引申出不用 NMS
的算法，給出了一個全新的思考方向。</p>
<p>這篇論文雖然 AP 與 Faster R-CNN
相當，但帶出的觀念給後來的人非常多的想像，究竟 Transformer
可以到什麼程度呢，讓我們繼續往下看吧 XD</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/340149804</p>
<p>https://medium.com/%E8%BB%9F%E9%AB%94%E4%B9%8B%E5%BF%83/detr%E7%9A%84%E5%A4%A9%E9%A6%AC%E8%A1%8C%E7%A9%BA-%E7%94%A8transformer%E8%B5%B0%E5%87%BAobject-detection%E6%96%B0pipeline-a039f69a6d5d</p>
<p>https://zhuanlan.zhihu.com/p/326647798</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>NLP 與 CV 的結合：Deformable DETR: Deformable Transformer For End-To-End Object Detection - 正面對決 DETR 的缺點！</title>
    <url>/2021/07/09/NLP-%E8%88%87-CV-%E7%9A%84%E7%B5%90%E5%90%88%EF%BC%9ADeformable-DETR-Deformable-Transformer-For-End-To-End-Object-Detection-%E6%AD%A3%E9%9D%A2%E5%B0%8D%E6%B1%BA-DETR-%E7%9A%84%E7%BC%BA%E9%BB%9E%EF%BC%81/</url>
    <content><![CDATA[<p>Deformable DETR 的提出是為了解決 DETR 的兩個缺點：</p>
<ul>
<li>訓練時間超長
<ul>
<li>因為 CNN 是 Attention Map 的一種特例，也就是說 Attention Map
的組合性多，效果效好，但是複雜度高</li>
</ul></li>
<li>計算複雜度高
<ul>
<li>同上 Attention Map 是 <span class="math inline">\(N_q \cdot
N_k\)</span> 維的，而 CNN 是 <span
class="math inline">\(HW\)</span></li>
</ul></li>
</ul>
<p>論文中使用了 Deformable conv
的觀念來達成減少運算量及加入多重解析度。</p>
<p><a
href="https://arxiv.org/pdf/2010.04159.pdf">https://arxiv.org/pdf/2010.04159.pdf</a></p>
<p>keywords: Deformable DETR <span id="more"></span></p>
<h3 id="計算時間長詳解">計算時間長詳解</h3>
<p>假設 Batch 為一，圖片經 CNN 後的維度會變成 <span
class="math inline">\((H \cdot W \cdot C)\)</span> 的特徵向量，後經
reshape 變成 <span class="math inline">\((HW \cdot C)\)</span> 再加上
Positional Enbedding 後放進 Transformer。其中 <span
class="math inline">\((HW \cdot C)\)</span> 可看成長度為 <span
class="math inline">\(HW\)</span> 大小為 <span
class="math inline">\(C\)</span> 的 sequence</p>
<p><img src="https://i.imgur.com/6pAB8h3.png"
alt="image-20210709115659980" /></p>
<p>以下 <span class="math inline">\(N_q N_k\)</span> 其實就是 <span
class="math inline">\(HW\)</span>，則輸入向量 <span
class="math inline">\((N \cdot C)\)</span>，乘上一個 <span
class="math inline">\(W\)</span> 轉換矩陣 <span class="math inline">\((C
\cdot 1)\)</span> 則計算 self attention 的時間複雜度為：</p>
<p><span class="math display">\[
O(N_qC^2 + N_kC^2 + N_qN_kC)
\]</span></p>
<p>分別對應</p>
<p><span class="math inline">\(O(N_qC^2)\)</span> 計算 Query
的複雜度</p>
<p><span class="math inline">\(O(N_kC^2)\)</span> 計算 key 的複雜度</p>
<p><span class="math inline">\(O(N_qN_kC)\)</span> Attention 的複雜度
<span class="math inline">\((N_qC \cdot CN_k) = (N_qN_k)\)</span></p>
<p><img src="https://i.imgur.com/gr4y6sI.png"
alt="image-20210709120129060" /></p>
<p>透過以上可以發現當圖片的解析度越大，Attention
的計算複雜度為所有像素數量的平方，也就是 <span
class="math inline">\((HW)^2 = N^2\)</span>
，這就導致了圖片越大，模型越不好收斂的原因。</p>
<h2 id="網路架構">網路架構</h2>
<p>作者引用了 Deformable conv 這篇論文，最大的觀念就是突破以往 conv 固定
size 的卷積核 (3x3) ，而是改用一個 (3x3) +
偏移量的方式來做，如下圖：(每一個原 conv 的點都會加上一偏移量)</p>
<p><img src="https://i.imgur.com/7x5XPIH.png"
alt="image-20210709153338566" /></p>
<p>而這個偏移量是透過一層的 conv 來自己學出來的，如下圖：(注意綠色 conv
的深度為 2N，代表 x 軸與 y 軸的偏移量)</p>
<p><img src="https://i.imgur.com/VuXpX2L.png"
alt="image-20210709153515728" /></p>
<p>原 Deformable 作者認為這個變型 conv 的好處有：</p>
<ul>
<li>對物體的形變能力更強 (超畸形都沒在怕)</li>
<li>對圖片的視野更廣擴，因為不受矩型 conv
的限制，可以自由奔放的去找特徵點。</li>
</ul>
<p>本論文 Deformable DFTR
的作者就把這個觀念放到網路中的…任何地方，(基本上想到的地方都加上了)，包含
CNN 層、Encoder</p>
<p><img src="https://i.imgur.com/WXO4wG5.png"
alt="image-20210709154053491" /></p>
<h2 id="deformable-attention-module">Deformable Attention Module</h2>
<p>於是作者提出 Deformable Attention Module 來解決 DETR 的問題，與原
Attention 公式對比如下：(上式為原 Attention、下式為 Deformable
Attention)</p>
<p><span class="math display">\[
\mathrm{MultiHeadAttn}(z_q,x_k)={\sum^M}_{m=1}W_m[\textcolor{purple}{\sum_{k\in\Omega_k}}A_{mqk}\cdot
W&#39;_m\textcolor{red}{x_k}]
\]</span></p>
<p><span class="math display">\[
\mathrm{DeformAttn}(z_q,p_q,x)={\sum^M}_{m=1}W_m[\textcolor{purple}{\sum^K_{k=1}}A_{mqk}\cdot
W&#39;_mx\textcolor{red}{(p_q+\Delta p_{mqk})}]
\]</span></p>
<p>用非常白話來講兩個最大的不同點就是：</p>
<ul>
<li>key 的數量不同：
<ul>
<li>原本的 self attention 「每個」 query 會與「每個」 key
做計算，如上一節提到的 <span
class="math inline">\((N_qN_k)\)</span></li>
<li>而 Deformable 則是使用一個自定數 <span
class="math inline">\(K\)</span> ，來限制 query 只與 <span
class="math inline">\(K\)</span> 個 key 做計算，變成 <span
class="math inline">\(N_qK\)</span> (作者的 K 取 4，很小喔…)</li>
</ul></li>
<li>key 的意義不同：
<ul>
<li>原本的 self attention 就是單純計算第 i 個 query 與第 j 個 key
之間的關系</li>
<li>而 Deformable，則是引入了 Deformable 的觀念，把原本點上 <span
class="math inline">\((p_q)\)</span> 做一個位移偏差 <span
class="math inline">\(\Delta p_{mqk}\)</span> ，總偏移點的數量為 <span
class="math inline">\(K\)</span>，如下圖所示：</li>
<li>意義就變為「只與 <span class="math inline">\(p_q\)</span>
點附近的其它點做 query key 的計算了」</li>
</ul></li>
</ul>
<p><img src="https://i.imgur.com/lhqVF0k.png"
alt="image-20210709155840876" /></p>
<ul>
<li>Attention 做法小不同：
<ul>
<li>在 Deformable DETR 中的 Attention 塊並不是把 key 與 query
做內積，而是直接做線性轉換，之後再乘上 <span
class="math inline">\(K\)</span> 個偏差特徵點就可以了。完整的 Deformable
Attention Module 如下圖：</li>
</ul></li>
</ul>
<p><img src="https://i.imgur.com/IJQwucJ.png"
alt="image-20210709160943065" /></p>
<p>改用這個架構時間複雜度算出來為：結果就會與圖片大小的 <span
class="math inline">\(WH\)</span> 無關啦啦</p>
<p><span class="math display">\[
O(NKC^2)
\]</span></p>
<h2 id="multi-scale-deformable-attention-module">Multi-scale Deformable
Attention Module</h2>
<p>在這一章作者要來解決 DETR 中沒有使用 FPN
使得在小物件偵測效果不好的問題。公式如下：</p>
<p><span class="math display">\[
\mathrm{MSDeformAttn}(z_q,\hat{p_q},\{x^i\}^L_{l=1}) =
\sum^M_{m=1}W_m[\sum^L_{l=1}\sum^K_{k=1}A_{mlqk}\cdot
W&#39;_mx^l(\phi_l(\hat{p_q})+\Delta p_{mlqk})]
\]</span></p>
<p>簡單來說就是在每一個 CNN 的特徵向量中，假設有 <span
class="math inline">\(L\)</span> 層，每一層各取 <span
class="math inline">\(K\)</span> 個點的意思，因此 key 可以表示成 <span
class="math inline">\(K\cdot L\)</span>，在乘上 query
後，這個意義其實就融合了各層的特徵，所以作者認為不需要再做 FPN。<span
class="math inline">\(K\cdot L\)</span> 乘上 query
天生就有相加的效果了。下圖為完整架構：</p>
<p><img src="https://i.imgur.com/WXO4wG5.png"
alt="image-20210709154053491" /></p>
<p>下圖則為 CNN 特徵向量到 Encoder 的架構圖：可以發現 Encoder 的 C 皆為
256，因此要對不同解析度的特徵圖做 1x1 conv，以及多做一層卷積層得到放大 6
倍的特徵圖。</p>
<p>所以 Encoder 中為 CNN 第 3, 4, 5, 6 層的特徵向量。</p>
<p><img src="https://i.imgur.com/TEeAUU8.png"
alt="image-20210709162927599" /></p>
<h2 id="decoder">Decoder</h2>
<p>Decoder 中有兩個 Block：cross-attention、self-attention。兩個
attention 的三個 input 彼此都不太一樣。由於 Deformable attention
只能用在與 CNN 相關的層上，所以 cross-attention 可以做修改，而
self-attention 就維持原樣了。</p>
<p><strong>self attention</strong></p>
<p>Query 來自 Object query Key 來自 Object query
維持原做法，不做任何調整</p>
<p><strong>cross attention</strong></p>
<p>Query 來自 Object query Key 來自 Encoder 的輸出 使用的是 Deformable
Attetion Module</p>
<p>另外最後一層的 FFN 預測 BBox 的輸出有一點點的不一樣，變成預測出相對於
<span class="math inline">\(p_q\)</span> 參考點的偏移量 <span
class="math inline">\(b_{q\{x,y,w,h\}}\)</span> (x軸 y軸 長
寬)，(嗯…好像有那麼一點點 YOLO 的味道)，公式如下：</p>
<p><span class="math display">\[
\hat{b_q}= \{\sigma(b_{qx}+\sigma^{-1}(\hat{p_{qx}})), \sigma(b_{qy} +
\sigma^{-1}(\hat{p_{qy}})), \sigma(b_{qw}), \sigma(b_{qh}))\}
\]</span></p>
<h2 id="experiments">Experiments</h2>
<p>與 DETR 的效果相比：在 epoch 與 Traning GPU hours 上與 DETR
少很多</p>
<p><img src="https://i.imgur.com/ob3Smaa.png"
alt="image-20210709170526824" /></p>
<p>與目前的 SOTA 比較：</p>
<p><img src="https://i.imgur.com/46THxYA.png"
alt="image-20210709170657400" /></p>
<h2 id="結論">結論</h2>
<p>Deformable DETR 透過使用 Deformable 的方法來使 Transformer
中的運算數減少許多，不再 depend on
圖片大小，而且因運算減少所以可以加上類似 FPN 的多重解析度。效果比 DETR
好一點點</p>
<p>比較神奇的是不知道為什麼 BBox 的預測又跑回去 YOLO
那一套了，說是比較好收斂啦…</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/342261872</p>
<p>https://blog.csdn.net/irving512/article/details/109713148</p>
<p>https://www.jianshu.com/p/8524abf10018</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: An Image is Worth 16x16 Words:Transformers for Image Recognition at Scale - 正式開始 Transformer 元年</title>
    <url>/2021/07/09/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale-%E6%AD%A3%E5%BC%8F%E9%96%8B%E5%A7%8B-Transformer-%E5%85%83%E5%B9%B4/</url>
    <content><![CDATA[<p>如果說之前的 DETR 是 Transformer 系列的開山始祖的話，那 ViT
就一定是發揚光大的人了。2020 Google 提出了 Vision
Tranformer，提一個完全不用 CNN 只使用 Transformer
的網路架構，整體來說網路架構並不複雜，但對後來的影響力可不小，從 ViT
之後的論文名字都會變成 …T 什麼什麼 Transformer
的意思，而我系列的文章也改名為：「Vision Transformer 演化史」。</p>
<p><a
href="https://arxiv.org/pdf/2010.11929.pdf">https://arxiv.org/pdf/2010.11929.pdf</a></p>
<p>keywords: <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p>這篇論文的名字：An Image is Worth 16x16
Words，彷彿就在告訴我們如果把圖片切成一塊一塊的，是不是就能變成一串
Sequence 呢？這樣就可以放近 Transformer
中訓練了。而這篇論文提出的方法非常簡單，完全沒有使用任何 CNN
架構，以使用「最原汁原味」的 Transformer 為主要目標。告訴大家：其實只用
Transformer 在分類上效果很不錯喔！</p>
<h2 id="網路架構">網路架構</h2>
<p>以下為 ViT 網路架構，可發現只使用了 Transformer 的 Encoder，沒有使用
Decoder，(我個人理解為，Transformer 設計初衷是要
Seq2Seq，但分類問題不需要輸出 Sequence 所以把 Decoder
取消掉了)。以下一一介紹。</p>
<p><img src="https://i.imgur.com/xGVHCtB.png"
alt="image-20210710132412756" /></p>
<h3 id="圖片預處理由圖片變為-patch">圖片預處理：由圖片變為 Patch</h3>
<p>這一步最大的精神就是，想辨法把三維圖片 <span
class="math inline">\((HWC)\)</span> 表示成二維 sequence <span
class="math inline">\((ND)\)</span>。sequence 中的每一塊稱作為一個
Patch。而這篇文提出「切塊 (Patch)」的方法。具體做法如下：</p>
<p>把 <span class="math inline">\(x\in H\cdot W\cdot C\)</span>
根據切塊圖片大小 <span class="math inline">\(P\)</span> 變成一個 <span
class="math inline">\(x_p\in N \cdot(P^2 \cdot C)\)</span>
的二維向量，而 <span class="math inline">\(N\)</span> 等於 <span
class="math inline">\(HW/P^2\)</span>，也就是說 squence
可表示成：特徵數長度為 <span
class="math inline">\(C\)</span>，一塊大小為 <span
class="math inline">\(P^2\)</span>，sequence 長度為 <span
class="math inline">\(N = HW/P^2\)</span> 的
sequence。嗯…用文字好像不好描述，看圖。</p>
<p>左手邊原圖大小為 <span class="math inline">\(H\cdot W\cdot C\)</span>
記為 <span class="math inline">\(x\)</span>，而切塊後大小為 <span
class="math inline">\(N \cdot(P^2 \cdot C)\)</span> 的向量記為 <span
class="math inline">\(x_p\)</span></p>
<p><img src="https://i.imgur.com/mG3JoYk.png"
alt="image-20210710135026339" /></p>
<h3 id="patch-embedding">Patch Embedding</h3>
<p>得到 <span class="math inline">\(x_p\)</span> 後，要再把維度 <span
class="math inline">\(N \cdot(P^2 \cdot C)\)</span> 轉換成 <span
class="math inline">\((N\cdot D)\)</span> ，而 <span
class="math inline">\(D\)</span> 是自定義的參數，目的是做維度的整理
(或說降低維度) (假設從 3072 變成 1024)</p>
<p>做法是經過一個可學習的 Linear 層 <span
class="math inline">\(E\)</span> 來得到</p>
<p>公式如下：</p>
<p><span class="math display">\[
z_0 = [x_{class}; x^1_pE; x^2_pE;...;x^N_pE] + E_{pos}
\]</span></p>
<p>把預處理得到的 Patch ，再經過一個可學習 Linear <span
class="math inline">\(E\)</span> ，得到最後輸進網路的 <span
class="math inline">\(D\)</span>。這一步稱作 Patch Embedding</p>
<h3 id="class-token">class token</h3>
<p>接著把 <span class="math inline">\(N\)</span> 加上 1，多了一個 <span
class="math inline">\(x_{class}\)</span>
輸出。為什麼要加上這個東西呢…？我們可以回想上一章提到的 Object query
，假設我們 <span class="math inline">\(N=9\)</span> 代表我們有 9 個表示
Object 大小位置一些特性的向量，新增一個格子有點像新增一個 query 去和其它
9 個向量做 self attention
的感覺，而這新增的格子就是用來輸出分類結果，運算中會與其它 9 個格子做
self attention 計算相似度，找出最有可能的結果。而 <span
class="math inline">\(x_{class}\)</span> 是一個可學習的向量，通常是加在
0 這個地方。</p>
<h3 id="position-embedding">Position Embedding</h3>
<p>最後照著 Transformer 的傳統，加上代有位置訊息的 Position
embedding，只不過這裡 ViT 使用的不是 sincos
那樣固定的編碼，而是使用可自行訓練的變量。以下為視覺化的 Position
embedding 發現好像有那麼一點規律可循。</p>
<p><img src="https://i.imgur.com/6pEJ6Rh.png"
alt="image-20210710141000186" /></p>
<h3 id="encoder">Encoder</h3>
<p>Encoder 的地方真的是什麼也沒動，頂多最後輸出的 <span
class="math inline">\((N, B, C)\)</span> 向量經過一個全連接層變成 <span
class="math inline">\((N, B, class\_num)\)</span> 而已。</p>
<p><img src="https://i.imgur.com/mscGLxB.png"
alt="image-20210710141119408" /></p>
<h2 id="訓練方式">訓練方式</h2>
<p>這篇論文使用 Transfer Learning
的方法，先在大數據集上預訓練，在放到小數據集上 fine
tune。(後面會講效果)</p>
<p>同時設計了三種大小不同的模型：</p>
<p><img src="https://i.imgur.com/yvlYUpo.png"
alt="image-20210710142220216" /></p>
<h2 id="experiments">Experiments</h2>
<p>實驗用到數據集有：(越往下越難)</p>
<ul>
<li>ImageNet -&gt; 1000 classes</li>
<li>ImageNet-21k -&gt; 21k classes</li>
<li>JFT -&gt; 18k classes</li>
</ul>
<p><strong>實驗一、對比 CNN</strong> 這篇論文因為使用 Transfer Learning
(等等會提到更深入) 所以選用 Big Transfer (BiT) 以及 Noisy Student
來做比較。</p>
<p><img src="https://i.imgur.com/vLeP9Hx.png"
alt="image-20210710141942775" /></p>
<p>可以發現比 BiT 效果好一些，重點是參數的使用量！少非常非常多！</p>
<p><strong>實驗二、對比數據集</strong>
作者對比了不同大小的數據集，以及不同架構的網路，得出以下圖片：</p>
<p><img src="https://i.imgur.com/ucDF7IS.png"
alt="image-20210710142532275" /></p>
<p>發現一件重要的事情：</p>
<p><strong>在小預訓集上訓練時效果不比 CNN 好，但在大預訓集上 Transformer
的強大顯現出來了</strong> <strong>在小預訓練集上 Residual
還是比較強，在大預訓練集上 attention 才發揮能力</strong></p>
<h3 id="細看-transformer">細看 Transformer</h3>
<p>作者把 patch embedding 中的 <span
class="math inline">\(E\)</span>，做可視化分析，發現特別的地方是，patch
embedding 學到的東西與 CNN 有幾分相似，都是一些基本的特徵組合</p>
<p><img src="https://i.imgur.com/Eq2nJx9.png" alt="Image" /></p>
<p>接著作者分析了在 self-attention layer 中 各個 attention head
與各層之間的關系，以 Mean attention distance 作為分析目標。</p>
<p>Mean attention distance 的意思指的是，一個 pixel 能最遠與附近的其它
pixel 做相關性運算，也可以理解為就是 CNN 中的 receptive field
(空間感知域)</p>
<p>依據實驗結果可看到在網路第一層，假設網路中有 16 個 head，這 16 個
head 它們的 receptive field 有的大有的小，有些 head 天生就可以有比較
Global 的感知域，而有些則是比較 Local 的感知域。</p>
<p>隨著層數的增加，每個 head 的 receptive field
也隨之增加，意謂著層數越深越能看到更全局 Global 的資訊</p>
<p>與 CNN 不一樣的是，CNN
在一開始並不會出現全局的感知域，而是像底下藍線一樣，隨著層數而呈線性關性，但
Transformer 能做到的是紅色圈圈部份，這些早期全局資訊是 CNN
所沒有的。</p>
<p><img src="https://i.imgur.com/spEkO2q.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>這篇論文實作出一個完全 Transformer based 的方法解決分類問題，由於
Transformer 在大訓練集上的效果比較好，因此如果要使用的話，會要使用
Transfer learning 最後在 fine tune 這樣。</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/356155277</p>
<p>https://zhuanlan.zhihu.com/p/342261872</p>
<p>https://www.youtube.com/watch?v=j6kuz_NqkG0&amp;t=1173s</p>
<p>https://www.youtube.com/watch?v=TrdevFK_am4</p>
<p>https://www.youtube.com/watch?v=DVoHvmww2lQ</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Big Transfer (BiT) - Transfer Learning 的總結</title>
    <url>/2021/07/15/Big-Transfer-BiT-Transfer-Learning-%E7%9A%84%E7%B8%BD%E7%B5%90/</url>
    <content><![CDATA[<p>在 2020 同樣熱門的研究主題還有 pre-training、fine tune
這一個領域，一個 Google 大神又再次以 BiT
這篇論文，結出了一個簡單全面的結論，來看看 pre-training
可以做到什麼程度，效果如何</p>
<p>keywords: Big Transfer、pre-training <span id="more"></span></p>
<h2 id="介紹">介紹</h2>
<p>這篇論文非常簡單，全部網路架構使用了 ResNet 50
做為實驗對照，分別在三種不同大小的資料集上實驗，分別是：ILSVRC-2012
(就是 ImageNet 最原始的版本)、ImageNet-21k (ImageNet 加強版)、JFT-300M
(傳說是 Google 內部的資料集，未開放)。來看看不同大小的資料集對
pre-training 有什麼影響</p>
<h2 id="改進">改進</h2>
<p>有三個不同方向的改進：</p>
<ul>
<li>scale，不同大小的資料集就用不同大小的網路來訓練，也就是大資料集用大模型、小資料集用小模型</li>
<li>使用 Group Norm 以及 Weight Standardization 來替換掉 Batch
Normalization。作者認為 BN 對 Transfer Learning
有很大的影響，因為模型太大，所以一次的 Batch 不能設太大，所以 BN
取平均下來反而效果不好</li>
<li>使用 BiT-HyperRule，設定了 training schedule, resolution, whether to
use MixUp regularization ，三個方面</li>
</ul>
<p><img src="https://i.imgur.com/9JvaQln.png"
alt="image-20210715145932148" /></p>
<h2 id="pre-training">pre-training</h2>
<p>真的 pre-training 在效果上會比較好嗎？真的會比從頭開始 (Train from
scratch) 好嗎？簡單來說結論如下：</p>
<p>自己的模型使否有公開 pre-training 好的參數。若有，選pre-training
任務最接近的參數來用。 若沒有，觀察自己的dataset數量是否夠、是否有足夠的
variance。若足夠全面，可以直接train from
scratch，訓練過程的中前段可以留一個 checkpoint 加速未來實驗。
若沒有，挑選 domain 盡量接近的公開 dataset 自己pre-training。</p>
<h2 id="結論">結論</h2>
<p>pre-training
的確有用，而且在越複雜的模型上要使用越大的資料集來訓練。最後再使用自己的
data 來 fine tune
，不僅效果可能會好一點，但肯定的是，訓練起來的時間一定少了不少。</p>
<h2 id="reference">Reference</h2>
<p>https://medium.com/%E8%BB%9F%E9%AB%94%E4%B9%8B%E5%BF%83/deep-learning-%E4%BD%BF%E7%94%A8pre-training%E7%9A%84%E6%96%B9%E6%B3%95%E8%88%87%E6%99%82%E6%A9%9F-b0ef14e777e9</p>
<p>https://zhuanlan.zhihu.com/p/142864566</p>
<p>https://blog.csdn.net/qq_14845119/article/details/106825219</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>pre-training</tag>
      </tags>
  </entry>
  <entry>
    <title>嘎拉賀野溪溫泉一日遊</title>
    <url>/2021/07/23/%E5%98%8E%E6%8B%89%E8%B3%80%E9%87%8E%E6%BA%AA%E6%BA%AB%E6%B3%89%E4%B8%80%E6%97%A5%E9%81%8A/</url>
    <content><![CDATA[<p>趁著烟花颱風來還沒來前又去了一趟野溪溫泉XD，一來因為是颱風前所以天氣特別好，二來也沒有什麼人。這一次去的是嘎拉賀野溪溫泉，位在台
7 線過了巴陵大橋後的光華道路內的嘎拉賀部落上。嘎拉賀部落在光華道路約 14
公里處，其實算是有點距離，要上山前記得要抓好時間喔。</p>
<p>keywords: 嘎拉賀野溪溫泉 <span id="more"></span></p>
<p>早上 6:00 一大早就出門了
XD，為了趕在太陽下山前趕快離開山下，主要是擔心颱風快要來的原因，畢竟山上天氣變化多端。不愧也多虧了颱風今天天氣真好
XD。下面是途中在三民的萊爾富停下來休息的照片。</p>
<p>阿對了這一次我們是騎車上山，有鑒於上次開車的痛苦
XDDD，真的不要隨便開山路阿，那個累的程度完全無法想像，而且最重要的一點：開車會比騎車還慢！！</p>
<p><img src="https://imgur.com/3kCLt8x.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/DXmTS8W.png" alt="Image" /></p>
<p>過了巴陵大橋後轉近光華道路，大約騎車騎了 30
分鐘，終於到了溫泉的入口啦！可以看到接下來的路變成水泥路了，而且「超極陡」，還是不要隨便挑戰車子的極限好了
XDD，不然下得去上不來</p>
<p><img src="https://i.imgur.com/IROHMEx.png" alt="Image" /></p>
<p>嘎拉賀溫泉又稱新興溫泉，名字取自附近的嘎拉賀部落，後來改名成新興部落因而得名，不過大家都還是比較習慣叫它嘎拉賀溫泉啦。嘎啦賀溫泉的特色就是它無色無味，屬於碳酸鹽沉澱的溫泉。</p>
<p><img src="https://i.imgur.com/DAcAWWR.jpg" alt="Imgur" /></p>
<p>在附近的山上隨手拍，這裡的海拔真的很高呢，風景很壯闊</p>
<p><img src="https://i.imgur.com/2oOknEp.jpg" alt="Imgur" /></p>
<p>接下來就準備前往嘎啦賀溫泉啦，其實一路上一半是水泥路一半是石階走起來不難走
(跟上次四稜溫泉比起來…)，但是這個坡度真的有點太陡了…，對我的膝蓋挺傷的阿…</p>
<p><img src="https://i.imgur.com/dNFRrq5.jpg"
alt="image-20210723225119394" /></p>
<p>前半段超陡的水泥路，千萬不要騎車下去…很有機會騎不上來。這個說個題外話，在來嘎啦賀的路上，剛好在一處崩塌處遇到一位正在巡邏中的員警，除了好心提醒我們颱風快要來了要早點下山外，也特別提到「千萬」不要騎車下去。親自體會過更能覺得真是如此呢…</p>
<p><img src="https://i.imgur.com/dbm3p43.jpg" alt="Imgur" /></p>
<p>看到這個涼亭就代表路已經走了一半啦</p>
<p><img src="https://i.imgur.com/6oxeOLT.jpg" alt="Imgur" /></p>
<p>接下來全剩下這種看起來像好漢坡的「無限階梯」</p>
<p><img src="https://i.imgur.com/okFnHJV.jpg" alt="Imgur" /></p>
<p><img src="https://i.imgur.com/0ce9Bxw.jpg" alt="Imgur" /></p>
<p>大概走了快一個小時吧，會先看到一座露營地</p>
<p><img src="https://i.imgur.com/2rOAjFz.png" alt="Image" /></p>
<p>接著會到一個大石頭的地方，嗯…很適合來拍些網美照呢 www</p>
<p><img src="https://i.imgur.com/l3QksN6.png" alt="Image" /></p>
<p>最後步道的終點是一個超長的梯子，這個梯子好像有整修過一樣，跟我們之前在網路上作功課看到的好像升級了不少
w</p>
<p><img src="https://i.imgur.com/MNbpLMK.jpg" alt="Imgur" /></p>
<p><img src="https://i.imgur.com/3HjL8oT.png"
alt="image-20210723231111872" /></p>
<p>梯子往下爬後就到我們今天的目的地啦，話不多說，先上圖來體會大自然的美景。</p>
<p><img src="https://i.imgur.com/cnePUne.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/kgjTQQS.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/CIYCgAf.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/ou88zf1.jpg" /></p>
<p>來張全景圖！</p>
<p><img src="https://i.imgur.com/See6HfS.jpg" alt="Imgur" /></p>
<p>過到河岸後，那兩個小瀑布湧出來的水就是溫泉啦</p>
<p><img src="https://i.imgur.com/aXVyU1P.jpg" alt="Imgur" /></p>
<p>可以看到有人用石頭圍出一個小池子呢</p>
<p><img src="https://i.imgur.com/D8lGHwM.jpg" alt="Imgur" /></p>
<p>最裡面的那池水最大，同時水溫也是最燙的，真的超極燙</p>
<p><img src="https://i.imgur.com/VAEMr5L.jpg" /></p>
<p>泡完溫泉後就來釣魚啦</p>
<p><img src="https://i.imgur.com/0iidryQ.jpg" /></p>
<p>釣完後就來烤個肉
(哈哈固定流程)，就在剛剛的露營地起火，當然垃圾不落地囉，烤完後垃圾收好收滿，留給下一個來使用的人</p>
<p><img src="https://i.imgur.com/wPPoPCU.jpg" alt="Imgur" /></p>
<p><img src="https://i.imgur.com/FsOnniT.jpg" /></p>
<p>烤完肉後準備回家啦啦，第一個要先面對的就是那無止盡的階梯…，阿爬得好累
w</p>
<p><img src="https://i.imgur.com/azG4I9f.jpg" alt="Imgur" /></p>
<p>除中經過一棵看起來很有靈性的樹，在陽光的照射下好像有那麼一回事
(山中這樣的風景真的不嫌少)</p>
<p><img src="https://i.imgur.com/RrGZYL8.jpg" /></p>
<p>最後花了 1
個半小時的時間，終於爬回停車的地方啦啦，結束今天的旅程趕緊下山回家躲颱風。</p>
<p>回家途了在北橫公路旁休息時，順手照了一張山間與公路的美景，阿…在山中這樣的美景真的是混然天成</p>
<p><img src="https://i.imgur.com/mBbtn4F.jpg" alt="Imgur" /></p>
<p>結論：</p>
<p>這次好再在颱風來前快快得去了一趟野溪溫泉，對比上一次的四稜溫泉，我個人覺得嘎啦賀溫泉對新手比較友善，難度比較底，而且溫泉比較燙泡起來比較爽
XD
，但是四稜溫泉的特色就是它是有硫磺味的溫泉，有特別喜歡這個屬性的人，比較推四稜溫泉喔！</p>
<p>阿天氣好的北橫真的很漂亮呢…我真的好喜歡最後一張照片 XD</p>
]]></content>
      <categories>
        <category>遊記</category>
      </categories>
      <tags>
        <tag>嘎拉賀野溪溫泉</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Training data-efficient image transformers &amp; distillation through attention - DeiT 使用知識蒸餾來改進 ViT 要使用大訓練集的缺點</title>
    <url>/2021/07/24/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Training-data-efficient-image-transformers-distillation-through-attention-DeiT-%E4%BD%BF%E7%94%A8%E7%9F%A5%E8%AD%98%E8%92%B8%E9%A4%BE%E4%BE%86%E6%94%B9%E9%80%B2-ViT-%E8%A6%81%E4%BD%BF%E7%94%A8%E5%A4%A7%E8%A8%93%E7%B7%B4%E9%9B%86%E7%9A%84%E7%BC%BA%E9%BB%9E/</url>
    <content><![CDATA[<p>讀完 Google 發表的 ViT
論文後，不禁讓人覺得：哇塞這樣也行！，直接把圖片用一個字串來表示放進
Transformer 中。然而在原論文中也明確提到了：「that transformers do not
generalize well when trained on insufficient amounts of
data.」，意思即是在資料集不大的情況下 Transformer 的效果是比 CNN
還是來得差的，因此 Google 大神使用了 JFT-300 這個資料集做 pre-training
，但…Google 沒跟你說的是，這個資料不公開阿。因此 Facebook 提出 DeiT
模型，使用 distillation 的方法只需要使用 ImageNet
就可以有不錯的效果。</p>
<p>https://arxiv.org/pdf/2012.12877.pdf</p>
<p>keywords: DeiT, distillation <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>作者提出了一新架構叫 Data-efficient image Transformers，簡稱
DeiT。旨在用更少的資料集也能達到 CNN sota 的效果。</p>
<p>DeiT 一共有下列幾項特色：</p>
<ol type="1">
<li>整個架構不使用任何 CNN，全為 Transformer，在同樣都是使用 ImageNet
來訓練的前提下，與純 CNN 的 sota 效果不相上下。</li>
<li>提出了一個根據 Transformer 設計的 distillation 流程，用上了叫
distillation token 的東東。distillation token 會與 class token 在
Transformer 中不斷的交互做計算。使結果更好 (後續會細講)</li>
</ol>
<p>下圖為論文中使用 ImageNet 做訓練 ImageNet 做測試的結果，可發現 ViT
在小資料集的效果確實不太出色。</p>
<p><img src="https://i.imgur.com/r083nae.png" alt="Image" /></p>
<h2 id="distillation-through-attention">2. Distillation through
attention</h2>
<p>與一般我們認識的 Distillation 不一樣的是，DeiT 提出了一個新的
Distillation token 流程，並且比較了 Soft Distillation 與 Hard-label
Distillation 的差別、及傳統 Distillation 與 Distillation token
的差別</p>
<p>讓我們先回想一下 ViT 的做法…，ViT 在 Encoder 輸入字串長度 <span
class="math inline">\(N\)</span> 的地方改成 <span
class="math inline">\(N+1\)</span> 並取名叫 class token，而其餘的 <span
class="math inline">\(N\)</span> 取名叫 patch token，class token
目的在於輸出分類結果，最後直接經過一個 softmax 就是最後輸出了。</p>
<p><img src="https://i.imgur.com/w1oU12e.png" alt="Image" /></p>
<h3 id="soft-distillation-vs-hard-label-distillation">Soft Distillation
vs Hard-label Distillation</h3>
<p>Distillation 分為兩種一是「軟蒸餾」Soft
Distillation、一是「硬蒸餾」Hard-label Distillation</p>
<p><strong>Distillation</strong>：首先最原始的蒸餾指的是：Student model
經 softmax 後的結果與 ground truth 做 cross entropy：<span
class="math inline">\(\mathcal{L}_{CE}\)</span> 為 cross entropy、<span
class="math inline">\(\psi\)</span> 為 softmax、<span
class="math inline">\(y\)</span> 為 ground truth、<span
class="math inline">\(Z_s\)</span> 為 student 的 logits</p>
<p><span class="math display">\[
\mathcal{L}_{Distillation} = \mathcal{L}_{CE}(\psi(Z_s), y)
\]</span></p>
<p><strong>Soft Distillation</strong>：簡單說就是把蒸餾式子多加上一個與
Teacher model 的 logits 互做 KL Divergence：<span
class="math inline">\(Z_t\)</span> 為 teacher 的 logits、<span
class="math inline">\(Z_s\)</span> 為 student 的 logits (<span
class="math inline">\(\lambda\)</span> <span
class="math inline">\(\tau\)</span> 為超參數)</p>
<p><span class="math display">\[
\mathcal{L}_{Soft} = (1-\lambda)\mathcal{L}_{CE}(\psi(Z_s), y) + \lambda
\tau^2KL(\psi(Z_s / \tau), \psi(Z_t / \tau))
\]</span></p>
<p><strong>Hard
Distillation</strong>：這個方法是這篇論文所提出來的，加上的部份改成為
student model 的結果與 teacher model 的結果做 Cross
Entropy，可理解為：同時與真正的 Ground truth 與 把 Teacher model
產生的結果當成 Ground truth 各做一次 Cross Entropy：<span
class="math inline">\(y_t\)</span> 定義為 <span
class="math inline">\(y_t = \mathrm{argmax}_cZ_t(c)\)</span> 即 teacher
model 經 softmax 的最後結果</p>
<p><span class="math display">\[
\mathcal{L}_{Hard} = \frac12\mathcal{L}_{CE}(\psi(Z_s), y) +
\frac12\mathcal{L}_{CE}(\psi(Z_s), y_t)
\]</span></p>
<p>我個人的理解為 Soft 加上了 student logits 與 teacher logits
之間的差，多了一個兩模型結果的比較項。而 Hard 的做法則是與 teacher 經過
softmax 最後處理得到的結果做 CE
。感覺一個是與前資料做比較、一個是與處理後資料做比較的概念。</p>
<h3 id="distillation-token">Distillation token</h3>
<p>與 ViT Transformer 不同的地方是，DeiT 在 token 的地方新加了一個
Distillation token，型成一個 <span class="math inline">\(N + 2\)</span>
的字串，與 Class token 計算方法一樣，會與所有的 token 一起做
attention。唯一的區別在於：</p>
<p>class token 目標與 GT 一樣、而 distillation token 目標與 teacher
結果一樣</p>
<p><img src="https://i.imgur.com/Qo7IYYS.png" alt="Image" /></p>
<p>而這個 Distillation token 最後對應到的就是 Distillation loss
(蒸餾損失)，可以選擇使用 hard distillation loss 或者 soft distillation
loss，加上這一項的 Loss funtion 可以讓我們在調整 Loss 時，可以多根據
teacher model 的結果來調整，也就是說最後的 Loss funtion 如下式：</p>
<p><span class="math display">\[
\mathcal{L}_{\mathrm{total}} = \mathcal{L}_{\mathrm{CE}} +
\mathcal{L}_{\mathrm{teacher(Distillation)}}
\]</span></p>
<p>以上就是 DeiT 全部的核心概念了，其實不難理解，就只是單純在
Transformer 後多加一個 token ，但目標與 class token
不一致。以下實驗來看看是不是有加新 token 的必要性。</p>
<h2 id="experiment">3. Experiment</h2>
<h3 id="deit-架構參數">DeiT 架構參數</h3>
<p>DeiT 分三個架構，依照參數量由小排至大，其中 DeiT-B 是參數最大，且與
ViT-B 參數量相同的架構</p>
<p><img src="https://i.imgur.com/kdyA0Dt.png" alt="Image" /></p>
<h3 id="實驗一哪種-teacher-model-更好">實驗一、哪種 Teacher model
更好？</h3>
<p>作者選用 FB 之前提出近似 NAS 想法的網路 RegNet 來做 Teacher
model，結果如下：</p>
<p><img src="https://i.imgur.com/FvdgxsR.png" alt="Image" /></p>
<p>嗯…當然，teacher model 架構越大效果越好。⚗ 代表蒸餾的意思
(也太可愛)</p>
<h3 id="實驗二哪種-distillation-方法更好">實驗二、哪種 Distillation
方法更好？</h3>
<p>作者提出三種不同的蒸餾方法，普通、軟蒸餾、硬蒸餾</p>
<p><img src="https://i.imgur.com/C1bIoGz.png" alt="Image" /></p>
<p>可以看到硬蒸餾效果最好</p>
<h3 id="實驗三哪種-token-的組合效果最好">實驗三、哪種 token
的組合效果最好？</h3>
<p>一共有：只使用 class token、只使用 distillation
token、以及兩個都使用</p>
<p><img src="https://i.imgur.com/SVDMCCG.png" alt="Image" /></p>
<p>這個實驗證明加上 distillation token 真的會讓結果好一些些，大概 0.6
%</p>
<h3 id="實驗四與-sota-的對比">實驗四、與 SOTA 的對比</h3>
<p><img src="https://i.imgur.com/PjBA2DI.png" alt="Image" /></p>
<h3 id="實驗五性能對比">實驗五、性能對比</h3>
<p>當然要來與 ViT 做一下比較啦，DeiT 用上了 distillation
因此在模型的運算速度上面非常的有優勢</p>
<p><img src="https://i.imgur.com/A1kvC5m.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>DeiT 使用 distillation 解決了 ViT
一定要用大資料集訓練效果才好的問題，好訓練、執行速度也快是它的一大特色</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/349315675</p>
<p>https://zhuanlan.zhihu.com/p/51431626</p>
<p>https://zhuanlan.zhihu.com/p/102038521</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Visual Transformers: Token-based Image Representation and Processing for Computer Vision - 使用 visual token 來強化傳統 CNN 的結果</title>
    <url>/2021/07/26/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Visual-Transformers-Token-based-Image-Representation-and-Processing-for-Computer-Vision-%E4%BD%BF%E7%94%A8-visual-token-%E4%BE%86%E5%BC%B7%E5%8C%96%E5%82%B3%E7%B5%B1-CNN-%E7%9A%84%E7%B5%90%E6%9E%9C/</url>
    <content><![CDATA[<p>這是一篇來自 UC Berkeley 的論文，論文提出了基於 Transformer
的一個類似強化的模組 Visual Transformer (visual
token)，可以加在任何現有的 Backbone 或是 FPN
上，可以比原架構效果好一些些，重要的是大大減少了參數運算量。</p>
<p><a
href="https://arxiv.org/pdf/2006.03677">https://arxiv.org/pdf/2006.03677</a></p>
<p>keywords: Visual Transformer、Tokenizer <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>作者發現傳統的 CNN 有以下三大缺點：</p>
<ol type="1">
<li>圖上的每一個 pixel 重要性都是不一的</li>
</ol>
<p>在 CNN 的每個 kernel 中的像素，它們的權重 (重要性)
是視為相當的，均匀排列的像素矩陣 (uniformly-arranged pixel
arrays)。但這個就會產生一個問題，如果今天的 task 是語意分割，CNN
會把物體、背景視為一樣重要的東西，使我們更難分離出偵測物體與背影</p>
<ol start="2" type="1">
<li>不是每一個圖片都可以表達「整體」</li>
</ol>
<p>這句話的意思是說，CNN
在處理小物件上非常強，像是直線、角落等等…，但是當這個小組件共同組合成大物件時，例如車子、房子，受限於
CNN 的 kernel size 處理起來並不直覺 (常見的做法有：增加 kernel
size、增加深度、放大倍率…)</p>
<ol start="3" type="1">
<li>CNN 對於遠距離的關聯性很弱</li>
</ol>
<p>同樣是受限於 kernel size 的問題，在一張圖片中距離相隔遙遠的兩點，CNN
是做不太到計算兩者之間的相關性的。同樣可以使用增加 kernel
size、增加深度來解決，但是付出的代價就是計算量上升。</p>
<p>因此作者提出了 Visual Transformer (VT) 架構 (嗯…這個名字好容易跟
Vision Transformer 搞混阿 XD)，使用 Visual Token
來描述高階圖片的特徵，並且用到了類似 Spatial attention 的概念來生成
Visual Token。接著把 Visual Token 放入 Transformer 中，透過 Transformer
可以找到 Token 與 Token 間的重要性。</p>
<p>這樣子 VT 就可以改進以下三點：</p>
<ol type="1">
<li>關注到重要的地方，而不像 CNN 一樣，全部視為一樣重要</li>
<li>多了一個 Visual Token 類似多了一個語意編碼 semantic tokens
的資訊來加強結果</li>
<li>使用 Transformer 建立 Token 之間的關系</li>
</ol>
<p>以上是論文原文的話，我個人的讀後想法是：CNN
在小物件上的偵測效果很強，但是隨著物件的放大，CNN
雖然說可以透過加廣加深來解決這個問題，但是付出的運算量也是非常可觀的。而
Transformer
在處理大物件上之間的關聯很強，因此本論文的作者試著將兩者結合，既有 CNN
小物件的強處，到了中後段改使用 Transformer 進一步分析結果。</p>
<h2 id="visual-transformer-vt">2. Visual Transformer (VT)</h2>
<p>以下細講 Visual Transformer (VT) 的架構流程</p>
<p><img src="https://i.imgur.com/6cBVuly.png" alt="Image" /></p>
<p>給定一張圖片，先對它做 CNN 層層卷積找到 low-level 低階特徵，輸出一個
feature map，接著通過一個 tokenizer 把 feature 轉換成 visual
tokens，其中這每一個 visual token 都代表一個 semantic concept。再把
visual token 放進 Transformer 中輸出也是 visual token，而這些 visual
token 可以直接當成分類的結果，或是可以再經過一個 Projector
變成語意分割任務。</p>
<p>如果我上述結論：作者先讓 CNN 處理低階特徵，再來用 Transformer
來處理高階特徵。</p>
<p>接著來依序講講</p>
<h3 id="tokenizer">Tokenizer</h3>
<p><strong>Filter-based Tokenizer</strong></p>
<p><img src="https://i.imgur.com/BgG41c0.png" alt="Image" /></p>
<p>先上公式：</p>
<p><span class="math display">\[
T=\mathrm{SOFTMAX}_{HW}(XW_A)^TX
\]</span></p>
<p>feature map <span class="math inline">\(X\)</span> 會先做一個 1x1
conv 從 <span class="math inline">\(HW \cdot C\)</span> 變成 <span
class="math inline">\(HW \cdot L\)</span> 得到一個 Spatial attention A
，<span class="math inline">\(XW_A\)</span> 接著把結果轉至 <span
class="math inline">\(A^T = (XW_A)^T\)</span> 再與原圖相乘 <span
class="math inline">\((XW_A)^TX\)</span> ，就得到最後的 Visual tokens
了</p>
<p>因為是透過 1x1 conv 來找尋特徵，所以稱為 Filter-based Tokenizer</p>
<p><strong>Recurrent Tokenizer</strong></p>
<p>為了加強 Filter-based Tokenizer 的不足，作者又提出了 Recurrent
Tokenizer 方法，簡單來說就只是把：第一次生成出來的 Visual tokens
拿來當成第二次生成的依據</p>
<p><img src="https://i.imgur.com/grv288k.png" alt="Image" /></p>
<p>公式如下：</p>
<p><span class="math display">\[
\begin{gathered}
W_R = T_{in}W_{T\rightarrow R} \\
T=\mathrm{SOFTMAX}_{HW}(XW_R)^TX
\end{gathered}
\]</span></p>
<p>所有有變動的地方就是從 <span class="math inline">\(W_A\)</span> 變為
<span class="math inline">\(W_R\)</span> 了。 首先上一次生成的 Visual
Token 會先乘生一個神奇的 <span class="math inline">\(W_{T\rightarrow
R}\)</span> ，維度大小為 <span class="math inline">\(W_{T\rightarrow
R}\in\R^{c\times c}\)</span> (我真的看不出來這個 <span
class="math inline">\(c \times c\)</span> 倒底從哪裡生出來？)
後序步驟與上面一致</p>
<h3 id="transformer">Transformer</h3>
<p>與原版 Transformer 有一點點不同，公式如下：</p>
<p><span class="math display">\[
T_{out} = T_{in} + \mathrm{softmax}_L((T_{in}Q)\cdot(T_{in}K)^T)\cdot
T_{in}
\]</span></p>
<p><span class="math display">\[
T_{out} = T_{out}&#39; + \sigma(T_{out}&#39;F_1)F_2
\]</span></p>
<p>其中 Query 與 Key 互做運算後，乘上的 Value 並沒有經過 1x1 conv
分割，而是乘上整體。 接下來就是 Add &amp; Norm 的部分了</p>
<h3 id="projector">Projector</h3>
<p>如果要把結果進一步成語意分割任務的話，作者認為再經過一步 Projector
效果會比較好，而 Projector 最主要的目的是把 Visual token
轉回用像素的方式來表達，這樣在以像素分割時效果較好。</p>
<p>公式如下：</p>
<p><span class="math display">\[
X_{out} = X_{in} +
\mathrm{softmax}_L((X_{in}W_Q)\cdot(X_{in}W_K)^T)\cdot T_{in}
\]</span></p>
<p>其中 <span class="math inline">\(X_{in}\)</span> 為 CNN
生出的最後一層特徵圖 可以看到 attenion 公式中，<span
class="math inline">\(X_{in}\)</span> (Query) 與結果 <span
class="math inline">\(T\)</span> (Key) 互相做運算，最後再乘上全部的
<span class="math inline">\(T\)</span> (Value) 也就是說 Projector
的重點是 原 feature map 與 visual token 互做運算的結果</p>
<p>最後把得到的重點特徵加會原圖，就是最後的結果了。</p>
<h2 id="用法">3. 用法</h2>
<p>這個 Visual Transformer
最強的地方在於，它是一個「模組」，因此可以安插在任何現有的網路模型之中。</p>
<h3 id="放在-resnet-中">放在 ResNet 中</h3>
<p>把最後一個 Stage 直接改為 Visual
Transformer，可看到效果好了一些些，運算量也少了一些些</p>
<p><img src="https://i.imgur.com/0MFnupv.png" alt="Image" /></p>
<h3 id="放在-fpn-中">放在 FPN 中</h3>
<p><img src="https://i.imgur.com/KYuX8xA.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/Emxn71Z.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>這是一篇試著把 CNN 與 Transformer 結合的一篇論文，提出了一個基於
Transformer
的「模組」，而可以達到效果好一些些，同時運算量也下降一些些的優勢。(但我個人覺得…這篇論文在
VT 的部分有一些地方沒說清楚…，那個 <span class="math inline">\(c \times
c\)</span> 倒底怎麼來的阿…)</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/349315675</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge Distillation 知識蒸餾 &amp; Noisy Student</title>
    <url>/2021/07/15/Knowledge-Distillation-%E7%9F%A5%E8%AD%98%E8%92%B8%E9%A4%BE-Noisy-Student/</url>
    <content><![CDATA[<p>2020 由於 BERT 在 NLP 的成功，Active Learning 與 Semi-supervised
Learning 研究是相當熱門的一年，Google 提出的 Noisy Student 藉由 Teacher
Student model 彼此之間的相互訓練，以及在 Student
加中雜訊來得到更好的結果。</p>
<p>https://arxiv.org/pdf/1503.02531 https://arxiv.org/abs/1911.04252</p>
<p>keywords: Knowledge Distillation、Noisy Student <span id="more"></span></p>
<h2 id="knowledge-distillation">Knowledge Distillation</h2>
<p>中文翻作知識蒸餾，屬於模型壓縮的一種方法。最大的核心想法就是找出一個模型簡單，但一樣能處理複雜問題的模型。</p>
<p>核心的做法是：使用 Teacher Student model (師徒模型)，先把 Teacher
訓練好後，再從中選精華作為 student 訓練的目標，使得 student 也能達到
teacher 一樣的效果。</p>
<p>詳細：</p>
<p>先預訓練好一個 Teacher model，把 Teacher model 所生成的結果 <span
class="math inline">\(q\)</span>，當成 student 訓練的目標 <span
class="math inline">\(p\)</span>，使的 <span
class="math inline">\(p\)</span> 與 <span
class="math inline">\(q\)</span> 越接近越好</p>
<p>但是直接使用 teacher 的輸出 q 可能會不太合適，因為經過 softmax
的結果，通常對正確答安非常肯定 (機率非常高)，而對非答案的選項非常不肯定
(機率非常低)，這樣會造成 student
在訓練時很快就收斂了，什麼也沒有學到。</p>
<p>因此我們在 Label 上的機率動手腳，讓彼此之間相距近一點。把 softmax
的公式改今 softmax-T 如下：</p>
<p><span class="math display">\[
q_i = \frac{exp(z_i/T)}{\sum_jexp(z_j/T)}
\]</span></p>
<p>當 <span class="math inline">\(T=0\)</span> 時與 softmax
公式相同，通常會把 <span class="math inline">\(T\)</span> 設成 3
以上，當 <span class="math inline">\(T\)</span> 的值越大，Label
機率分佈就會比較均勻。</p>
<h2 id="noisy-student">Noisy Student</h2>
<p>Google 在 2020 提出了 Noisy Student 終結了 Knowledge Distillation
的系列討論，提出了一個全盤的分析與實驗。Noisy Student 完整運用了
Knowledge Distillation 的特性來訓練網路，最大的重點則是將 student
的輸入加上雜訊的部份 (與 Knowledge Distillation
的概念類似，只是方法不同)，以下為步驟：</p>
<h3 id="training">Training</h3>
<p>有兩個重點： 慢慢使用架構增大的網路 Iterative Training，讓 student
的潛力至少大於等於 teacher。 在訓練 student 加上 noisy，這裡的 noidy
代表 data augmentation、dropout、stochastic depth</p>
<ul>
<li>第一步：
<ul>
<li>train teacher model 使用 GT 標記來訓練</li>
</ul></li>
<li>第二步：
<ul>
<li>generate pseudo labels (假標籤)</li>
<li>直接相信 teacher model 所判斷的結果，當成 pseudo labels
，當然也有可能有誤</li>
<li>相信網路的能力，把 confidence 大於一定的百分比樣本留下，做為 student
的訓練構本</li>
</ul></li>
<li>第三步：
<ul>
<li>train student model</li>
<li>把 pseudo label 來訓練 student model 且同時加入雜訊</li>
</ul></li>
<li>第四步：
<ul>
<li>整理過程重複 N 次，直到網路收斂。</li>
</ul></li>
</ul>
<h3 id="iterative-training">Iterative Training</h3>
<p>論文使用 Efficientnet-B7 當 teacher model，Efficientnet-L0 當 student
model 再把 Efficientnet-L0 當 teacher，Efficientnet-L1 當 student
model，它比 L0 更寬 最後用 Efficientnet-L1 當 teacher，Efficientnet-L2
當 student model，為最後結果</p>
<p>整個訓練起來喔…我覺得真的超複雜，模型大到一個不可思議，難怪後來的人很難訓練成功</p>
<p>自我訓練完後使用 FixRes 的觀念，train 圖片縮小，test 圖片放大</p>
<h3 id="結論">結論</h3>
<p>Google 的這篇論文簡單的在 Knowledge Distillation
系列上總結了一下：基本上 knowledge distillation
能用，而效果最好的一步是在 student
加上雜訊，使用在訓練時有更好的表現。但是整體訓練方法過於複雜，難以復刻。</p>
<h2 id="reference">Reference</h2>
<p>https://medium.com/%E8%BB%9F%E9%AB%94%E4%B9%8B%E5%BF%83/deep-learning-noisy-student-knowledge-distillation%E5%BC%B7%E5%8C%96semi-supervise-learning-4e0c2d11520a</p>
<p>https://zhuanlan.zhihu.com/p/102038521</p>
<p>https://zhuanlan.zhihu.com/p/81467832</p>
<p>https://zhuanlan.zhihu.com/p/164597142</p>
<p>https://chtseng.wordpress.com/2020/05/12/%E7%9F%A5%E8%AD%98%E8%92%B8%E9%A4%BE-knowledgedistillation/</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Knowledge Distillation</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Visual Transformers: Transformer in Transformer - 使用雙層 Transformer 來重新思考 Patch Embedding</title>
    <url>/2021/07/27/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Visual-Transformers-Transformer-in-Transformer-%E4%BD%BF%E7%94%A8%E9%9B%99%E5%B1%A4-Transformer-%E4%BE%86%E9%87%8D%E6%96%B0%E6%80%9D%E8%80%83-Patch-Embedding/</url>
    <content><![CDATA[<p>很多人會覺得 (包括我 XD) ViT 的方法實在太神奇了，直接把圖片表示在
16x16 的字串？！然後竟然還可以
work？這篇論文覺得直接把二維轉換成一維流失了太多空間上的資訊了，包括圖片像素與像素之間的關系，提出了
TNT Transformer in Transformer 架構，希望可以以內外兩層 Transformer
來加強圖片轉序列的可解釋性及可行性。</p>
<p><a
href="https://arxiv.org/pdf/2103.00112">https://arxiv.org/pdf/2103.00112</a></p>
<p>keywords: TNT、Transformer in Transformer、word embedding</p>
<span id="more"></span>
<h2 id="introduction">1. Introduction</h2>
<p>在之前的研究中包含 ViT、DeiT 都沒有去探討一個問題：patch embedding
的可行性。倒底這種直接把圖片用 16x16 個區塊來表示，並且直接經過一個
linear transform 的做法可行嗎？這種方法真的是最合適的方法嗎？</p>
<p>作者認為 ViT、DeiT 等直覺 (intuitive)
的做法會有一個大問題：<strong>會乎略掉每個 patch
之間的訊息</strong>，也就是說在 16x16 之一的小區塊，經過了一個 linear
的轉換後會破壞像素與像素之間的關聯性。</p>
<p>因此作者提出了 TNT (Transformer iN Transformer)，<strong>試圖在 patch
內再新增一個 Transformer 來取得 patch 內的訊息</strong>，保留外部的
Transformer 的同時也新增一個內部的
Transformer，利用內外不同視野的獲取資訊，來使網路有更好的效果。</p>
<h2 id="approach">2. Approach</h2>
<p>整體架構如下：</p>
<p><img src="https://i.imgur.com/V102VCT.png" alt="Image" /></p>
<h3 id="patch-embedding-pixel-embedding">Patch Embedding &amp; Pixel
Embedding</h3>
<p>首先依照 ViT、DeiT 的方法把一張 <span class="math inline">\(H\times
W\times C\)</span> 的圖片分割成大小為 <span
class="math inline">\(p\)</span> 數量為 <span
class="math inline">\(n\)</span> 的 patch</p>
<p><span class="math display">\[
\mathcal{X} = [X^1,X^2,...,X^n] \in \mathbb{R}^{n\times p\times p\times
c}
\]</span></p>
<p>接著把得到的 patch 再做一次一模一樣的操作得到更小的 patch，把 <span
class="math inline">\(p\times p\times C\)</span> 的圖片分割成大小為
<span class="math inline">\(p&#39;\)</span> 數量為 <span
class="math inline">\(m\)</span> 的 patch</p>
<p><span class="math display">\[
\mathcal{Y_0} = [Y^1_0,Y^2_0,...,Y^n_0] \in \mathbb{R}^{n\times
p&#39;\times p&#39;\times c}
\]</span></p>
<p><img src="https://i.imgur.com/YHc678K.png" alt="Image" /></p>
<p>而比較大的 patch 稱為 <strong>Patch Embedding</strong> 比較小的 patch
稱為 <strong>Pixel Embedding</strong></p>
<p>接著各別不同大小的 Embedding 會經過不同的 Transformer</p>
<p>Patch Embedding 經過 Outer Transformer，負責 patch 之間的全局
(Global) 資訊 Pixel Embedding 經過 Inner Transformer，負責 pixel
之間的局部 (Local) 資訊</p>
<p><img src="https://i.imgur.com/CWKy2QU.png" alt="Image" /></p>
<h3 id="outer-transformer-inner-transformer">Outer Transformer &amp;
Inner Transformer</h3>
<p>Inner Transformer 的公式，先做 MAT 再做 MLP，與 ViT 相同：</p>
<p><span class="math display">\[
\begin{gathered}
  Y&#39;^i_l=Y^i_{l-1} + MSA(LN(Y^i_{l-1}))\\
  Y^i_l=Y&#39;^i_{l-1} + MLP(LN(Y&#39;^i_{l}))
\end{gathered}
\]</span></p>
<p>Outter Transformer 的公式，與上述差不多：</p>
<p><span class="math display">\[
\begin{gathered}
  X&#39;^i_l=X^i_{l-1} + MSA(LN(X^i_{l-1}))\\
  X^i_l=X&#39;^i_{l-1} + MLP(LN(X&#39;^i_{l}))
\end{gathered}
\]</span></p>
<p>那兩個不同視野的 Transformer 要怎麼合併資訊呢？作者這邊是使用在進入
Outter Transformer 前 會與 Inner Transformer 的結果 concat 起來。</p>
<p>首先 Inner Transformer 的結果會先 flattern，接著經過一層 linear
層把維度轉換成與 Outter Transformer 相同，再與 Outter Transformer
相加，做為下一時間點的輸入。公式如下：</p>
<p><span class="math display">\[
Z^i_{l-1}=Z^i_{l-1}+Vec(Y^i_{l-1})W_{l-1}+b_{l-1}
\]</span></p>
<p>既：原 Outter Transformer 加上 flattern 後 乘上 <span
class="math inline">\(W\)</span> 轉維度，再加上一個 b 權重值
(這裡不知怎麼多出來的…)</p>
<h3 id="positional-encoding">Positional Encoding</h3>
<p>與 ViT 不同，TNT 使用的是 1D 的 Positional Encoding，公式如下：</p>
<p><span class="math display">\[
\mathcal{Z} \leftarrow \mathcal{Z} + E_{patch}
\]</span></p>
<p><span class="math display">\[
E_{patch} \in \mathbb{R}^{(n+1)\times d}
\]</span></p>
<p>剛剛的 Patch Embedding &amp; Pixel Embedding
在運算前都分別加上去。</p>
<p>一樣 Patch Positional Encoding 負責全局空間的訊息 (global spatial
information) 而 Pixel Positional Encoding 負責局部相對的訊息 (local
relative information)</p>
<h3 id="運算量分析">運算量分析</h3>
<p>看起來 TNT 的運算量是 ViT 的兩部之多，因為整整多做一次
Transformer，但其實不然，如果仔細去分析複雜度
(論文有細詳推論過程這邊不多說)，會發現 Pixel Embedding
的部分因為圖片太小而 (Pixel 的大小遠小於
Patch)，因此複雜度並不會多很多，多一點點而已 (1.09倍)
並沒有想像中的大。</p>
<h3 id="網路架構">網路架構</h3>
<p>設計了大小 (B-S) 模型，一律：patch size 設為 16×16，小 patch size
設為 4×4</p>
<p><img src="https://i.imgur.com/OVAy35n.png" alt="Image" /></p>
<h2 id="experiment">3. Experiment</h2>
<p>嗯…不錯呢，超越了 ViT 及 DeiT！</p>
<p><img src="https://i.imgur.com/ahN4tGk.png" alt="Image" /></p>
<h3 id="一定要-positional-encoding-嗎">一定要 Positional Encoding
嗎？</h3>
<p>作者有試著把兩個 Encoding 都拿掉看看效果有沒有影響，結論是在做
attention 之前的 flattern 步驟，如果沒有位置的話，flattern
後的結果不管怎麼排都沒差。因此實驗也證明加上 Encoding 效果比較好。</p>
<p><img src="https://i.imgur.com/7wUmjOf.png" alt="Image" /></p>
<h3 id="head-數量">head 數量</h3>
<p>2 或 4 為最佳</p>
<p><img src="https://i.imgur.com/OTUQ9cf.png" alt="Image" /></p>
<h3 id="小-patch-size-的大小設定">小 patch size 的大小設定</h3>
<p>大 patch size 是 16x16，那小的呢？ 實驗證明 4x4 為最佳</p>
<p><img src="https://i.imgur.com/h83UkUz.png" alt="Image" /></p>
<h3 id="可視化">可視化</h3>
<p>Patch Embedding 可視化，兩個 Transformer
的結果好處有特徵抓取的能力更強了，比 DeiT 相比，特徵分佈的更寬廣</p>
<p><img src="https://i.imgur.com/KH3M8tZ.png" alt="Image" /></p>
<p>Pixel Embedding 可視化，隨著網路越深越抽象</p>
<p><img src="https://i.imgur.com/SYveZi7.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>如何把三維圖片表示成二維字串真的是一大難題，也是研究的熱門話題阿，而
TNT 提出了雙重 Transformer
的解法，雖然運算量大了一咪咪，但效果不錯，且有試著往解釋神奇的 16x16
前進了一小步，相信未來一定有更好的做法來解釋 16x16。</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/354913120</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Conditional Positional Encodings for Vision Transformers - 可變序列長短的 Positional Encoding</title>
    <url>/2021/07/28/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Conditional-Positional-Encodings-for-Vision-Transformers-%E5%8F%AF%E8%AE%8A%E5%BA%8F%E5%88%97%E9%95%B7%E7%9F%AD%E7%9A%84-Positional-Encoding/</url>
    <content><![CDATA[<p>論文提出 Conditional Positional Encoding (CPE) 模組，以及應用 CPE
模組的 Conditional Position encoding Vision Transformer (CPVT)
網路架構，負責來解決 Transformer 輸入圖片大小要固定的問題。</p>
<p>keywords: CPVT、CPE、PEG、zero padding <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>論文指出雖然最原始的 Transformer
是可以支持不同長度向量的輸入，但由於在把向量放進 Encoder 前會多做一步
Positional
Encoding，不管是公式解或是機器自行學習解，都會遇到這一層長度無法修改的問題，因為在大部份的實作中會設置一層可學習層
(pytorch 中的
nn.Parameter)，網路在學習時不斷修改其中的參數，因此不行處理不同長度的輸入。雖說在之前的研究有人使用雙三次插值
bicubic 來補充遺失的位置資訊。</p>
<p>下圖為各種 Encoding 方式的效果：可看到有 Positional Encoding
效果還是會比較好。</p>
<p><img src="https://i.imgur.com/c820JAY.png" alt="Image" /></p>
<p>再來作者提到 CNN 有平移不變性
(translation-invariance)，即圖中特徵點的位置不影響分類任務的效果，如果加上了絕對位置編碼
(absolute positional encoding) 會破壞 CNN 混然天成平移不變性
(translation-invariance) 的優點，而如果使用相對位置編碼 (relative
positional encodings) 則會有更多運算、需修改 Transformer
架構、與絕對位置編碼衝突等問題。</p>
<p>因此作者提出一個完全不同於絕對、相對位置編碼的想法，不是在 input
上「加上」位置資訊，而是在 Encoder
中用「算」出來的，這邊劇透一下，這邊用到了 CNN zero-padding
會加上位置資訊的特點來達成
(關於這方面的議題，下面會細講，而且之後會專開一系列來討論這個話題)</p>
<p>提出了 Conditional Positional Encoding (CPE) 模組，中間使用了
Positional Encoding Generator (PEG) 小模組。以及應用 CPE 設計出的
Transformer 架構 Conditional Position encoding Vision Transformer
(CPVT)。</p>
<p><img src="https://i.imgur.com/Z7GY1H0.png" alt="Image" /></p>
<h2 id="網路架構">2. 網路架構</h2>
<p>基於以下三點來設計架構：</p>
<ol type="1">
<li>效果好</li>
<li>避免排列不變性 (permutation
equivariance)，也就是：輸入序列順序變化時，结果也不同。且隨著輸入圖片
size 的改變要也可以有對應變化</li>
<li>能直接套用在現成 Transformer 架構上</li>
</ol>
<p>下圖為架構圖：</p>
<h3 id="positional-encoding-generator-peg">Positional Encoding Generator
(PEG)</h3>
<p><img src="https://i.imgur.com/O5dfLIf.png" alt="Image" /></p>
<ol type="1">
<li>把 input token，(class token 以及 patch token (這裡論文稱為 feature
token)) 的 patch token reshape 成原圖片的二維大小
(也就是一種回去原維度的感覺)，公式如下：</li>
</ol>
<p><span class="math display">\[
X\in\mathbb{R}^{B\times N\times C}
\]</span></p>
<p><span class="math display">\[
\rightarrow X\in\mathbb{R}^{B\times H\times W\times C}
\]</span></p>
<ol start="2" type="1">
<li>接著經過一個 transform 定義為 <span
class="math inline">\(\mathcal{F}\)</span>，而這個 <span
class="math inline">\(\mathcal{F}\)</span> 其實就是一個 conv
做卷積運算，其中 kernel size <span
class="math inline">\(k\ge3\)</span>，<strong><span
class="math inline">\(\frac{k-1}{2}\)</span> 的 zero
padding</strong>，而這裡的 zero padding
正是網路獲得位置資訊的重要來源。</li>
<li>再把三維圖片 reshape 至二維序列</li>
</ol>
<p><span class="math display">\[
X\in\mathbb{R}^{B\times H\times W\times C}
\]</span></p>
<p><span class="math display">\[
\rightarrow X\in\mathbb{R}^{B\times N\times C}
\]</span></p>
<ol start="4" type="1">
<li>而 class token 的部份則不參與 PEG 計算，直接加回二維序列中</li>
<li>最後一步把新算出來帶有位置資訊的二維序列，「加」concat
回原二維序列中，再當成下一個時間點的 Encoder 輸入</li>
</ol>
<h3
id="conditional-position-encoding-vision-transformer-cpvt">Conditional
Position encoding Vision Transformer (CPVT)</h3>
<p><img src="https://i.imgur.com/Z7GY1H0.png" alt="Image" /></p>
<p>而 CPVT 的做法也很直覺，不像 ViT DeiT 一樣在輸入 Encoder
前加上位置資訊，而是選擇在第一個 Encoder 做完後執行 PEG
模組，藉此加上位置資訊，再完成乘下的 Encoder。</p>
<p>而 CPVT-GAT 想要解決的是 class token 視為額外 token 的問題，因為
class token 是不能隨便與 patch token 順序亂混的。但作者認為 GAP (global
average pooling) 在垂直上是無序的 (inherently
translation-invariant)，因此現在就可以直接把 token 們視為一個整體放入
PEG 中做計算，最後再經一個 GAP 得到最後結果。作者發現 CPVT-GAT
是效果最好的方法。</p>
<h2 id="experiment">3. Experiment</h2>
<h3 id="與-sota-的比較">與 SOTA 的比較</h3>
<p>作者主要與相同架構的 DeiT 做比較，可以發現效果好個
<strong>1%</strong> 上下</p>
<p><img src="https://i.imgur.com/Tw8krlq.png" alt="Image" /></p>
<h3 id="與其它-positional-embedding-的比較">與其它 Positional Embedding
的比較</h3>
<p>LE 代表 learnable encoding，RPE 代表 relative positional
encoding，sin-cos 代表 absolute positional encoding。</p>
<p>結論：sin-cos 和 LE 差別不大，作者提出的 PEG 優於所有的方法</p>
<p><img src="https://i.imgur.com/58eHqTn.png" alt="Image" /></p>
<h3 id="peg-插入-encoder-位置的比較">PEG 插入 Encoder 位置的比較</h3>
<p>發現在第一個 Encoder 到第四個 Encoder 之間插入效果最好</p>
<p><img src="https://i.imgur.com/yqi3PIT.png" alt="Image" /></p>
<p>那每一個 Encoder
後面都加呢？作者發現不會越多越好，運算量增加但效果基本不變</p>
<p><img src="https://i.imgur.com/m333C3N.png" alt="Image" /></p>
<h3 id="神奇的-padding-比較">神奇的 Padding 比較</h3>
<p>在 CNN 加上了一個 zero padding，真的有必要嗎？</p>
<p>結論：zero padding
真的學到了位置的資訊，知道哪裡是角、哪裡是邊。也側面證實了絕對位置編碼的作用。</p>
<p><img src="https://i.imgur.com/3f96HYb.png" alt="Image" /></p>
<h3 id="cnn-vs-padding">CNN vs Padding</h3>
<p>作者在網路中加上了一層 CNN，那…倒底效果變好是因為 CNN
學習的關系，還是單純有了 zero padding 呢？</p>
<ul>
<li>如果是 PEG 位置表示能力起了作用，那我把 conv 換成 FC
層，效果應該會差一點</li>
<li>如果是 PEG 的 CNN 運算 (representative power) 起了作用，那讓 conv
參數固定不更新 (不學習)，效果應該會差一點</li>
</ul>
<p>而作者實驗的結論是：就算把 conv 參數固定不訓練，效果依舊好，證明了是
zero padding 起了作用，而非 CNN 起了作用</p>
<p><img src="https://i.imgur.com/km1k7zF.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>這篇論文討論了位置編碼這個議題，而使用的方式竟是 CNN 神奇的 zero
padding 特性來達成。關於 CNN 的特性我後面會開系列文章解釋
(因為已經有不少論文討論過相關話題了)。總之對我而言，透過這篇論文學到一種新的位置資訊獲得方法，以及了解到原來
CNN 有絕對位置的特性。</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/354913120</p>
<p>https://zhuanlan.zhihu.com/p/99766566</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN 與絕對位置資訊 - CNN 倒底學到了什麼？</title>
    <url>/2021/07/30/CNN-%E8%88%87%E4%BD%8D%E7%BD%AE%E8%B3%87%E8%A8%8A-CNN-%E5%80%92%E5%BA%95%E5%AD%B8%E5%88%B0%E4%BA%86%E4%BB%80%E9%BA%BC%EF%BC%9F-Position-Padding-and-Predictions-A-Deeper-Look-at-Position-Information-in-CNNs-CNN-%E8%88%87%E7%B5%95%E5%B0%8D%E4%BD%8D%E7%BD%AE%E8%B3%87%E8%A8%8A/</url>
    <content><![CDATA[<p>在上一篇 Transformer 中我們提到作者使用 zero padding
來當作位置資訊的考量，在這一篇文章中我引用了兩篇論文來更進一步了解一下，CNN
與絕對位置之間的關系。分別是 Uber 提出的 coordConv 以及一篇專門解釋 zero
padding 的文章。</p>
<p><a
href="https://arxiv.org/abs/1807.03247">https://arxiv.org/abs/1807.03247
(coordConv)</a></p>
<p><a
href="https://arxiv.org/pdf/2101.12322.pdf">https://arxiv.org/pdf/2101.12322.pdf
(zero padding)</a></p>
<p>keywords: zero padding、coordConv <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p>我們都知道 CNN 卷積神經網路有一大特性：平移不變性 (translation
invariant)，也就是說，圖中的特徵點不管位置在哪、大小多大，對於機器而言都是一樣的特徵。在分類任務上這個特徵似乎帶給我們很大的幫助，因為不管目標物出現在圖中任何位置，機器都會把視為相同特徵
(例如：不同地方的兩台車)</p>
<p><img src="https://i.imgur.com/onTxnsC.png" alt="Image" /></p>
<p>而在另一個分割領域情況就稍微比較複雜一點了，分割分為三種：語意分割
(Semantic segmentation)、實例分割 (Instance segmentation)、全景分割
(Panoramic Segmentation)。如下圖：</p>
<p><img src="https://i.imgur.com/UsG2YNH.png" alt="Image" /></p>
<p>簡單來說語意分割 (Semantic segmentation)
是依照「像素」級別來分割的，把每一個像素對應一個類別，就可以達成類似分割的效果了。語意分割比較簡單</p>
<p>但實例分割 (Instance segmentation)
就比較困難了，是把每個「物件」都分離出來，就算是同類別也是一樣，概念有點類似分類
+ 語意分割的結合。</p>
<p>全景分割 (Panoramic Segmentation)
更困難，是加上了背影的實例分割。</p>
<h2 id="coordconv">coordConv</h2>
<p>首先 Uber 在 2018 提出相關問題：CNN
在絕對位置上的能力沒有很好。而論文中真正實驗討論的問題是將直角座標系轉換成
one-hot 的能力。如以下影片所述</p>
<p><a
href="https://www.youtube.com/watch?v=8yFQc6elePA&amp;t=1s">https://www.youtube.com/watch?v=8yFQc6elePA&amp;t=1s</a></p>
<p>可以看到論文中最原始的想法就是要實作出一個可以在直角座標系轉換成
one-hot 的網路</p>
<p><img src="https://i.imgur.com/GUmUB3h.png" alt="Image" /></p>
<p>但是做出來的效果非常差，於是 coordConv
的核心想法就是：在特徵層多加兩層，分別為 x 軸座標以及 y 軸座標</p>
<p><img src="https://i.imgur.com/xRe9gdk.png" alt="Image" /></p>
<p>這兩層座標想法非常的直接，就是直接加入了 0 ~ 1
之間的數字，新增在最後兩層特徵圖中，而當這兩層特徵圖全為 0
時，就等同於原始的 CNN 網路。</p>
<p>藉由人工加入了「絕對座標」資訊，網路在「生點點
one-hot」的能力上有顯著的進步</p>
<p><img src="https://i.imgur.com/s0fai2C.png" alt="Image" /></p>
<h2 id="zero-padding">zero padding</h2>
<p>而另外一篇論文則是在討論，其實不用像 coordConv
那像人工加上絕對位置資訊，CNN
本身就好像自帶有這種能力了，只是以前大家都不是很清楚倒底是怎麼來的，反正「it
just works！」</p>
<p>有在做實例分割的人心中應該都有一個疑問：那就是 CNN
倒底是怎麼知道同一類不同位置的物件？還可以成功得把它分割出來呢？如同實例分割始使論文中提到的觀念：可參考以下論文</p>
<p><a href="https://arxiv.org/abs/1708.02551">Semantic Instance
Segmentation with a Discriminative Loss Function (CVPR2017)</a></p>
<p>論文中寫到透過一個 Loss function
使得「同一個實例的像素更加靠近、不同的實例像素盡可能地遠離」。嗯…這下子就神奇了，CNN
是怎麼知道它是不同的實例阿，不是有平移不變性嗎？那為什麼效果還不錯呢？會不會是…CNN
透過某種神秘的方法學習到了有關位置的資訊，使得機器知道同一類不同實例的物體？</p>
<p>因此本篇論文設計了一串實驗來證實：是的！以前大家都沒有想到，但是 CNN
是天生具有學習位置的能力的！而關鍵就發生在 zero padding
上面！以下介紹論文：</p>
<h2 id="experiment">Experiment</h2>
<p>使用的方法是輸入一個雜訊圖，目標要先出對應的座標圖
(像是下圖中的黃綠圖)，可看到 VGG、ResNet 輸出效果皆帶有位置的資訊</p>
<p><img src="https://i.imgur.com/Dm1mawX.png" alt="Image" /></p>
<p>作者認為 CNN 之所以會有絕對位置資訊 (absolute position) 的原因是因為
zero padding。zero padding 最初是用來使 CNN
的輸入輸出維度相同而設置的，但在不經意間 zero padding
會透露出邊邊、角等資訊，為了證實這一件事情，作者設計了有做 padding
以及沒做 padding 的實驗看看誰效果比較好：</p>
<p><img src="https://i.imgur.com/b7hqfCG.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>其實這篇論文設計了非常多實驗，我也沒有很認真的把每一個看完，但最重要的結論就是：zero
padding 這一步使得 CNN
有了微微的絕對位置資訊能力。而在往後的其它論文中提出其它更好的方法來解決絕對位置的問題中，也可發現加上絕對位置資訊後的效果大概是從
80 分到 100，而非 0 分到 100 分，這也是 zero padding
在幕後默默的推了一把的關系吧！</p>
<h2 id="reference">Reference</h2>
<p>https://medium.com/ching-i/%E5%BD%B1%E5%83%8F%E5%88%86%E5%89%B2-image-segmentation-%E8%AA%9E%E7%BE%A9%E5%88%86%E5%89%B2-semantic-segmentation-1-53a1dde9ed92</p>
<p>https://medium.com/ching-i/%E5%BD%B1%E5%83%8F%E5%88%86%E5%89%B2-image-segmentation-%E5%AF%A6%E4%BE%8B%E5%88%86%E5%89%B2-instance-segmentation-1-2a796c4fa738</p>
<p>https://www.codenong.com/cs105241864/</p>
<p>https://zhuanlan.zhihu.com/p/99766566</p>
<p>https://zhuanlan.zhihu.com/p/39919038</p>
<p>https://blog.piekniewski.info/2018/07/14/autopsy-dl-paper/</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: CvT: Introducing Convolutions to Vision Transformers - CNN 與 Transformer 各取所長</title>
    <url>/2021/08/06/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-CvT-Introducing-Convolutions-to-Vision-Transformers-CNN-%E8%88%87-Transformer-%E5%90%84%E5%8F%96%E6%89%80%E9%95%B7/</url>
    <content><![CDATA[<p>作者提出了新架構：Convolutional vision Transformer (CvT)，試著把 CNN
與 Transformer 做結合，並各取所長。</p>
<p>CvT 同時擁有了 CNN 的優點 (local receptive fields 局部感受視野,
shared weights 權重共享, spatial subsampling 空間下採樣)</p>
<p>以及 Transformer 的優點 (dynamic attention 動態的注意力機制, global
context fusion 更關注全局訊息的整合, better generalization
更好的歸化能力)</p>
<p><a
href="https://arxiv.org/pdf/2103.15808.pdf">https://arxiv.org/pdf/2103.15808.pdf</a></p>
<p>keywords: CvT <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>我們已經知道 CNN 在局部的空間內提取特徵的能力非常強，並且藉著不斷的
downsample 來把圖片越縮越小、特徵向量越來越長，使得 CNN
關注到的特徵越來越複雜。而相反的 Trasnformer
更在乎的是全局的關系，藉由整個圖片的 attention 來取得每個 pixel 與每個
pixel 之間的關系。</p>
<p>在之前介紹 self attention 的文章中也有提到，CNN 可以看成是 self
attention 的一種特例，因此也可以來解釋全局與局部的關系。</p>
<p><img src="https://i.imgur.com/HY2RA43.png" alt="Image" /></p>
<p>於是作者在這篇論文中提出 CvT，除了試著把 CNN 與 Transformer
各取所長，建立出一個效果更好的模型之外，也拜 CNN 所賜改進 ViT
訓練資料集過大的問題，同時在執行的效率上也有不錯的降低。</p>
<p>下圖為作者比較 ViT T2T TNT PVT DeiT 等等新模型的效能：</p>
<p><img src="https://i.imgur.com/6kdnMTl.png" alt="Image" /></p>
<h2 id="網路架構">2. 網路架構</h2>
<p><img src="https://i.imgur.com/vsBhvUE.png" alt="Image" /></p>
<p>作者提出了兩個新的模組：<strong>Convolutional Token
Embedding</strong> 以及 <strong>Convolutional
Projection</strong>，輸入的圖片會依序經過這兩個步驟，如 CNN
一樣會不斷的把特徵圖大小縮小，同時增加 channel 特徵圖的數量，在最後一個
Stage 才加上 cls token 做為分類的輸出，以上就是 CvT overall
的架構，接下來細講兩個新模組的做法：</p>
<h3 id="convolutional-token-embedding">Convolutional Token
Embedding</h3>
<p><img src="https://i.imgur.com/oOSrjYQ.png" alt="Image" /></p>
<p>首先我們先做一個正常的 Conv 卷積，kernel size 為 <span
class="math inline">\(s\)</span> ，使得維度上的變化如下列公式：</p>
<p><span class="math display">\[
\begin{gathered}
x_{i-1} \in \mathbb{R}^{H_{i-1}\times W_{i-1}\times C_{i-1}}\\
f(x_{i-1}) \in \mathbb{R}^{H\times W\times C}
\end{gathered}
\]</span></p>
<p>接著再經過一個 reshape，把卷積出來的三維圖片，轉換維度至二維序列</p>
<p><span class="math display">\[
\begin{gathered}
f(x_{i-1}) \in \mathbb{R}^{H\times W \times C}\\
H_iW_i\times C_i
\end{gathered}
\]</span></p>
<p>最後再經過一層 layer normalization</p>
<p>到目前為止就是 Convolutional Token Embedding 全部的架構了，而
Convolutional Token Embedding 架構則是在模擬 CNN 會把圖片大小 (<span
class="math inline">\(HW\)</span>) 不斷的減少同時增加特徵 (<span
class="math inline">\(C\)</span>)
的數量，只是最後我們會把三維的結果轉換成二維序列，因此上面的步驟也可以想成：序列的長度會越來越短，同時序列的特徵數會越來越多。</p>
<p>藉由這個模擬 CNN 的方法，可以使用二維的 (patch token)
會學習到更複雜的特徵。</p>
<p>而與原本 ViT 的 Patch Embedding 不同的是，Patch Embedding
是把圖片使用 16x16 來表示成 token，而 Convolutional Token Embedding
則是使用卷積運算來變成 token</p>
<h3 id="convolutional-projection">Convolutional Projection</h3>
<p><img src="https://i.imgur.com/crlPDW1.png" alt="Image" /></p>
<p>接著我們把 Convolutional Token Embedding 做完的二維序列放到
Convolutional Projection 中進行下一個步驟，而 Convolutional Projection
架構如上同所示</p>
<p>為什麼叫做 Convolutional Projection 呢？其實這個名詞是從 ViT 中的
Linear Projection 而來的，在 ViT 中我們為了做 self attention
於是把輸入序列 (patch token) 經過三個 Linear Projection (線性轉換)
得到三個不同的新序列，各有對應的新名稱 (query key value)。在原本 ViT
中的做法就只是單純的使用不同的線性組合來達成而已。而在 CvT 中作者改用
Conv 卷積的方法來實作。如下圖：</p>
<p><img src="https://i.imgur.com/AJyXO58.png" alt="Image" /></p>
<p>而具體 Convolutional Projection 的方法為：</p>
<p>先將 Convolutional Token Embedding 的結果 reshape 成回三維，接著做
Depthwise-separable Convolution，得到三種不同的 token map 分別對應
(Query Key Value)</p>
<p>具體流程公式如下：先經一個 Depth wise Conv 以及一個 Batch
Norm，最後再經 Point wise Conv</p>
<p><span class="math display">\[
\mathrm{Depth \ wise \ Conv2d\rightarrow BatchNorm2d \rightarrow Point \
wise \ Conv2d}
\]</span></p>
<p>而 Depthwise-separable Convolution 是由 Depth-wise Conv 和 Point-wise
Conv 所組成的，如下圖所示：</p>
<p>Depth-wise Conv：</p>
<p><img src="https://i.imgur.com/IWs8Hp1.png" alt="Image" /></p>
<p>Point-wise Conv：</p>
<p><img src="https://i.imgur.com/Oeu5jat.png" alt="Image" /></p>
<p>Depthwise-separable Convolution
是普通的卷積運算的子集合，線性組合的數量比較少，因此在執行上速度比較快，但是效果可能差一些些。</p>
<p>這邊特別注意在卷積運算時加上了 zero padding，這篇論文使用到了 CVPR
的概念，也就是使用 zero padding 來取代 positional encoding</p>
<p>其餘的部份皆與原版 ViT 的 Encoder 相同</p>
<h3 id="在效率上更進一步">在效率上更進一步</h3>
<p>作者提出 Convolutional Projection
後又更進一步減少網路運算量，作者把生成的 Key 和 Value 的卷積運算改成
stride 2，使得出來的 Key 和 Value 比原本的做法大小少 4
倍，整體的運算量也同樣少了 4
倍，但根據作者的實驗，網路的效能不會下降太多</p>
<p><img src="https://i.imgur.com/6gQDZGJ.png" alt="Image" /></p>
<h2 id="experiments">3. Experiments</h2>
<h3 id="網路架構-1">網路架構</h3>
<p>設計了三種不同大小的網路，數字代表使用了多少 Transformer Block</p>
<p><img src="https://i.imgur.com/Ga9TISk.png" alt="Image" /></p>
<h3 id="與-sota-的比較">與 SOTA 的比較</h3>
<p><img src="https://i.imgur.com/7qwl5sx.png" alt="Image" /></p>
<h3 id="與-transfer-learning-的比較">與 Transfer Learning 的比較</h3>
<p><img src="https://i.imgur.com/ojWf8MX.png" alt="Image" /></p>
<h3 id="實驗一位置編碼的影響">實驗一、位置編碼的影響</h3>
<p>CvT 中並沒有使用位置編碼，而是使用 zero
padding，作者設計了一系列的實驗來看看哪一種方法效果最好，以及 zero
padding 是否有給 CvT 位置的訊息。</p>
<p>發現 CvT 特別加上了位置訊息效果不會變更好，效果反而是差不多，證明了
zero padding 的功效</p>
<p><img src="https://i.imgur.com/6JU4Xxi.png" alt="Image" /></p>
<h3
id="實驗二convolutional-token-embedding-的影響">實驗二、Convolutional
Token Embedding 的影響</h3>
<p>作者比較了 ViT 16x16 的 Patch Embedding 以及 Convolutional Token
Embedding。發現不做位置資訊的 Convolutional Token Embedding
效果最好，其次是做位置資訊的 Patch Embedding</p>
<p><img src="https://i.imgur.com/PLHKeOf.png" alt="Image" /></p>
<h3
id="實驗三convolutional-projection-的-stride-1-stride-2">實驗三、Convolutional
Projection 的 Stride 1 Stride 2</h3>
<p>究竟把 Key Value 的大小縮小 4 倍對效能影響有多大呢？可看到運算量少
1.5 倍，但是效果只少一些些</p>
<p><img src="https://i.imgur.com/ulfsKV5.png" alt="Image" /></p>
<h3 id="實驗四convolutional-projection-的影響">實驗四、Convolutional
Projection 的影響</h3>
<p>實驗證明把全部的 Linear Projection 換成 Convolutional Projection
效果最好，證明了 Convolutional Projection 是個有用的測略</p>
<p><img src="https://i.imgur.com/D9U4SAH.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>CvT 嘗試把 CNN 與 Transformer
結合，各取一點好處來使效果更好外，也有試著往運算量更少的方向進前。</p>
<p>比較特別的兩個點是，使用 zero padding 來當作位置資訊，以及把 cls
token 放到最後一個階段才加上去
(原文沒有特別著墨在這裡，不知道這麼做的用意是…？)</p>
<p>總之新增的兩個模組都把 Transformer 往 CNN 的地方又更像了一點</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/361112935</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Incorporating Convolution Designs into Visual Transformers - Convolution-enhanced image Transformer (CeiT) 又一篇 CNN 加 Transformer</title>
    <url>/2021/08/06/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Incorporating-Convolution-Designs-into-Visual-Transformers-Convolution-enhanced-image-Transformer-CeiT-%E5%8F%88%E4%B8%80%E7%AF%87-CNN-%E5%8A%A0-Transformer/</url>
    <content><![CDATA[<p>Convolution-enhanced image Transformer (CeiT)，與 CvT
的想法相同，都是想要藉助 CNN 的力量來改善 Transformer
的效能，而這兩篇論文提出的時間差不多，基本上思路也差不多，以下會簡單帶過</p>
<p><a
href="https://arxiv.org/pdf/2103.11816.pdf">https://arxiv.org/pdf/2103.11816.pdf</a></p>
<p>keywords: CeiT <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>這篇作者認為 CNN 最重要的兩大特色就是：Invariance 平移不變性，以及
Locality 局部性，與 CvT 強調的 (local receptive fields 局部感受視野,
shared weights 權重共享, spatial subsampling 空間下採樣)
概念相同。因此提出把 CNN 結合 Transformer 來解決以上問題。</p>
<p>網路架構有以下的改進：</p>
<ol type="1">
<li>提出 Image-to-tokens 的方法，來改善原本 ViT 16x16 patch 的做法</li>
<li>為了強化特徵的提取，CeiT 把 MLP 層的全連接層
(Feed-Forwardnetwork)，換成了 Locally-enhanced Feed-Forward layer，加強
token 之間的關聯性</li>
<li>在 Transformer 最後一步加上了 Layer-wise Class token Attention
進一步提升性能</li>
</ol>
<h2 id="網路架構">2. 網路架構</h2>
<h3 id="image-to-tokens">Image-to-tokens</h3>
<p><img src="https://i.imgur.com/hDpjVX4.png" alt="Image" /></p>
<p>為了解決 ViT 把圖片依照 patch 16x16 過於粗糙的做法，因此改成先經 CNN
再來分 patch，簡單來說原圖會先做一次卷積，加上一個 BN 再加上一個
Maxpolling，最後再用 ViT 的分 patch 的方法來分塊。公式如下：</p>
<p><span class="math display">\[
x&#39; = \mathrm{I2T(x)} = \mathrm{MaxPool(BN(Conv(x)))}
\]</span></p>
<p>I2T 利用了 CNN 在取得低階特徵的優勢，來縮小 patch
的大小減少訓練難度</p>
<p>與 CvT 不同的地方：CvT 是在每一個 Stage 都會做一次 CNN 卷積，而 CeiT
只會在網路的最一開始做一次而已</p>
<h3 id="locally-enhanced-feed-forward-layer">Locally-enhanced
Feed-Forward layer</h3>
<p><img src="https://i.imgur.com/MgIMIcc.png" alt="Image" /></p>
<p>作者把原本的 MLP 層中的 FF 換成 LeFF (Locally-enhanced Feed-Forward
layer)，加強 token 之間全局特徵提取的能力</p>
<ol type="1">
<li>首先會先把 input token 分成 patch token 以及 class token</li>
<li>class token 不動，而 patch token 則會經過以下步驟</li>
<li>經 Linear Projection 放大維度</li>
<li>再 reshape 成三維圖片</li>
<li>再做一次 Depth-wise 卷積運算</li>
<li>reshape 成二維序列</li>
<li>再做一次 Linear Projection</li>
<li>把 class token 加回來</li>
</ol>
<p>比較特別的是在每一個 Linear Projection 以及 Convolution
之後都會加上一個 BN 以及 GELU</p>
<p>公式如下：</p>
<p><span class="math display">\[
\begin{gathered}
  \mathrm{x_{cls}, x_{patch} = Split(x)}\\
  \mathrm{x_{patch} = GELU(BN(Linear1(x_{patch})))}\\
  \mathrm{x_{patch} = SpatialRestore(x_{patch})}\\
  \mathrm{x_{patch} = GELU(BN(DWConv(x_{patch})))}\\
  \mathrm{x_{patch} = Flatten(x_{patch})}\\
  \mathrm{x_{patch} = GELU(BN(Linear2(x_{patch})))}\\
  \mathrm{x = Cancat(x_{cls},x_{patch})}\\
\end{gathered}
\]</span></p>
<p>與 CvT 不同的地方：其實這一步與 CvT 基本上差不多，只是 CvT 是作用在
MSA 層上，而 CeiT 是作用在 MLP 層上。以及最重要的，CeiT 在網路中使用了
GELU</p>
<h4 id="gelu">GELU</h4>
<p>關於 GELU 這裡不多做介紹。可以參考以下文章：我自己的大意是，GELU 與
ReLU 很像，都是把值乘上 0 或 1，只是 GELU 會根據當下值的機率來決定要乘 0
還是 1。</p>
<p>而 CeiT 之所以會使用 GELU 是因為之前在 NLP 流行的 GPT-2、BERT
都使用上了 GELU，並且在 語音辨識上取得不錯的成積。嗯…可以理解成 NLP
專用的 Activate funtion 吧哈哈</p>
<p><a
href="https://www.jiqizhixin.com/articles/2019-12-30-4">https://www.jiqizhixin.com/articles/2019-12-30-4</a></p>
<h3 id="layer-wise-class-token-attention">Layer-wise Class token
Attention</h3>
<p><img src="https://i.imgur.com/8On4Qq8.png" alt="Image" /></p>
<p>Layer-wise Class token Attention 是加在整個 Transformer
的最後面的，由圖可以看出，作者提出的這個新的 LCA 層是加在 Encoder
之外。</p>
<p>作者認為隨著網路不斷的加深，希望能在 layer 與 layer
層與層之間加深彼此的關系，因此把每一層的 class token 都拿出來，經過一次
self-attention 得到 Layer-wise 的 attention，也就是每個層的 class token
之間的關系，而最後的 output 也是整個 CeiT 的 output</p>
<h2 id="experiments">3. Experiments</h2>
<h3 id="網路架構-1">網路架構</h3>
<p><img src="https://i.imgur.com/7RxBVHO.png" alt="Image" /></p>
<h3 id="實驗一sota-比較">實驗一、SOTA 比較</h3>
<p>效果沒有比 EfficientNet 來得好，但是運算量及參數使用量比較少</p>
<p><img src="https://i.imgur.com/XPzxExY.png" alt="Image" /></p>
<h3 id="實驗二transfer-learning-比較">實驗二、Transfer Learning
比較</h3>
<p>雖然還是沒有超過 EfficientNet ，但在 ImageNet 上的 Transfer Learning
超過了 ViT，證明 CNN + Transformer 是有潛力的</p>
<p><img src="https://i.imgur.com/3MOfyr3.png" alt="Image" /></p>
<h3 id="實驗三i2t-的比較">實驗三、I2T 的比較</h3>
<p>使用了不同卷積的 kernel size、stride，以及 maxpooling
BN，看看哪一個排列組合效果最好：</p>
<p><img src="https://i.imgur.com/Ps4TVZH.png" alt="Image" /></p>
<h3 id="實驗四leff-的比較">實驗四、LeFF 的比較</h3>
<p>同樣比較了 kernel size 以及是否使用 BN 來找出最好的排列組合</p>
<p><img src="https://i.imgur.com/8ZERYDf.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>CeiT 基本與 CvT 想法一模一樣，都是把 CNN 加上了 Transformer
來改善運算量、資料夾大小、效能等等問題。</p>
<p>而 CeiT 我自己認為比較特別的點在於，使用到了 GELU 這個 NLP 才在用的
Activate funtion，以及在最後加上了 LCA，把每一個不同 stage 的 class
token 拿出做一個 self attention，找出一個橫跨 Layer 之間的關系。</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/361112935</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet - T2T-ViT</title>
    <url>/2021/08/13/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Tokens-to-Token-ViT-Training-Vision-Transformers-from-Scratch-on-ImageNet-T2T-ViT/</url>
    <content><![CDATA[<p>這篇論文發表在 CvT、CeiT 之前，但想要解決的問題是一樣的 (解決分
patch、運算量大等…)。CvT、CeiT 是使用 CNN 來解決問題，而 T2T-ViT
則是使用 Token-to-Tokens 來解決問題。</p>
<p><a
href="https://arxiv.org/pdf/2101.11986.pdf">https://arxiv.org/pdf/2101.11986.pdf</a></p>
<p>keywords: T2T-ViT <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>T2T-ViT 相要改進 ViT 在 ImageNet 上訓練時不如傳統 CNN
的兩個缺點：</p>
<h3 id="vit-分-patch-的方法會使得圖片之間的訊息流失">1. ViT 分 patch
的方法會使得圖片之間的訊息流失</h3>
<p>在 ViT 中分 Patch 的公式算單來說是長以下這樣的：</p>
<p><span class="math display">\[
\begin{gathered}
H\times W\times C \rightarrow N\times (P^2\cdot C) \rightarrow (N, D)\\
\mathrm{where}, N=HW/P^2
\end{gathered}
\]</span></p>
<p>在原 source code 中是長以下這樣：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br></pre></td></tr></table></figure>
<p>可發現它其實就是一個 kernel, stride 皆為 16
的一個卷積運算而已，這樣子的做法無法很有效的去表達 16x16
中的局部特徵訊息</p>
<h3 id="vit-的-backbone-self-attention在特徵的提取上有點冗餘">2. ViT 的
backbone (self-attention)，在特徵的提取上有點冗餘</h3>
<p>這裡作者直接用實驗結果來證明，作者把 ResNet 與 ViT
的其中一層特徵向量取出來做視覺化，如以下：</p>
<p><img src="https://i.imgur.com/UH57D4h.png" alt="Image" /></p>
<p>可發現 ResNet 隨著網路越深 (圖片越往右)，特徵的紋理越來越多樣，但是
ViT 隨著網路越深，基本上特徵圖沒有什麼多樣的地方
(大部分都還是狗)，而且還會有全白全黑的問題 (用紅框的部份)。</p>
<p>實驗可證明 ViT 在特徵截取上的確不如 CNN 來的好</p>
<h2 id="網路架構">2. 網路架構</h2>
<h3 id="整體架構">整體架構</h3>
<p><img src="https://i.imgur.com/gdo0RUu.png" alt="Image" /></p>
<p>首先原圖會做一次 Unfold 成二維向量，與 ViT 的 patch embedding
不太一樣的是，T2T-ViT 中所有的 Unfold 都有 overlap，增加相關性。</p>
<p>接著經過一層 Transformer，再接回 T2T module，這個步驟重覆兩次。</p>
<p>接著會加上 cls token 以及 PE ，最後放到 Backbone 去</p>
<p><span class="math display">\[
\begin{gathered}
T_i = \mathrm{MLP(MSA(T_i))}\\
T_{i+1} = \mathrm{T2T\_module(T_i)}
\end{gathered}
\]</span></p>
<h3 id="tokens-to-token-module">Tokens-to-Token module</h3>
<p>為了解決以上兩個問題作者設計了一個 Tokens-to-Token
module，以下介紹：</p>
<p><img src="https://i.imgur.com/lDqEniJ.png" alt="Image" /></p>
<h4 id="第一步restructurization">第一步：Restructurization</h4>
<p>把二維向量 reshape 成三維，如下公式所示：</p>
<p><span class="math display">\[
\mathbb{R}^{l\times c} \rightarrow \mathbb{R}^{h\times w\times c}
\]</span></p>
<h4 id="第二步soft-split-ss">第二步：Soft Split (SS)</h4>
<p>剛剛把二維 reshape 成三維，現在我們又要把三維 reshape
回二維，只是做法不太一樣。這一步是為了進一步提取 local information
的。</p>
<p>為了達成可以提取 local information，作者使用了 pytorch 中的 Unfold
函式來達成。特別的是 Unfold 中每個 kernel 是有重疊的，增加 local
information。其實原理與一個 Conv 差不多，如以下公式：</p>
<p><span class="math display">\[
\begin{gathered}
(B, C, H, W) \rightarrow (B, Ck^2, HW)\\
k, \mathrm{kernel\_size}
\end{gathered}
\]</span></p>
<h3 id="t2t-vit-backbone">T2T-ViT Backbone</h3>
<p>為解決 ViT Backbone 很多特徵是多餘沒用的，T2T-ViT 參考 CNN
的做法，一共試了 5 種做法：</p>
<ul>
<li>參考 DenseNet：使用 Dense 連接</li>
<li>參考 Wide-ResNets：Deep-narrow vs. shallow-wide 結構比較</li>
<li>參考 SE module：使用 Channel attention 結構</li>
<li>參考 ResNeXt：在 attention 中使用更多的 heads</li>
<li>參考 GhostNet：使用 Ghost module</li>
</ul>
<p>經過了大量的實驗後，作者得出使用 CNN 的 Deep-narrow
深窄結構效果最好，可以增加特徵的多樣性</p>
<p>所以作者設談的 T2T backbone 它的 Embedding dimension (二維序列長度)
比較小，同時層數比較多</p>
<h2 id="experiments">3. Experiments</h2>
<h3 id="與-vit-比較">與 ViT 比較</h3>
<p>不論在參數量、運算量、效能上，皆比 ViT 好</p>
<p><img src="https://i.imgur.com/KJGikvO.png" alt="Image" /></p>
<h3 id="與-resnet-比較">與 ResNet 比較</h3>
<p>與 CNN 的對比則當然比較好啦，效能好一些，不過計算量高一些些</p>
<p><img src="https://i.imgur.com/bt9DepO.png" alt="Image" /></p>
<h3 id="與-mobilenet-比較">與 MobileNet 比較</h3>
<p>與小小模型比較，在相同參數量的前提下，效能提高，但運算量高一些些</p>
<p><img src="https://i.imgur.com/8yQ4IwG.png" alt="Image" /></p>
<h3 id="各種不同-backbone-的比較">各種不同 backbone 的比較</h3>
<p>可以直接看結論：使用 DN (Deep-Narrow) 深窄結構效果最好</p>
<p><img src="https://i.imgur.com/end7cto.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>T2T-ViT 是在 2021 1月發表的文章，比 CvT、CeiT 還早，但已經有想要使用
CNN 來結決問題的大方向。整體網路架構印象最深的地方是 T2T 的 Unfold
運算，不知道這樣子的做法是不是真的會比較好…</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/386955720</p>
<p>https://zhuanlan.zhihu.com/p/348055832</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions - 把金字塔網路應用在 Transformer</title>
    <url>/2021/08/17/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Predictionwithout-Convolutions-%E6%8A%8A%E9%87%91%E5%AD%97%E5%A1%94%E7%B6%B2%E8%B7%AF%E6%87%89%E7%94%A8%E5%9C%A8-Transformer/</url>
    <content><![CDATA[<p>這篇論文是南京大學、香港大學在 2021 2 月提出的，這篇論文提出了
Pyramid Vision Transformer (PVT) 架構，其實就是把 CNN
已經非常廣泛使用的概念搬到 ViT 上面來。主要創新點包含兩點：Progressive
shrinking stategy 加入金字塔網路、Spatial Reduction Attention
減少運算量。</p>
<p><a
href="https://arxiv.org/pdf/2102.12122.pdf">https://arxiv.org/pdf/2102.12122.pdf</a></p>
<p>keywords: PVT、Progressive shrinking stategy、Spatial Reduction
Attention <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p><img src="https://i.imgur.com/vrcjsZQ.png" alt="Image" /></p>
<p>有鑒於 CNN 在電腦視覺的成功，PVT 提出的動機希望能把已經在 CNN
非常成功的概念 Feature Pyramid Network (FPN) 應用在 Transformer
上面，藉此更善 ViT 的一些缺點：</p>
<ol type="1">
<li><strong>加上多重解析度</strong>：不同於 ViT
低解析度輸出、高運算複雜度，PVT 可以得到更高解析率的輸出</li>
<li><strong>減少運算</strong>：如同 FPN
一樣會慢慢減少特徵圖數量，減少運量，改善 ViT
遇到解析度大圖片時運算量會爆增的問題</li>
<li><strong>增加應用範圍</strong>：傳統 ViT 只能用在分類任務上，PVT
不但也能分類，也因為有多重解析度，因此也能運用在辨識、分割任務上</li>
</ol>
<p>下圖為不同網路架構能做的電腦視覺任務比較圖：</p>
<p><img src="https://i.imgur.com/O69770a.png" alt="Image" /></p>
<h2 id="網路架構">2. 網路架構</h2>
<h3 id="整體架構">整體架構</h3>
<p>整體架構圖如下：</p>
<p><img src="https://i.imgur.com/iJ2sJ1d.png" alt="Image" /></p>
<p>作者為了模仿 FPN 的多重解析度，因此本論文的 PVT
架構設計了四個階段用於生成不同解析度的特徵，每個階段的操作都相同，包含兩個步驟：<strong>Patch
Embedding、Transformer
Encoder</strong>，步驟相同但是圖片的解析度會隨著網路而慢慢加深</p>
<p>整體架構文字流程如下：</p>
<ul>
<li>首先會輸入一張 <span class="math inline">\(H\times W\times
3\)</span> 的影像</li>
<li>與 ViT 的 Patch 大小為 16x16 不同，PVT 的 Patch 大小設為 4x4</li>
<li>接著把三維的圖片 <span class="math inline">\(H\times W\times
3\)</span> reshape 至二維 <span class="math inline">\(HW/4^2 \times
C_1\)</span></li>
<li>把二維序列放進 Transformer 中</li>
<li>Transformer 輸出的結果 <span class="math inline">\(HW/4^2 \times
C_1\)</span> reshape 回 <span class="math inline">\(H/4 \times W/4
\times C_1\)</span></li>
</ul>
<p><span class="math display">\[
\begin{gathered}
H\times W\times 3\\
\rightarrow HW/4^2 \times C_1\\
\rightarrow H/4 \times W/4 \times C_1
\end{gathered}
\]</span></p>
<p>作者在論文提出 Feature Pyramid for Transformer 以及 Transformer
Encoder 來詳細介紹架構</p>
<h3 id="feature-pyramid-for-transformer">Feature Pyramid for
Transformer</h3>
<p>與 ViT 提出的 Patch Embedding 不同，ViT 中的 Patch Embedding
只有在網路的一開始出現，而 PVT 中的 Patch Embedding 會在每一個 Stage
中出現 (在這篇論文舉的例子一共出現 4 次)。</p>
<p>而在 PVT 中這些 Patch Embedding 擔任了 progressive shrinking
重要的責任，負責把 Transformer
中的特徵圖慢慢減少圖片大小、增加特徵圖</p>
<p>透過這樣在每個 Encoder 前做一次 Patch Embedding
的方法，就可以人為的控制我們想要的各種不同解析度了</p>
<p>主要網路公式如下：</p>
<p>輸入前一網路特徵圖 <span class="math inline">\(F_{i-1}\)</span> ，經
reshape 至二維，經 Transformer 得到二維結果，接著 reshape
回三維。特別注意是在這篇論文中 Patch <span
class="math inline">\(P\)</span> 設為 4 or 2 (可參考
Experiment)。最後經過一個 LayerNorm 即為最後結果</p>
<p><span class="math display">\[
\begin{gathered}
F_{i-1} \in \mathbb{R}^{H_{i-1}\times W_{i-1} \times C_{i-1}}\\
\rightarrow \frac{H_{i-1} W_{i-1}}{P^2_i}\times C_i\\
\rightarrow \frac{H_{i-1}}{P_i}\times \frac{W_{i-1}}{P_i} \times C_{i-1}
\end{gathered}
\]</span></p>
<p>與 ViT 的 patch embedding 中的 source code 相同，在 pytorch
中的實作方法就是使用 kernel size 、 stride 皆為 Patch <span
class="math inline">\(P\)</span> (4) 的 Conv2d</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br></pre></td></tr></table></figure>
<h3 id="transformer-encoder">Transformer Encoder</h3>
<p>對於每一層的 Encoder PVT
也有做一些調整。由於圖片的解析度會越來越大，需要的運算也會放大，為了解決運算量的問題，作者提出了
SRA(spatial-reduction attention) 代替原本的
MHA(multi-head-attention)</p>
<p><img src="https://i.imgur.com/gDRAYaY.png" alt="Image" /></p>
<p>解決方法也很簡單，把網路中的 K 、 V 的維度縮小，再放進 MHA
中做計算。</p>
<p>這一步與 TNT 中的做法相同，一樣是通過減少 K V
的長度來減少運算量，且效能不會減少太多 (可看 TNT 實驗)。至於為什麼減少 K
V 對效果不會影響太多嗎…目前我還不清楚 XD</p>
<p>公式如下：</p>
<p><span class="math display">\[
\mathrm{head}_j =
\mathrm{Attention}(QW^Q_j,\mathrm{SR}(K)W^K_j,\mathrm{SR}(V)W^V_j)
\]</span></p>
<h2 id="experiments">3. Experiments</h2>
<h3 id="網路架構-1">網路架構</h3>
<p>參考 ResNet 設計了四個 Stage，特徵圖放大了 32
倍，並且也有四個不同大小的網路 (好玩的是 Stage 重複次數跟本與 ResNet
一模一樣 XD，同樣用到了越深重覆次數越多的概念)</p>
<p><img src="https://i.imgur.com/d4R63cK.png" alt="Image" /></p>
<p>由於有多重解析度，Transformer
系列終於不只能做分類了，於是作者與分類、偵測、語義分割、實例分割都來比較了一下</p>
<h3 id="與分類比較">與分類比較</h3>
<p>使用 ImageNet 來做比較，實驗發現效果比 CNN 好，比 ViT 好，沒有 TNT
T2T-ViT 好，但是參數量與運量少非常非常多，證明 CNN
的多重解析度可以非常有效率的截取特徵</p>
<p><img src="https://i.imgur.com/JDMZ5tR.png" alt="Image" /></p>
<h3 id="與偵測比較">與偵測比較</h3>
<p><img src="https://i.imgur.com/QQbp295.png" alt="Image" /></p>
<h3 id="與實例分割比較">與實例分割比較</h3>
<p><img src="https://i.imgur.com/50I4xEw.png" alt="Image" /></p>
<h3 id="與語音分割比較">與語音分割比較</h3>
<p><img src="https://i.imgur.com/WbXqa14.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>由以上實驗可證實，在不同參數設置下，PVT 的效果皆比 ResNet、ResNeXt
還要來得好，尤其在分割上面 Transformer
更關注全局，這個特性對分割來說是個非常有效的，因此效果比想像中好，未來也可以試著往這方向結合。</p>
<p>總而言之，PVT 試著把 FPN 與 Transformer 結合，並且把 Transformer
能完成的任務大大的拓展了，不再只能用來分類。且套用了 CNN
的概念參數量有大大下降的趨勢。</p>
<h2 id="reference">Reference</h2>
<p>https://mp.weixin.qq.com/s/LCLQltmBxL9f1XzV4Ci-iw</p>
<p>https://www.jianshu.com/p/d2a878723af4</p>
<p>https://blog.csdn.net/P_LarT/article/details/114157235</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Bottleneck Transformers for Visual Recognition - BoT 把 Bottleneck 加上 Transformer</title>
    <url>/2021/09/02/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Bottleneck-Transformers-for-Visual-Recognition-BoT-%E6%8A%8A-Bottleneck-%E5%8A%A0%E4%B8%8A-Transformer/</url>
    <content><![CDATA[<p>2021 年 1 月 Google 提出了 BoTNet 架構，其最核心的思想就是替換 ResNet
中的 Bottleneck，把最後幾層的卷積層 (Conv) 替換為 Multi-Head
Self-Attention (MHA)。實驗證實在僅僅只修改幾層網路下，BoTNet
在實例分割任務上取得了 44.4% 的 Mask AP 與 49.7%的 Box AP，與純 ResNet
相比，<strong>在分類、分割任務上皆有效能上的提升，同時還可以降低參數量</strong>。</p>
<p><a
href="https://arxiv.org/pdf/2101.11605.pdf">https://arxiv.org/pdf/2101.11605.pdf</a></p>
<p>keywords: BoT、Bottleneck <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>本篇論文主要是在討論實例分割的改進，因而作者以「應用 Transformer
在實例分割上」，以及像 ViT 一樣使用純 Transformer
為出發點下，提出了兩大的問題：</p>
<ol type="1">
<li>通常分割的圖片 (1024x1024) 相較於分類的圖片 (224x224)
大小還有來得大。</li>
<li>在圖片大的情況下，attention 的計算量會呈現指數的上升 (<a
href="https://mushding.space/2021/07/09/NLP-%E8%88%87-CV-%E7%9A%84%E7%B5%90%E5%90%88%EF%BC%9ADeformable-DETR-Deformable-Transformer-For-End-To-End-Object-Detection-%E6%AD%A3%E9%9D%A2%E5%B0%8D%E6%B1%BA-DETR-%E7%9A%84%E7%BC%BA%E9%BB%9E%EF%BC%81/">可參考以前的文章</a>)</li>
</ol>
<p>為了解決以上的問題作者提出了改進方法：</p>
<ol type="1">
<li>先使用普通的卷積運算的強項，從大解析度的圖中截取低階特徵</li>
<li>在最後一層卷積，圖片被 downsampling 成小解析度後，再做 Transformer
運算</li>
</ol>
<p>因此作者直接引用了非常成熟的 ResNet-50
來滿足低階特徵截取的部分，並將其最後幾層改為 Transformer。由於 ResNet-50
使用了 Bottleneck，於是作者將這個 ResNet 與 Transformer 結合的網路稱為
Bottleneck Transformer，簡寫為 BoT。</p>
<p>BoT 的特色為網路架構非常單純的僅僅把 bottleneck 中的卷積層替換成
Self-Attention 層。如下圖所示：</p>
<p><img src="https://i.imgur.com/KUAnnj9.png" alt="Image" /></p>
<p>而我個人認為，BoT 雖然在創新上占比不多 (而且也只是加個 Self-Attention
就說自己是 Transformer…)，但是嘗試著把 Transformer 與 CNN
結合，以及最後的實驗證明結果，都可以當成研究 Transformer 與 CNN
互利共生的好的出發點。</p>
<h2 id="網路架構">2. 網路架構</h2>
<p>前面也提到了，作者直接把 ResNet-50 拿來用，所以 BoT
在網路架構上並不複雜。下圖左邊為傳統的 Transformer
Block，而中間則是作者提出的 BoT Block。作者將其視為與 Transformer Block
同階級的模組</p>
<p><img src="https://i.imgur.com/erOrnzF.png" alt="Image" /></p>
<p>而其中的 MHSA 為 Multi-Head Self-Attention 的簡寫 (也可寫為
MSA)，其詳細架構如下圖：</p>
<p>可看到 BoT 是有做 positional encoding 的，作者提到這邊作的是 Relative
Position Encodings，而且是與 Query 做矩陣乘法而非加法</p>
<p><img src="https://i.imgur.com/zfvcaNS.png" alt="Image" /></p>
<p>而這樣子的 MHA 架構，只會套用在 ResNet-50 的最後一層 (c5
層)，其餘架構<strong>全部</strong>與 ResNet-50
一致。這是為了達成減少運算量這個需求。</p>
<p><img src="https://i.imgur.com/Nu1aVpt.png" alt="Image" /></p>
<h2 id="experiment">3. Experiment</h2>
<p>作者在實驗上沒有特別與其它網路或者是 sota 互相比較，而是單純比較 R50
與 BoT50 之間的差別。</p>
<h3 id="在實例分割上的比較">在實例分割上的比較</h3>
<p>資料集選用 COCO，可看到在不同 epochs 下 BoT50 皆比 R50 優秀</p>
<p><img src="https://i.imgur.com/Wx1Bw0o.png" alt="Image" /></p>
<h3 id="位置編碼的比較">位置編碼的比較</h3>
<p>單 attention 增加 0.6，而加上「相對位置」後的效果明顯好了一些</p>
<p><img src="https://i.imgur.com/n7JbimZ.png" alt="Image" /></p>
<h2 id="botnet-s1">4. BoTNet-S1</h2>
<p>在論文的最後，作者把 BoTNet 改成分類任務，並改稱作為 BoTNet-S1。</p>
<p>作者發現如果單純的把 BoTNet 直接放進 ImageNet 分類的話，效果與
ResNet50 不相上下，但是如果參考 ViT，只把圖片 downsample 到 1/16 的大小
(換算成 ResNet 為第四個階段 (c4))，而非 ResNet 的1/32
的話效果會變好。</p>
<p>於是作者把 BoT 的最後一層 (c5) 的 stride 2 給取消，稱為 S1 (少一個
stride 的意思…？)，網路稱為 BoTNet-S1</p>
<p>實驗比較結果：</p>
<p><img src="https://i.imgur.com/Wj9yHNC.png" alt="Image" /></p>
<p>BoTNet-S1 與其它 sota 比較</p>
<p><img src="https://i.imgur.com/lcC7ebU.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>本篇論文架構簡單，改進的地方並不多，許多實驗也並未與 sota 比較。</p>
<p>但是如果換個角度來看：只把 ResNet 最後一層改成 attention
效果就可以好上 1%
這點來說，還是挺有意思的，以最低的更改成本就可以達到效果好且參數少。</p>
<p>且這篇論文也在 CNN 與 Transformer
的結合，不管是效能還是運算量，都給出了一些見解，使這兩個網路往各取所長之路前進。</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/347602463</p>
<p>https://bbs.cvmart.net/articles/4142</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>基隆嶼三合一 &amp; 龍洞一日遊</title>
    <url>/2021/08/29/%E5%9F%BA%E9%9A%86%E5%B6%BC%E4%B8%89%E5%90%88%E4%B8%80-%E9%BE%8D%E6%B4%9E%E4%B8%80%E6%97%A5%E9%81%8A/</url>
    <content><![CDATA[<p>最近二級警戒有比較趨緩，趁著快要開學了趕緊跟家人一起出去玩一波。這一次選的地方是位在基隆外海的基隆嶼，單純抱著從來沒有上去過的新奇感，於是立馬手刀買了三張船票，隔天馬上開車上基隆出海去囉
XD。</p>
<p>keywords: 基隆嶼、龍洞 <span id="more"></span></p>
<p>七點起了一個大早，一早就要馬上趕路去基隆搭船，一路上天氣真的超極好，尤其是靠近海邊的景色，藍天的藍、大海的藍、再加上群山的綠，美景盡收眼底。我們買的船票是
10:00 從八斗子漁港出發的，船公司大約在 9:45
分就會集合大家準備準備了，所以如果之後有安排這個行程的話，記得要預留一些時間喔。</p>
<p>由於很早出門，大約 9:00
就到了八斗子漁港了，一到漁港馬上被超好的天氣收買。下圖為漁港入口。 <img
src="https://i.imgur.com/aH9ClYW.png" alt="Image" /></p>
<p>由停車場往另外一個方向看過去，藍天白雲，一球一球的小山正是基隆的一大特色。
<img src="https://i.imgur.com/DI49OLz.png" alt="Image" /> <img
src="https://i.imgur.com/QVPdbmY.png" alt="Image" /></p>
<p>準備出發啦啦 (口罩什麼的當然還是要乖乖戴好戴滿囉) <img
src="https://i.imgur.com/h5d17nl.png" alt="Image" /></p>
<p>我們搭的船是「華倫」這一家公司，而行程的部份我們是選擇「三合一」，也就是：山步道
+ 海步道 + 繞島一圈，一個人來回
850，我自己覺得非常值得，一生也只會上這麼一次基隆嶼哈哈 <img
src="https://i.imgur.com/xSQPNMO.png" alt="Image" /></p>
<p>一些港口的風景照： <img src="https://i.imgur.com/IyKUewT.png"
alt="Image" /> <img src="https://i.imgur.com/4TvJtdK.png" alt="Image" />
<img src="https://i.imgur.com/BSBWQnP.png" alt="Image" /></p>
<p>十點到啦！今天要上船的人還蠻多的呢，看起來大家也有好好把握到好天氣呢~下面就是今天要搭的船啦
<img src="https://i.imgur.com/omGBcln.png" alt="Image" /></p>
<p>船整體來說並不大，就像下面這張圖一樣，左右兩邊可以坐人，但是船內的空間非常小，大概只能容納得下
7 8
人左右，不過這可是海景第一排乀，抓著扶手看著船急速離開漁港的感覺真的超極爽的啦
<img src="https://i.imgur.com/r4Tcm3T.png" alt="Image" /></p>
<p>離開囉！ <img src="https://i.imgur.com/W41f8s1.png"
alt="Image" /></p>
<p>慢慢往基隆嶼前進中…會越來越大喔 XD <img
src="https://i.imgur.com/kWmmdag.png" alt="Image" /></p>
<p>在船後往港口照，可以看到忘幽谷，還有位在山上的九份 <img
src="https://i.imgur.com/kRVLkdb.png" alt="Image" /></p>
<p>基隆嶼越來越大啦！心情有點小期待呢！畢竟是人生第一次上去哈哈 <img
src="https://i.imgur.com/IimojOx.png" alt="Image" /></p>
<p>首先我們會先環島一圈，簡單的在船上看看基隆嶼不同角度的不同面貌 <img
src="https://i.imgur.com/J6NBLJM.png" alt="Image" /></p>
<p>基隆嶼的右手側，導遊特別提到，基隆嶼在冬天的時候會受到西北季風的強烈影響，海浪那時會非常非常的高，猛烈的海浪拍打在山壁上形成峭壁以及岩石，甚至沒有草的地方就代表海浪曾經有到過的地方，大浪真可怕…
<img src="https://i.imgur.com/YMI14br.png" alt="Image" /></p>
<p>接著來到了西北面，導遊說這一塊巨大岩石稱作「象鼻岩」，形狀非常像一個大象的鼻子，以及右邊的一個大耳朵。而象鼻岩的下方則是非常多個海蝕洞，也間接證明了這邊冬天的浪真的很大…
<img src="https://i.imgur.com/Ve1wkzu.png" alt="Image" /></p>
<p>在船上大約 30 分鐘後就到了基隆嶼的碼頭啦 <img
src="https://i.imgur.com/quJN2V7.png" alt="Image" /></p>
<p>由碼頭往基隆嶼群山望去，(其實山真的算高的呢…想到我們等等要直線往上爬不禁就覺得爆汗
XD) <img src="https://i.imgur.com/9O8PyHQ.png" alt="Image" /></p>
<p>港口邊可以看到河豚乀！(嗯…這是河豚嗎 XD) <img
src="https://i.imgur.com/ldjdY3b.png" alt="Image" /></p>
<p>接下來就正式往基隆嶼的登山步道前進啦。基隆嶼一共有兩個步道：海邊步道以及登山步道。登山步道可以一路往山上爬到全台數一數二高的燈塔
-
「基隆嶼燈塔」，而海邊步道則是繞基隆嶼半圈，路上會經過兩座小廟來保祐海上的漁民及作業人員的平安。
<img src="https://i.imgur.com/uGKxzwU.png" alt="Image" /></p>
<p>路上經過的基隆嶼上唯一的門牌號碼，聽說好像是以前上面是一間商店的樣子，但現在全撤掉了
<img src="https://i.imgur.com/hZMDuoW.png" alt="Image" /></p>
<p>由門牌往海邊照去，阿…波光粼粼的，天氣真好…。一路上帶領我們的導遊一直說：「今天天氣真的很好，都可以很清楚地看到基隆港了呢」，至少有五六遍了吧
XD <img src="https://i.imgur.com/I0EH1Vj.png" alt="Image" /></p>
<p>到了這邊就是山線與海線的分隔處啦，先往山上去 XD <img
src="https://i.imgur.com/tjmmI6C.png" alt="Image" /></p>
<p>分隔處隨手拍，真的是怎麼拍怎麼好看。 <img
src="https://i.imgur.com/c7Zy6aj.png" alt="Image" /> <img
src="https://i.imgur.com/fXROuDf.png" alt="Image" /></p>
<p>從爬山步道回頭看 <img src="https://i.imgur.com/VvN6Ip6.png"
alt="Image" /></p>
<p>往上看，嗯…好陡 XD <img src="https://i.imgur.com/jr4PnSA.png"
alt="Image" /></p>
<p>遙看小基隆嶼 <img src="https://i.imgur.com/DeucWIk.png"
alt="Image" /></p>
<p>整個登山步道很多都是由這種木階梯所組成，走起來非常舒服，就是有點多階
XD <img src="https://i.imgur.com/IBt4Jxz.png" alt="Image" /></p>
<p>大海！越往上爬風景越好越不一樣 <img
src="https://i.imgur.com/rRmjRnL.png" alt="Image" /></p>
<p>因為潮流的關系因此海水的顏色不太一樣，分層的感覺 <img
src="https://i.imgur.com/6z9Rdhg.png" alt="Image" /></p>
<p>木階梯走完後會看途中唯的休息點 - 小涼亭，超多人在休息的
XD。由小涼亭往山上拍。還有一半的路程要走…加油！ <img
src="https://i.imgur.com/ReXRrJo.png" alt="Image" /></p>
<p>回頭望小涼亭。接下來我們到了基隆嶼的稜線上，左右兩邊皆是藍藍的大海及天空，視野超開闊的啦
<img src="https://i.imgur.com/L6wM5cv.png" alt="Image" /></p>
<p>小基隆嶼以及海平線，真的是海天一色呢… <img
src="https://i.imgur.com/n31dF7S.png" alt="Image" /></p>
<p>自拍 XD <img src="https://i.imgur.com/dE3XweT.png" alt="Image" /></p>
<p>位在稜線上的步道，依舊的陡，而且路變成了原始的石子，比較相對不好走一些
<img src="https://i.imgur.com/wboXuVh.png" alt="Image" /></p>
<p>大黃花！導遊說這個叫做「金花石蒜」 <img
src="https://i.imgur.com/LHk0EOE.png" alt="Image" /></p>
<p>小基隆嶼與金花石蒜 <img src="https://i.imgur.com/M5xJtJD.png"
alt="Image" /></p>
<p>更多站在稜線上往回程拍，剛好有一艘船在水中劃出一道白色的浪花，好有動態的感覺呢。稜線的視野真的很好。
<img src="https://i.imgur.com/zBqYsRA.png" alt="Image" /></p>
<p>不知不覺離涼亭越來越高、越來越遠… <img
src="https://i.imgur.com/93ysIWG.png" alt="Image" /></p>
<p>飛機！ <img src="https://i.imgur.com/zTKbu9h.png" alt="Image" /></p>
<p>終於！登上了基隆最高的地方，標高 182
公尺，雖然很累不過全部被美景被打消了 XD <img
src="https://i.imgur.com/l5B806O.png" alt="Image" /></p>
<p>快到燈塔了！先和燈塔合照一張 XD <img
src="https://i.imgur.com/1WRtqGG.png" alt="Image" /></p>
<p>咦…剛剛停在小基隆嶼的船開走了，喔不對，一共有兩艘。 <img
src="https://i.imgur.com/BREuOeg.png" alt="Image" /></p>
<p>燈塔前的一座日本人留下來的堡壘。這是以前日治時代時，由日本人在基隆部署的一些軍事設施
<img src="https://i.imgur.com/r8c4QBf.png" alt="Image" /></p>
<p>蜘蛛與海 XD <img src="https://i.imgur.com/K81yrJF.png"
alt="Image" /></p>
<p>終於到燈塔啦啦，好累
XD，由於時間壓力，我們並沒有在燈塔上停留太久，速速下山往海邊步道前進。
<img src="https://i.imgur.com/sQnRhM2.png" alt="Image" /></p>
<p>海邊步道的景色，不一樣的風情呢 <img
src="https://i.imgur.com/f4u9L28.png" alt="Image" /></p>
<p>海邊步道一共有兩座廟，一是土地公，一是海上觀音。只可惜觀音廟因為步道毀損目前道達不了…
<img src="https://i.imgur.com/gBey22T.png" alt="Image" /></p>
<p>被封起來了 QQ <img src="https://i.imgur.com/Ne1XSKW.png"
alt="Image" /></p>
<p>海邊步道雜拍 <img src="https://i.imgur.com/NeuUueE.png"
alt="Image" /> <img src="https://i.imgur.com/nnl57eU.png" alt="Image" />
<img src="https://i.imgur.com/ut2PiaQ.png" alt="Image" /> <img
src="https://i.imgur.com/kOqtQtC.png" alt="Image" /></p>
<p>因為我們訂的船票算早的 10 點出發，大約 1
點半回到八斗子港口，趁著難得的好天氣，於是立馬規劃了去龍洞的行程，要去浮潛囉！</p>
<p>龍洞的照片我沒拍太多，畢竟我在水下嘛 XD，就隨意放上一些照片囉 <img
src="https://i.imgur.com/FFKK0bf.png" alt="Image" /> <img
src="https://i.imgur.com/5bRH5Yl.png" alt="Image" /> <img
src="https://i.imgur.com/eWWdLi0.png" alt="Image" /> <img
src="https://i.imgur.com/QzQsjEz.png" alt="Image" /></p>
<p>我們下水的地方 <img src="https://i.imgur.com/AKQc958.png"
alt="Image" /></p>
<p>今天的行程真的充實，又有爬山又有去海邊浮潛，真的是「遊山玩水」XD。本來大家真的都累到了，打算乘著下班時間前趕快上高速公路回家，結果開在濱海公路上時剛好正是夕陽西下時，於是趕快在路邊找一個停車場下車欣賞。(濱海公路旁怎麼那麼多停車場啦
XD) <img src="https://i.imgur.com/QcJf34S.png" alt="Image" /> <img
src="https://i.imgur.com/9l8PUmG.png" alt="Image" /> <img
src="https://i.imgur.com/TdanIR8.png" alt="Image" /> <img
src="https://i.imgur.com/BkUq3vK.png" alt="Image" /></p>
<p>回望濱海公路 <img src="https://i.imgur.com/HbjPXo0.png"
alt="Image" /></p>
<p>最後的夕陽真的是意外的驚喜，為整趟旅程更加上神來一筆。這個就是非常充實的基隆嶼一日遊啦，有山有水，再加上天氣真爆炸好，整個大加分，這樣子的美景還不是天天都有的呢
(不過回到家的曬傷又的另外一個故事了 XD)</p>
]]></content>
      <categories>
        <category>遊記</category>
      </categories>
      <tags>
        <tag>基隆嶼</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Going deeper with Image Transformers - CaiT 引入 LayerScale 及 class-attention layers 優化 DeiT</title>
    <url>/2021/09/08/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Going-deeper-with-Image-Transformers-CaiT-%E5%BC%95%E5%85%A5-LayerScale-%E5%8F%8A-class-attention-layers-%E5%84%AA%E5%8C%96-DeiT/</url>
    <content><![CDATA[<p>本篇論文是 Facebook AI 團隊在 2021 3 月所提出，作者 Hugo Touvron 與
DeiT 是同一個人。論文主要的貢獻有二：提出了 LayerScale 優化了
Transformer 的網路，以及 class-attention layers 進一步使得 class token
的使用變得更合理。</p>
<p>CaiT 沿用了 DeiT ViT 的核心精神，並再加入新概念加以改進，在 ImageNet
上取得了 86.3% 的 Acc1 performance，比原本的 DeiT 多了不少。</p>
<p>keywords: CaiT、LayerScale、class-attention layers <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>這篇論文的核心在於優化 Transformer
的網路架構，使得網路更好訓練，不因網路越深就越難收斂。作者的核心思想就是：<strong>網路架構
(architecture design) 與 optimization (優化)
是互相呼應的</strong>，ResNet 就是一個非常經典的例子。</p>
<p><span class="math display">\[
x_{l+1} = g_l(x_l) + R_l(x_l)
\]</span></p>
<p>加上 Residual
後並沒更改太多架構，但是變得非常好訓練，網路效果也因而上升了一個層級。這種明明沒改什麼網路效果確超乎想像的例子，證明了網路優化的重要性。</p>
<p>那 Transformer 呢？每個 Block 內的公式可寫成以下：<span
class="math inline">\(\eta\)</span> 為 LayerNorm</p>
<p><span class="math display">\[
\begin{gathered}
x_l&#39; = x + SA(\eta(x_l))\\
x_{l+1} = x&#39;_l + FFN(\eta(x&#39;_l))
\end{gathered}
\]</span></p>
<p>作者經由實驗給出的答案，有下列兩項的改進：</p>
<ol type="1">
<li>LayerScale 使加深後的 Transformer 更容易收斂，更好訓練</li>
<li>class-attention layers 更合邏輯的來處理 class token 的問題</li>
</ol>
<h2 id="網路架構">2. 網路架構</h2>
<h3 id="layerscale">LayerScale</h3>
<p>作者提到 ViT DeiT 與原 Transformer 的 Encoder 不同，原始 Transformer
的實作方法是把正規化放在後面 (post-norm)，而 ViT DeiT
等實作方法則為把正規化放在前面 (pre-norm)</p>
<p>因此作者設計了四種不同正規化的排列組合，來試試看哪一種對於網路的優化較高，優化高後下一步就可以往把網路加深的方向改進。</p>
<p><img src="https://i.imgur.com/VOPovED.png" alt="Image" /></p>
<p>對應上圖分別為：(a) ViT DeiT 原始作法、(b) ReZero and Fixup、(c)
ReZero and Fixup 加上正規化、(d) LayerScale</p>
<p><strong>(a) ViT DeiT 原始作法</strong>：經典的 pre-norm
作法，先做一次 LayerNorm 再進行 FFN 或者是 SA 運算。</p>
<p><strong>(b) ReZero and Fixup</strong>：取消了
LayerNorm，並新增了一個可學習的參數 <span
class="math inline">\(\alpha\)</span> 作用在 Residual 上，用來決定網路中
Residual 與運算 Block 各所占的比例。而 ReZero 為 <span
class="math inline">\(\alpha\)</span> 初始為 0、Fixup 為 <span
class="math inline">\(\alpha\)</span> 初始為
1。作者在後續實驗中證明這個方法不會使網路訓練時收斂</p>
<p><strong>(c) ReZero and Fixup 加上正規化</strong>：就是 (a) (b)
的結合，實驗證實有效</p>
<p><strong>(d) LayerScale</strong>：這是本篇論文提出效果最好的方法，也是
CaiT 使用的方法。把 (c) 乘上的 <span
class="math inline">\(\alpha\)</span>
改為乘上一個對角矩陣，公式如下：</p>
<p><span class="math display">\[
\begin{gathered}
  x_l&#39; = x_l + \mathrm{diag}(\lambda_{l,1},...,\lambda_{l,d}) \times
\mathrm{SA}(\eta(x_l))\\
  x_{l+1} = x_l&#39; + \mathrm{diag}(\lambda_{l,1},...,\lambda_{l,d})
\times \mathrm{FFN}(\eta(x_l&#39;))
\end{gathered}
\]</span></p>
<p>矩陣中的 <span class="math inline">\(\lambda\)</span>
是可學習參數，一般預設值都會設成很小，而且預設值會隨著網路的加深越來越小。論文提供的初始參數為：0
層時 -&gt; <span class="math inline">\(0.1\)</span>、18 層時 -&gt; <span
class="math inline">\(10^{-5}\)</span>、24 層時 -&gt; <span
class="math inline">\(10^{-6}\)</span></p>
<p>作者使用一個對角矩陣是為了可以<strong>各別調整各 Layer
中的重要度</strong>，而非像 <span class="math inline">\(\alpha\)</span>
一樣每個 Layer 一視同仁，一起乘上某個值。比起 <span
class="math inline">\(\alpha\)</span>，LayerScale
更能增加網路的多樣性，進一步調整及優化 Residual 與 Block 的關系。</p>
<p>而值一開始設定小的原因，是為了在學習時更能專注在自己的 Block
上，讓大部份的資訊向 shortcut 流，使得與 Identity Map 比較接近</p>
<h3 id="class-attention-layers">class-attention layers</h3>
<p>除了優化 Transformer Block 之外，作者對於 ViT 中使用的 class token
抱持懷疑。作者認為 ViT 在引入 class token 時，是直接放進網路一開始，與
patch token 一同訓練，這使得 class token
要在網路中起到以下兩個作用：</p>
<ol type="1">
<li>引導 patch token 一同截取出網路特徵 attention map</li>
<li>最後把 patch token 的訓息總合，得到最後分類的結果</li>
</ol>
<p>class token 要同時達到這兩個目的看似有些自我矛盾，因此作者提出
class-attention layers 把以上兩個目標分成兩個 stage 來實作。</p>
<p><img src="https://i.imgur.com/ZRJ2cRP.png" alt="Image" /></p>
<p>如上圖所示，作者試者把 class token (CLS)
移到最後一個階段才做運算。因而網路分成兩大部份：</p>
<ol type="1">
<li>patch token 之間的 self-attention，沒有 class token 來參與</li>
<li>class-attention，加入 class token</li>
</ol>
<p><strong>patch token</strong>：這個部份與 ViT 差不多，只是沒有 class
token 進來參數運算</p>
<p><strong>class-attention</strong>：加入 class-attention
後，<strong>patch token 會被 freeze 起來，不更新權重</strong>，而 class
token 會從 patch token 那提取特徵，也不會把訊息反向回傳給 patch
token。簡單來說 class token 單向的從 patch token
得到特徵訊息，接著再傳給 FFN 做最後的分類。</p>
<p>個人理解為，class token 有點像 student model。把前面 patch token
辛苦學到的特徵，用簡單的一兩層來吸收在自己身上。全程 class token
不參與運算，最後兩個 token 資訊是單向流動的，且最後 patch token
不參與分類，全由 class token 來負責。</p>
<p>詳細的 class-attention 公式為：</p>
<p>參與運算的有二：<span class="math inline">\(z=[x_\mathrm{class},
x_\mathrm{patches}]\)</span> 與 <span
class="math inline">\(x_\mathrm{class}\)</span>。首先先分三組，注意的地方是
Q 只有 class token，而 K, V 是 class token + patch token</p>
<p><span class="math display">\[
\begin{gathered}
  Q=W_qx_{\mathrm{class}}+b_q\\
  K=W_kz+b_k\\
  V=W_vz+b_v
\end{gathered}
\]</span></p>
<p>Q 乘上 K 的轉置，並 scale-dot</p>
<p><span class="math display">\[
a=\mathrm{Softmax}(Q\cdot K^T/\sqrt{d/h})
\]</span></p>
<p>最後乘上 V，並接上一個 Residual，把計算後的結果與原 class token
相加</p>
<p><span class="math display">\[
\mathrm{out}_\mathrm{CA} = W_oAV+b_o
\]</span></p>
<p>經作者實驗以上步驟做兩次就好了，太多效果不好。</p>
<h2 id="experiments">3. Experiments</h2>
<h3 id="與-sota-相比">與 SOTA 相比</h3>
<p>與 DeiT 比效果好很多，與最大的 NFNet 比，差一點點</p>
<p><img src="https://i.imgur.com/cEGfrYl.png" alt="Image" /></p>
<h3 id="不同大小網路架構">不同大小網路架構</h3>
<p>分 XXS XS S M 來代表 attention map
的數量，<strong>不是深度！</strong>，深度階為 24 或 36 層，相較於 ViT 的
16 層的確深了不少</p>
<p><img src="https://i.imgur.com/QKQzmKd.png" alt="Image" /></p>
<h3
id="實驗一不同使訓練更穩定的方法">實驗一、不同使訓練更穩定的方法</h3>
<p>作者除了試 LayerScale 外，還嘗試其它方法，結論如下圖：</p>
<p><img src="https://i.imgur.com/RkG7cTR.png" alt="Image" /></p>
<p><strong>調整不同深度的 drop rate</strong>：越深越大，結論：沒用</p>
<p><strong>正規化</strong>：比較 (b) 與 (c) 發現加上了 LayerNorm
後，網路就可以收斂了。單純使用 Fixup ReZero 沒什麼用</p>
<p><strong>LayerScale</strong>：橘色為沒加 LayerScale、藍色為有加
LayerScale。數值越大代表 Residual 的作用越大，代表模型離 Identity
越遠。作者發現加上 LayerScale 後每一層變得更 uniform
了，證明更能專注在每一個 Block 中</p>
<p><img src="https://i.imgur.com/Zj2hSq5.png" alt="Image" /></p>
<h3 id="實驗二class-attention-layers-的作用">實驗二、class-attention
layers 的作用</h3>
<p>先是證實了加上 class-attention 會比沒加的 DeiT 好上一點點</p>
<p>再來得到 class attention layer 最好的層數是 2 層</p>
<p><img src="https://i.imgur.com/NaTTaY7.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>CaiT 在模型優化上有兩個貢獻：引入 LayerScale 使得 Residual 在
Transformer 中更能專注在一個 Block 上。引入 class-attention 使得 class
token 操作變得合理一些些。</p>
<p>同時也因為網路優化的關系，CaiT 在層數方面，從 ViT 的 16 層到達了 36
層 (最高還有 48 層的…)</p>
<p>本論文成功的證明了網路優化的重要性，Transformer
整體架構的合理性又往前了一小步…</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/363370678</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>為什麼 BLoC 中要使用 Equatable 呢？</title>
    <url>/2021/09/23/%E7%82%BA%E4%BB%80%E9%BA%BC-BLoC-%E4%B8%AD%E8%A6%81%E4%BD%BF%E7%94%A8-Equatable-%E5%91%A2%EF%BC%9F/</url>
    <content><![CDATA[<p>這幾天在學 flutter ，看到大家說當程式大起來的時候，state
會不好整理及控制。而 React 中有 Redux ，在 flutter
中大家最受歡迎的方法是 flutter_bloc ，以下簡單筆記我學 BLoC
的一些心路歷程</p>
<p>keywords: BLoC、Equatable <span id="more"></span></p>
<h2 id="為什麼-bloc-中要使用-equatable-呢">為什麼 BLoC 中要使用
Equatable 呢？</h2>
<p>在了解為什麼要使用 Equatable 之前，我們先來看看什麼是
equals、hashCode</p>
<h3 id="vs-equals-vs-hashcode">== vs equals vs hashcode</h3>
<p>最初在使用這些觀念的語言是 Java，而 Java
對於以上三個值有不同的定義</p>
<p>所謂「==」是指符號兩邊的「記憶體位值」是否相等，兩對象是不是參考同一個位置</p>
<p>而 equal 則是 Java 提供的一個 Override 方法，如果我們沒有特別去
Override 它的話，功能就與 「==」一致。那什麼時候我們會用到 equal
呢？當我們今天要比較的資料是自定義的 class 時，如：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">People</span> </span>&#123;</span><br><span class="line">	String shirt_color;</span><br><span class="line">	<span class="keyword">int</span> age;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>要怎麼比較兩個不同的 People 呢？不太可能直接用 ==
來比較吧，這個時候我們就會用到 equal ，把原本的定義 override
加上自己的定義</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">People Tom = <span class="keyword">new</span> People(<span class="string">&quot;blue&quot;</span>, <span class="number">20</span>);</span><br><span class="line">People Alex = <span class="keyword">new</span> People(<span class="string">&quot;red&quot;</span>, <span class="number">15</span>);</span><br><span class="line"></span><br><span class="line">Tom == Alex <span class="comment">// ??!!</span></span><br></pre></td></tr></table></figure>
<p>像上面這個例子中 People 中有兩個 member，shirt_color 以及
age，因此在實作 equal 時，要特別去比較這兩個 member
的值是否相等，完整程式如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">EqualsDemo</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> String shirt_color;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">int</span> age;</span><br><span class="line"> </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span> </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// (一) </span></span><br><span class="line">  	<span class="keyword">if</span> (<span class="keyword">this</span> == o) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  	</span><br><span class="line">    <span class="comment">// (二)</span></span><br><span class="line">    <span class="keyword">if</span> (o == <span class="keyword">null</span> || getClass() != o.getClass()) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  	</span><br><span class="line">    <span class="comment">// (三)</span></span><br><span class="line">    EqualsDemo that = (EqualsDemo) o;</span><br><span class="line">  	<span class="keyword">if</span> (name != <span class="keyword">null</span> ? !name.equals(that.name) : that.name != <span class="keyword">null</span>) </span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  	</span><br><span class="line">    <span class="keyword">return</span> info != <span class="keyword">null</span> ? info.equals(that.info) : that.info == <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  	<span class="keyword">int</span> result = name != <span class="keyword">null</span> ? name.hashCode() : <span class="number">0</span>;</span><br><span class="line">  	result = <span class="number">31</span> * result + (info != <span class="keyword">null</span> ? info.hashCode() : <span class="number">0</span>);</span><br><span class="line">  	<span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到 (三) 的地方，我們自己多自定義比較 shirt_color 與 age
是否相同，來符合我們的需求</p>
<p>那 (一) 與 (二) 呢？這裡就要先提到 equals() 的 4 個特性</p>
<ol type="1">
<li>反射性：<code>x.equals(x)</code> 必需是 True</li>
<li>非空性：<code>x.equals(null)</code> 必需是 False</li>
<li>對稱性：<code>x.equals(y)</code> 與 <code>y.equals(x)</code>
必需同時成立</li>
<li>類推性：如果 <code>x.equals(y)</code> True、
<code>y.equals(z)</code> True 則 <code>x.equals(z)</code> 也必定
True</li>
</ol>
<p>而式中的 (一) (二) 正是實作了 equals()
的前兩個特性，確保不違反定律</p>
<p>但是最下面為什麼還要再 override hashCode 呢？所謂的 hashCode 是
<strong>Java 把變數所存的實體記憶體位置經過一個 hashmap
後得到的值</strong>，在 Java 中每一個變數都會有一個獨一無二的 hashCode
，如果它們是同一個變數，則 hashCode 會相同</p>
<p>那為什麼要這樣設計呢？假設我們今天有 1000 個變數，今天新增第 1001
個變數，我們要怎麼知道這第 1001 變數是不是與前 1000
的其中一個相同呢？當然最笨的方法就是一個一個找，可是太沒效率了。於是
hashCode 就來解決這個問題，hashCode 利用 hashmap
的特性來達到：只要是同一個變數，則 hashCode 就會相同</p>
<p>注意 hash 的小細節喔！</p>
<ul>
<li>兩對象相等，所產生的 hashCode 一定一樣</li>
<li>兩 hashCode 一樣，不一定代表這兩個對象相等喔 (因為 hash 的
collide)</li>
</ul>
<p>總結：如果要在 Java 中 override 「==」的話，除了要 override equals
比較其它自定變數，也要 override hashCode
記這這個為了加速了誕生的東西，不然會產生兩相同對象但 hashCode
不同的事情發生</p>
<h3 id="bloc-與-equatable">BLoC 與 Equatable</h3>
<p>經過上面的解釋可以了解了 equal 以及 hashCode，而 Dart 與 Java
類似也有相同的概念，於是有了 Equatable 這個套件讓我們不用再手動 override
equal 以及 hashCode 了，它會自動幫我們做這一件事情</p>
<p>只是…為什麼 BLoC 中要使用到它呢？</p>
<p>當我們建立一個 class 繼承 Equatable 時，我們可確保 LoginStates
是唯一的，當這個 state
發生兩次以上時，不會再一個一模一樣的呼叫，也不會再重建裡面全部的
Widget</p>
<figure class="highlight dart"><table><tr><td class="code"><pre><span class="line"><span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">LoginStates</span> <span class="keyword">extends</span> <span class="title">Equatable</span></span>&#123;&#125;</span><br></pre></td></tr></table></figure>
<p>或者是 Stream 與 Equatable 之間，當 Stream 中有兩個一模一樣的 state
被呼叫時，第二個會自動省略</p>
<figure class="highlight dart"><table><tr><td class="code"><pre><span class="line"><span class="meta">@override</span></span><br><span class="line">Stream&lt;LoginStates&gt; mapEventToState(MyEvent event) <span class="keyword">async</span>* &#123;</span><br><span class="line">  <span class="keyword">yield</span> LoginData(<span class="keyword">true</span>, <span class="string">&#x27;Hello User&#x27;</span>);</span><br><span class="line">  <span class="keyword">yield</span> LoginData(<span class="keyword">true</span>, <span class="string">&#x27;Hello User&#x27;</span>); <span class="comment">// This will be avoided</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>更詳細的解說可以到以下網址：<a
href="https://medium.com/flutterworld/flutter-equatable-its-use-inside-bloc-7d14f3b5479b">https://medium.com/flutterworld/flutter-equatable-its-use-inside-bloc-7d14f3b5479b</a></p>
<h3 id="結論">結論</h3>
<p>Equatable 省下了我們 override equals 與 hashCode 的時間</p>
<p>而 BLoC 中加入 Equatable 可以避免重覆 state 不必要的重覆呼叫及重建
Widget，優化了速度以及記憶體</p>
]]></content>
      <categories>
        <category>Dart &amp; Flutter 開發</category>
      </categories>
      <tags>
        <tag>BLoC</tag>
      </tags>
  </entry>
  <entry>
    <title>合歡山上看星星 - 合歡山主峰北峰兩天一夜遊 (一)</title>
    <url>/2021/10/03/%E5%90%88%E6%AD%A1%E5%B1%B1%E4%B8%8A%E7%9C%8B%E6%98%9F%E6%98%9F-%E5%90%88%E6%AD%A1%E5%B1%B1%E4%B8%BB%E5%B3%B0%E5%8C%97%E5%B3%B0%E5%85%A9%E5%A4%A9%E4%B8%80%E5%A4%9C%E9%81%8A-%E4%B8%80/</url>
    <content><![CDATA[<p>大概在九月初的時候與高中同學約去山上看夜景，那時我無意之間提到：「要是在合歡山上風景一定更棒」。沒想到這無意的一句話竟然打動了我同學，也就是主揪「Jack」成為這次旅行的主要契機。</p>
<p>一年前，當時還是在台中讀大學的我去過一次合歡山，時隔一年，現在與「Jack」以及另一名同學「ㄨㄐ」重新上山。對我同學來說這一趟旅行是充滿未知的挑戰，但對我而言，卻是一個趟充滿懷念的旅程。</p>
<p>keywords: 合歡山 <span id="more"></span></p>
<p>我們這一次的旅程一共分成三天，前一天我們先在台中住一晚，接著隔天一大早立馬上埔里租車，開始合歡山的兩天一夜遊。</p>
<p>下台中的第一天，我一樣搭著國光號下台中，但與以前不同的是我這次要選擇搭台中捷運回中興，至於為什麼嘛…就好玩囉
XD，想說疫情這麼嚴重下台中捷運的營收一定很不好，來支持一下 XD</p>
<p><img src="https://i.imgur.com/O2RNyjZ.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/CqXx3WX.png" alt="Image" /></p>
<p>但萬萬沒想到的事情是我在豐樂公園站下車要等 73
公車回中興時，竟然跑錯公車站，跑到對向的公車站去了，害我多等了 30
分鐘的公車，時間上完全沒有比較快，而且還快被台中的天氣給熱死了…。只能說…台中的公車接駁動線完全不行呢…
(還是這是我個人的問題阿哈哈)</p>
<p>雖然第一天回台中時出了一個小烏龍，但這完全不影響我上合歡山的心情。晚上好好的在大學同學家休息，第二天起了個大早，準備要搭
7 點往埔里的公車囉！</p>
<p>到了埔里我們速速到租車行去租到機車，沒錯這一次旅程我們三個人是租了兩台機車上山的，我負責把帳棚以及睡墊運上山，而另一台車則是
Jack 載著 ㄨㄐ 上山去。</p>
<p><img src="https://i.imgur.com/DPfoWvu.png" alt="Image" /></p>
<p>在台 14 線上的一家 7-11
合照一下，記念一下今天超極好的天氣。準備好後就要一 ~ 路上山囉！</p>
<p><img src="https://i.imgur.com/r8FuRsQ.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/rhFPTGC.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/eQekcLX.png" alt="Image" /></p>
<p>騎了約 30
分鐘，在途中經過的莫那魯道記念碑休息一下，順便看看常常在歷史課本上看到的雕像
XD</p>
<p>正在地上一起弄手機腳架的我和 Jack XD</p>
<p><img src="https://i.imgur.com/6HxJz3H.png" alt="Image" /></p>
<p>與雕像的合照 (是剛剛放在地上的腳架拍出來的 XD)</p>
<p><img src="https://i.imgur.com/mM3ASLd.png" alt="Image" /></p>
<p>蔥鬱的樹林與林間映下了的陽光，更是讓這個具有歷史意義的地方多了一層靜謐的莊嚴感。</p>
<p><img src="https://i.imgur.com/Bj4lxQP.png" alt="Image" /></p>
<p>離開了莫那魯道記念碑後，我們繼續往山林裡前進。隨著我們海拔的上昇，空氣中一股微微的涼意也慢慢湧上來了，是時候該把外套給穿上身了。</p>
<p>越過清境不多做停留，而在清境的最高的全家做最後的補給，同時也是台 14
線上最後一家便利商店。這個「清境最高」的名稱真是名不虛傳，所有的包裝都鼓成下一秒就是爆掉的樣子
XD</p>
<p><img src="https://i.imgur.com/uhXrdr2.png" alt="Image" /></p>
<p>這家全家外面看起來超像走可愛風格的民宿 XD ，連門口都做的小小的</p>
<p><img src="https://i.imgur.com/FGVxLrJ.png" alt="Image" /></p>
<p>要爆掉的零食們</p>
<p><img src="https://i.imgur.com/A6yWYhN.png" alt="Image" /></p>
<p>做完最後的補給後，終於來到了我們今天第一個目的地 -
卡爾小鎮。卡爾小鎮位在台 14 線 18
公里處，在翠峰附近，特色是可以在這個露營地看到滿天的星星呢。</p>
<p><img src="https://i.imgur.com/web0LtH.png" alt="Image" /></p>
<p>而老闆娘也很熱情，一直跟我們說你們沒有桌子是要怎麼煮飯
XD，在老闆娘的大力推薦下我們多花了 500
元租了一個野炊的地方，事後想起來真是正確的選擇呢…</p>
<p>露營地與機車們</p>
<p><img src="https://i.imgur.com/Mw8rJ3g.png" alt="Image" /></p>
<p>多花 500 元租的野炊地，還好有租，不然晚上煮飯什麼都看不到…</p>
<p><img src="https://i.imgur.com/Q2YazsK.png" alt="Image" /></p>
<p>在卡爾小鎮整理整理把比較重的行李就先放在那了，準備輕裝上山，除了正式開始今天漂亮的合歡山美景外，等等也要開始今天爬合歡北峰的重頭戲。</p>
<p>路過翠峰的路牌，過了這個路牌就正式要上山了！</p>
<p><img src="https://i.imgur.com/SxK51cT.png" alt="Image" /></p>
<p>翠峰的管制亭，看影子就知道山上的太陽有多大…</p>
<p><img src="https://i.imgur.com/BQbFiSM.png" alt="Image" /></p>
<p>上山前合照一張</p>
<p><img src="https://i.imgur.com/GAACVvg.png" alt="Image" /></p>
<p>經過了大約一個小時的車程，我們因為時間因素一路直上合歡北峰登山口，只可惜沒有辨法一路沿著景點慢慢拍照了。</p>
<p>到停車場休息一下，山上的天氣真的超棒的呢</p>
<p><img src="https://i.imgur.com/KhyOC5N.png" alt="Image" /></p>
<p>北峰登山口，與登山口的群山景，藍天綠地的，每個顏色都超飽合的 XD</p>
<p><img src="https://i.imgur.com/wNGbbzo.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/hplZ3ks.png" alt="Image" /></p>
<p>上山前大家都還充滿自信，面帶笑容，都不知道接下來會遇到怎樣子的困難…</p>
<p><img src="https://i.imgur.com/16cDYCf.png" alt="Image" /></p>
<p>爬阿爬…</p>
<p><img src="https://i.imgur.com/P5IOYFV.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/hF7ncix.png" alt="Image" /></p>
<p>在我們三人之中，唯一有登山經驗的 Jack
一直是我們一行的掌鏡人，因為他永遠走在最前面 XD (by the way
這張景也取得太好了吧… Jack 真有你的)</p>
<p><img src="https://i.imgur.com/leMEz9w.png" alt="Image" /></p>
<p>偶爾休息一下才可以抓到機會幫你拍幾張</p>
<p><img src="https://i.imgur.com/NgBKZOk.png" alt="Image" /></p>
<p>到了半山腰突然發生突發狀況，ㄨㄐ
的生理期一到山上突然大爆血，於是我們不得不停下來討論該怎麼辨。看著面有難色的
ㄨㄐ 我決定先馬上離開北峰，再想想還有什麼其它替代行程，而 ㄨㄐ
則看著我們說：我可以的！，Jack 看著 ㄨㄐ
決定先休息一下，看看身體有沒有好轉後再上山。</p>
<p>這個突發狀況讓我和 Jack 的意見產生了分歧，我擔心 ㄨㄐ
的硬撐身體不舒服，而 Jack
覺得如果就此折返這趟程旅程就留下了個遺憾。雖然我們不會到吵起來的地步，但也為爬北峰這個旅程多了一個未知數。後來我們決定走
5 分鐘休息 5 分鐘，能走多少就走多少，一路上 Jack
也一直不斷的在尋問山友們前方的路況，確認是不是 ㄨㄐ 可以負荷的，而 ㄨㄐ
也同意這個決定，繼續往山頂前進。</p>
<p><img src="https://i.imgur.com/7E9BEbB.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/jBie9dY.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/e8Oas3Z.png" alt="Image" /></p>
<p>辛苦了 ㄨㄐ ！</p>
<p><img src="https://i.imgur.com/RokTPFS.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/yMdv9F6.png" alt="Image" /></p>
<p>到 0.9 k 了！</p>
<p><img src="https://i.imgur.com/B8vVKsC.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/WE2ZQnv.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/l64Uvpt.png" alt="Image" /></p>
<p>根據山友的資訊回報，步道 0 ~ 1k 大概都是不好走的上坡路，1k ~ 1.6k
是平緩路，而 1.6k ~ 2k 也就是最後一哩路是急上升。我與 Jack
決定以先努力到 1k 後的平緩路為目標，之後就可以慢慢走了。</p>
<p>更多的休息…</p>
<p><img src="https://i.imgur.com/gDCEk6P.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/4osZAvN.png" alt="Image" /></p>
<p>終於到了 1k 的路程牌了！也宣告這趟行程已經走了一半了！</p>
<p><img src="https://i.imgur.com/GVPd7Iw.png" alt="Image" /></p>
<p>只是在這個時候，我看 ㄨㄐ 越來越不舒服的樣子，好像真的沒有辨法撐到 1k
後的平緩路，在與 Jack 的討論下，決定放下登上北峰的目標，在 1k
哩程牌處折返下山。</p>
<p>我說：一趟旅程的重點不是在目的，而是在過程。如果過程不完美，即使達成了目的，日後的回憶不一定也是完美的。</p>
<p>很感謝 Jack
願意相信我一意孤行的決定，在這麼困難的選擇下做出了選擇。</p>
<p>決定要下山的我們…</p>
<p><img src="https://i.imgur.com/PimgE9S.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/gNCjrav.png" alt="Image" /></p>
<hr />
<p>在下山的路上，一路上一直聽到山友們口中的「很好拍的石頭」，這句話讓我們很感興趣，決定要去看看這傳說中的石頭</p>
<p>「這個石頭如果角度拍得好，會有種身在山中峭壁的感覺喔。」by
某位山友</p>
<p>的確…這個石頭天身就散發出一種網美照的
fu，與身後的山、雲渾然天成的感覺真的很像在懸崖邊拍照的感覺…來秀幾張 Jack
的拍照功力 XD</p>
<p><img src="https://i.imgur.com/FEDtXxo.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/q0vyf9Q.png" alt="Image" /></p>
<p>花了半小時終於回到登山口啦！那時大概 4
點多，騎了一整天的車還爬了山，大家也都累到了，只想趕快回到露營地休息吃晚餐
XD</p>
<p><img src="https://i.imgur.com/pWkrxju.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/a1rcC56.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/gm9Qbpc.png" alt="Image" /></p>
<p>結果沒有想到台 14
線路上在進行施工放行管制，害我們多等了快一個小時才回到露營地，等的時候太陽也快要下山了，空氣中的風不再溫馴，開始吹起了冷冽的寒風…</p>
<p>被雲給擋住啦！我需要太陽的溫暖！</p>
<p><img src="https://i.imgur.com/tWDiA8y.png" alt="Image" /></p>
<p>花了九牛二虎之力終於回到了武嶺，在武嶺的停車場上與最後的夕陽合照，接下來就要手刀趕回營地了</p>
<p><img src="https://i.imgur.com/2Yta2NK.png" alt="Image" /></p>
<p>回去的路上太陽如水彩畫一般的在雲層間穿梭，真美！</p>
<p><img src="https://i.imgur.com/jPvb6C8.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/ESorPCO.png" alt="Image" /></p>
<p>在我們手刀趕回露營地下，成功趁太陽還在時回到目的地。這個時候大約晚上
6 點整，此時大家都已經冷到了，而且肚子也超極餓的啦！</p>
<p>我們第二天早上的行程也在轟轟烈烈的趕車中劃下了句點，今天的行程在大家的討論與包容下才能有這麼順利的過程，雖然以結果來看有些不盡人意，但也正是如此也為這趟旅程多了更多難忘的回憶呢！</p>
<p>至於後續的露營狀況嘛…想知道我們晚餐倒底吃什麼嗎？想知道我們晚上倒底怎麼睡的嗎？且待下回分曉。</p>
]]></content>
      <categories>
        <category>遊記</category>
      </categories>
      <tags>
        <tag>合歡山</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: DeepViT: Towards Deeper Vision Transformer - 試著把 Transformer 變深</title>
    <url>/2021/09/07/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-DeepViT-Towards-Deeper-Vision-Transformer-%E8%A9%A6%E8%91%97%E6%8A%8A-Transformer-%E8%AE%8A%E6%B7%B1/</url>
    <content><![CDATA[<p>如果 CNN 可以透過增加網路深度來使效果更好，那 Transformer
呢？此篇作者發現，如果 Transformer 想要仿照 CNN
一樣加深度的話，效果不增反減，作者稱為注意力坍塌 (attention
collapse)，因而提出了 Re-attention 機制，來取代原本的
Self-Attention。</p>
<p><a
href="https://arxiv.org/pdf/2103.11886.pdf">https://arxiv.org/pdf/2103.11886.pdf</a></p>
<p>keywords: attention collapse、Re-attention <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>ViT 的模型根據不同的層數一共有三種
ViT-B、ViT-H、ViT-L，分別對應的層數為 12、24、32。此篇作者發現，如果
Transformer 想要仿照 CNN 一樣加深度的話，神奇的事是 32 層的效果並沒有 24
層來的得好。</p>
<p><img src="https://i.imgur.com/30Rltc0.png" alt="Image" /></p>
<p>於是作者深入研究了一下，發現 Transformer 當層數越深時，同一層內的
Attention Map 特徵會越來越相近。也就是說如果我們只單純的把 Transformer
加深，就會因 Attention Map
之間的相似度越來越近，作者稱作這個現象為：注意力坍塌 (attention
collapse)。</p>
<p><img src="https://i.imgur.com/F6uggiB.png" alt="Image" /></p>
<p>而作者使用計算的公式如下：如果有興趣的話可以自己去看原文，簡單解釋就是計算
<span class="math inline">\(p\)</span>, <span
class="math inline">\(q\)</span> 兩層在 <span
class="math inline">\(h\)</span> (head) <span
class="math inline">\(t\)</span> (token) 下的 cos 相似度。</p>
<p><span class="math display">\[
M_{h,t}^{p,q} =
\frac{(A^p_{h,:,t})^TA^q_{h,:,t}}{||A^p_{h,:,t}||||A^q_{h,:,t}||}
\]</span></p>
<h2 id="網路架構">2. 網路架構</h2>
<p>為了解決注意力坍塌 (attention collapse)，作者提出 Re-Attention
架構，把原本的 Self-Attention 的地方取代掉了。如下圖 (左邊為 ViT，右邊為
DeepViT)：</p>
<p><img src="https://i.imgur.com/BgC7d0v.png" alt="Image" /></p>
<h3 id="re-attention">Re-Attention</h3>
<p>作者進一步發現，雖然 Transformer 越深時層與層之間的 Attention Map
差距很小沒錯，但是同層不同 head
間的差距卻很大。因此作者提出一個想法，如果能將在計算 head 時，把不同
head 的訊息結合起來，再利用它們來產生 Attention Map</p>
<p>所以 Re-attention 是一種使用「可學習」方式，來<strong>整合不同
attention heads 的資訊</strong>，使得生成的 Attention Map
內有更多樣的特徵資訊。</p>
<p>詳細的作法為，在 attention 經過 softmax
後，再經過一個轉置矩陣做一次的 Linear Transformeation <span
class="math inline">\(\Theta\)</span>，公式如下：</p>
<p><span class="math display">\[
\mathrm{Re-Attention}(Q,K,V)=\mathrm{Norm}(\Theta^T(\mathrm{Softmax(\frac{QK^T}{\sqrt{d}})}))V
\]</span></p>
<h2 id="experiments">3. Experiments</h2>
<h3 id="與-vit-attention-map-的比較">與 ViT Attention Map 的比較</h3>
<p>作者把 Re-Attention 與 Self-Attention，兩者皆做一次上述的 cos
相似公式得到下圖：</p>
<p><img src="https://i.imgur.com/jkdEgfS.png" alt="Image" /></p>
<p>可發現 Re-Attention 可以有效的把 Attention Map
相似的層數往後移了不少，但是神奇的是，仍然會在層數約 30
的地方相似度急速上升 (像魔咒一樣…)</p>
<h3 id="與-sota-比較">與 SOTA 比較</h3>
<p>單單的把 Self-Attention 換成 Re-Attention 效果就可以好很多…
(神奇)</p>
<p><img src="https://i.imgur.com/mlKDMP0.png" alt="Image" /></p>
<h3 id="section"></h3>
<h2 id="結論">結論</h2>
<p>這篇論文提出了一個有趣的想法：如果把 Transformer
加深會發生什麼事呢？並且發現以 ViT
來說，不能單單的加深深度，不然會發生注意力坍塌 (attention
collapse)。並用 Re-Attention 來解決它。</p>
<p>我個人認為，這篇文是用實驗的方法來找到這個問題，而非從理論基礎上找到真正的問題，Re-Attention
結合 Attention head
是一個不錯的做法，但這讓我有種治標不治本的感覺，一定還有什麼背後原因使用
Transformer 不能單單加深，不然為什麼這篇論文的實驗結果在層數 30
附近相似度又上升了呢？</p>
<h2 id="reference">Reference</h2>
<p>https://zhuanlan.zhihu.com/p/363370678</p>
<p>https://zhuanlan.zhihu.com/p/359601694</p>
<p>https://zhuanlan.zhihu.com/p/359191305</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>合歡山上看星星 - 合歡山主峰北峰兩天一夜遊 (二)</title>
    <url>/2021/10/04/%E5%90%88%E6%AD%A1%E5%B1%B1%E4%B8%8A%E7%9C%8B%E6%98%9F%E6%98%9F-%E5%90%88%E6%AD%A1%E5%B1%B1%E4%B8%BB%E5%B3%B0%E5%8C%97%E5%B3%B0%E5%85%A9%E5%A4%A9%E4%B8%80%E5%A4%9C%E9%81%8A-%E4%BA%8C/</url>
    <content><![CDATA[<p>前情提要：第一天合歡山之旅我們到了卡爾小鎮放了行李，接著到了合歡北峰登山口爬山，雖然爬山之路跌跌撞撞，但山上超美的風景以及熱情的人們並沒有澆熄我們的興致。</p>
<p>下了北峰後我們趁著太陽還在趕快手刀趕回露營地下，成功趁太陽還在時回到目的地。這個時候大約晚上
6 點整，此時大家都已經冷到了，而且肚子也超極餓的啦！</p>
<p>keywords: 合歡山 <span id="more"></span></p>
<p>成功回到營地的我們三人，立馬馬上開始做起正事來，check in
、搭帳棚、拿瓦斯爐…。然而我們搭帳棚的速度並沒有太陽下山的速度快，在一陣手忙腳亂中，我們已經不知不覺深陷在黑暗之中了。</p>
<p>身陷伸手不見五指的我們開始把準備的頭燈拿出來，繼續在黑暗之中把最後帳棚給撐起來，也要把等等煮飯的器具的材料給準備好…</p>
<p><img src="https://i.imgur.com/2jO81Bh.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/4OCYGSA.png" alt="Image" /></p>
<p>搭完帳棚後我們就馬上到對面的野炊區立馬準備今天的晚餐。</p>
<p>還記得之前有提到我們多花 500
元租一個野炊區，沒錯就是這個地方，一開始我們還為著要省錢不想多支出這筆開銷，單純的想著說：「我們都有帶著頭燈、手機也有手電筒，這樣子在山上應該夠用了吧…」</p>
<p>錯！實際上山上的露營地黑到不能再黑，小小的頭燈完全沒辨法應付煮飯這麼困難的工作…</p>
<p>我們邊在野炊區一邊用租來的流理臺洗著食材，一邊看著頭上一盞超極亮的露營燈，一邊覺得：「這
500 塊花得真值得…」，要是沒有這些東西還真不知道該怎麼煮飯呢
XD。(果然都出門玩了，要花錢什麼的…，都花都花！)</p>
<p>租的野炊區負的長桌</p>
<p><img src="https://i.imgur.com/6CDSzCs.png" alt="Image" /></p>
<p>我們今天的露營餐是海底撈的番茄火鍋包！把火鍋湯包放進鍋子內，再把所有在台中全聯買的食材全部放到裡面，就是這麼簡單！</p>
<p><img src="https://i.imgur.com/NuncolI.png" alt="Image" /></p>
<p>在越來越冷的山上能吃到這樣的火鍋，跟本是天堂等級的美味 XD</p>
<p><img src="https://i.imgur.com/lNuVmrU.png" alt="Image" /></p>
<p><img src="https://imgur.com/5DwMoyo.gif" alt="Imgur" /> <a
href="https://imgur.com/5DwMoyo">動圖連結</a></p>
<p>不知不覺在吃飯的同時，山上有慢慢漂起了霧來，不禁為等一下看星星的旅程擔心。而且更重要的是，我們的帳棚全被霧水弄得超溼的，完了…我開始覺得我們今天晚上的睡覺一定不得安眠…</p>
<p>在大家吃到九點休息一下後，我們開始全覆武裝，把所有帶上山的厚外套全穿到身上，準備今天最後一個行程：合歡山上看星星！</p>
<p>這一次我們看星星的地點選在離我們比較近的鳶峰暗空公園，沒有選擇上武嶺看星星是因為真的太遠了啦！而且考量到晚上騎山路不安全，而且要常常跟菜車打架…</p>
<p>時隔一年又可以再度上山拍星星啦！兩次上山的心情不太一樣呢</p>
<p>猜猜看這是什麼星座呢？</p>
<p><img src="https://i.imgur.com/ltrhfI8.png" alt="Image" /></p>
<p>是獵戶座喔！</p>
<p><img src="https://i.imgur.com/Ny0Qw7G.png" alt="Image" /></p>
<p>這一趟上山看星星主要去看黃道十二宮的星座，現在是 10
月份…所以可以看到摩羯、寶瓶、雙魚、白羊、金牛…</p>
<p>花了一整個下午把天空中的星星是線連起來，全部放上網站上面給大家欣賞一下
XD</p>
<p><img src="https://i.imgur.com/UBZoilf.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/iS6Cn7j.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/D258yT4.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/e6R9BTA.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/6QoAoCO.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/9ZNkiCA.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/yQh0GfG.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/zwkG0t3.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/5wFFGs3.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/0ofUrQf.png" alt="Image" /></p>
<p>看完不知不覺就 11
點多了，大家也覺得越來越冷，是時候該回營地洗洗睡啦。</p>
<p>回到卡爾小鎮發現其它來露營的人們也都躲進帳棚裡睡覺去了，於是我們也速速的洗了一下澡，躥進又潮溼又冷的帳棚裡睡覺，準備明天一早的行程。</p>
<hr />
<p>早上四點半的鬧鐘劃破山中寧靜的空氣，響得特別得大聲。會起這麼早的原因也只有一個，就是我們要去合歡主峰上看日出啦！繼晚上看星星後的另一個瘋狂行程
XD。</p>
<p>拖著超極沈重眼皮以及睡在地板上腰痠背痛的身體，還是不得不在寒冷的清晨打理打理，整理要上山看日出的行李…</p>
<p>到了主峰登山口，天已經微微亮起來了，於是我們趕緊加快腳步上山</p>
<p><img src="https://i.imgur.com/DNgkMRC.png" alt="Image" /></p>
<p>大家看起來都超累的樣子 XD</p>
<p><img src="https://i.imgur.com/Fhy4yoF.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/Zhiya96.png" alt="Image" /></p>
<p>慢慢看到太陽要突破地平線的雲海了！但是我們還沒走到山頂阿！</p>
<p><img src="https://i.imgur.com/2oTEc9Q.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/DsUmNIN.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/EkLCdcV.png" alt="Image" /></p>
<p>出來啦！好亮！結果我們在半山腰的地方看日出呢 XD</p>
<p><img src="https://i.imgur.com/gorZkuQ.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/oLJDiy1.png" alt="Image" /></p>
<p>看到這樣的美景，早上的睏意瞬間消失，雖然還是很累
XD，但不經一番寒徹骨，焉得梅花撲鼻香，是吧！</p>
<p>從太陽突破地平線後，整個山谷瞬間明亮了起來，彷彿告訴著我們新的一天正式開始。看完日出的我們接下來繼續往山頂前進，以登主峰頂為目標！</p>
<p><img src="https://i.imgur.com/MppNw8V.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/07TBo0p.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/IByokau.png" alt="Image" /></p>
<p>太陽完全出來了阿！山上越來越熱了呢…</p>
<p><img src="https://i.imgur.com/FMrwbfx.png" alt="Image" /></p>
<p>經過了 30
分鐘後，終於到達山頂啦！從山頂往山下看，早晨的山林又是不一樣的景色呢 ！
<img src="https://i.imgur.com/TrDPYmJ.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/pyjhdy7.png" alt="Image" /></p>
<p>合歡主峰海拔標高 3417
公尺，是台灣百岳的其中之一，也就是說這是我人生繼合歡東峰、石門山的人生第三座百岳呢！對
ㄨㄐ 而言則是人生第一座百岳呢！恭禧解鎖人生新成就哈哈</p>
<p><img src="https://i.imgur.com/cK7jUh7.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/vZEHRZD.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/oCpaJx4.png" alt="Image" /></p>
<p>來個三人合照</p>
<p><img src="https://i.imgur.com/Ad79Gjg.png" alt="Image" /></p>
<p>從主峰遙望北峰的反射板，…下次一定要攻頂！</p>
<p><img src="https://i.imgur.com/pTdof7n.png" alt="Image" /></p>
<p>今天的早餐就在合歡主峰上度過啦！是在山下就先準備好的罐頭花生湯，沒有什麼事是比剛攻頂喝花生湯還要更爽的啦！</p>
<p><img src="https://i.imgur.com/P4eiWzV.png" alt="Image" /></p>
<p>在山上待了一下，時間也慢慢到了早上 8 點，該下山 check out
啦！。慢慢的往山下移動…</p>
<p><img src="https://i.imgur.com/vee3ljK.png" alt="Image" /></p>
<p>看向主峰登山口</p>
<p><img src="https://i.imgur.com/Wi2gckS.png" alt="Image" /></p>
<p>時間來到早上 9 點，昨天晚上跟本沒什麼睡…來一罐 RedBull 來提神
XD，等等就要收帳棚下山去啦！</p>
<p><img src="https://i.imgur.com/qcRipnY.png" alt="Image" /></p>
<p>東西都收完啦，準備要離開合歡山啦</p>
<p><img src="https://i.imgur.com/32PRgS1.png" alt="Image" /></p>
<p>在路過仁愛鄉農會時，順便繞進去買了三罐暗空公園合作的精釀啤酒，當做一路上的紀念品，包裝非常的精美呢。賣給我們的櫃台姐姐也很熱情的跟我們分享農會改建的故事呢！</p>
<p><img src="https://i.imgur.com/68Y4TfL.png" alt="Image" /></p>
<p>後續下山到埔里還車後就沒有再多拍照紀念了，因為我們大家全都累到了
XD，沒有一個人能在又冷又溼又硬的帳棚內睡著哈哈。不管是坐回台中的客運或是坐回桃園的火車全部都睡爆！合歡山兩天一夜遊就在超累的回家中劃下了完美的句點。</p>
<p>這一次非常感謝 Jack
一路的規劃，從行程規劃到住宿到拍照擔當，全都一個人負責完成。仔細想想這一切的一切全都來自我無意的一句話，就覺得人生中最快樂最驚奇的時間就是這麼不按牌理出牌，一切全都是驚喜。</p>
<p>雖然回到家中馬上暴睡，而且隔天早上起床還發現我的鼻子曬傷了…
(痛爆)，但這次的合歡山行程收獲滿滿，從北峰到主峰、從看星星到看日出。雖然行程滿到炸出來沒什麼休息的時候，但一路上有歡笑有涙水，也解鎖了不少人生的新成就。</p>
<p>期待再未來的某一天還能再度上山，把北峰的缺給填上，來個北峰西峰大縱走，或是能拍到冬天的星座也不錯哈哈！</p>
<p>合歡山上看星星 - 合歡山主峰北峰兩天一夜遊 (完)</p>
]]></content>
      <categories>
        <category>遊記</category>
      </categories>
      <tags>
        <tag>合歡山</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows - 打破各項 SOTA 的新網路</title>
    <url>/2021/10/07/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Swin-Transformer-Hierarchical-Vision-Transformer-using-Shifted-Windows-%E6%89%93%E7%A0%B4%E5%90%84%E9%A0%85-SOTA-%E7%9A%84%E6%96%B0%E7%B6%B2%E8%B7%AF/</url>
    <content><![CDATA[<p>微軟提出 <strong>S</strong>hifted <strong>Win</strong>dows，簡稱 Swin
Transformer，目的是要解決 Transformer
在處理文本與處理影像差異的問題。然而效果卻出奇的好，甚至達到各項領域的
SOTA，在未來的幾篇論文介紹中也會繼續以 Swin 做為出發點。</p>
<p><a
href="https://arxiv.org/pdf/2103.14030.pdf">https://arxiv.org/pdf/2103.14030.pdf</a></p>
<p>keywords: Swin Transformer、Shifted Windows <span id="more"></span></p>
<h2 id="abstract">1. Abstract</h2>
<p>作者提出了 <strong>S</strong>hifted <strong>win</strong>dows 網路架構
(<strong>Swin</strong> Transformer)，通過這個「移動視窗」架構可以使原本
Transformer 的架構有以下兩個優點：</p>
<ol type="1">
<li>可以偵對不同圖片的大小 (Scale) 處理</li>
<li>在時間複雜度上不受圖片大小而成平方關系 <span
class="math inline">\(O(n^2)\)</span>，而是成線性關系 <span
class="math inline">\(O(n)\)</span></li>
</ol>
<p>在各種電腦視覺領域上，Swin Transformer 也都刷新了各項
SOTA，其中尤其以 semantic segmentation (ADE20K) 最為顯著</p>
<h2 id="introduction">2. Introduction</h2>
<h3 id="對偵對不同圖片的大小-scale-解釋">對偵對不同圖片的大小 (Scale)
解釋</h3>
<p>作者認為 CNN 與 Transformer
兩個架構最大的不同在於「Scale」，也就是訓練資料的大小。在 NLP
中一個「patch」就非常固定得為一個字詞 (ex：I am student -&gt; 分為 3 個
patch)，而在最一開始的 Transformer 論文中，像是
ViT，所選擇的做法也同像為固定 patch，把圖片固定切成 <span
class="math inline">\(H/4 \cdot W/4\)</span> 個 patch，而每個 patch
的大小不會隨著網路深度而改變，全部皆為 16x16。</p>
<h3 id="時間複雜度上的解釋">時間複雜度上的解釋</h3>
<p>與 NLP 相比，在影像上尤其是高解析度影像，如果單純使用 Transformer
計算量會與圖片大小呈平方關系。而且如果是要全高解釋度的語意分割領域，使用
Transformer 的效果一定不好。</p>
<h3 id="overall-in-swint">Overall in SwinT</h3>
<p>於是作者提出了 Swin Transformer，改進了以上兩點：</p>
<ol type="1">
<li><p>Patch size 會隨著網路越深，由小慢慢放大，而這一步的用意是為了模仿
CNN 的全局視野會隨著網路越深視野越大 (在 CNN 中 kernel size
不變，但圖片大小會變、在 Transformer 中 patch size
會變，但圖片大小不變)。同時也因為這樣加入了多重解析度，可以應付更多電腦視覺的領域
(其中以分割效果最好)</p></li>
<li><p>會把圖片分割成 non-overlapping windows (不會重疊的視窗)，只單純在
window 裡面做 self-attention，而非在整張圖片中做。因為一個 window
中所包含的 patch number
遠遠的小於圖片的大小，所以時間複雜度可以降到與圖片大小呈線性關系
(在文章後續會細講)</p></li>
</ol>
<p><img src="https://i.imgur.com/AZdaM1E.png" alt="Image" /></p>
<p>上圖灰色小格為一個 patch，紅色格子為一個 window。每個 window
中包含固定數量的 patch，且 self-attention 只會在一個 window
中做計算。同時也可發現 Swin Transformer 的 patch 與 window
大小會隨著網路深度而變大，而且也有多重解析度的觀念在裡面。</p>
<h3 id="shifted-window">shifted window</h3>
<p>Swin Transformer 最核心的觀念就是 shitfed window。為了使 window 和
window 之間也能學到彼此相關性，每做完一次 self-attention 後，window
會往斜角的方向移動。</p>
<p><img src="https://i.imgur.com/cYvXzn0.png" alt="Image" /></p>
<h2 id="網路架構">3. 網路架構</h2>
<p>先上完整架構圖，接下來慢慢由左至右一塊塊介紹</p>
<p><img src="https://i.imgur.com/aCJj2hV.png" alt="Image" /></p>
<h3 id="patch-partition">Patch Partition</h3>
<p>與 ViT 一樣，會先經過一個 Patch embadding (SwinT 稱這一步為 Patch
Partition) 的步驟，把三維 <span class="math inline">\(H\times W \times
C\)</span> 的圖片表示成二維序列 <span class="math inline">\(N \times
(P^2 \times C)\)</span></p>
<p><img src="https://i.imgur.com/mG3JoYk.png"
alt="image-20210710135026339" /></p>
<p>在 SwinT 中，<span class="math inline">\(P\)</span> 預設是
4，輸入圖片大小為 <span class="math inline">\(H\times W\times
3\)</span>，所以網路的輸入維度是</p>
<p><span class="math display">\[
\begin{gathered}
\frac{HW}{4^2} \times (4^2 \times 3) \\
= \frac{H}{4}\times \frac{W}{4}\times 48
\end{gathered}
\]</span></p>
<h3 id="stage-1">stage 1</h3>
<h4 id="linear-embedding">Linear Embedding</h4>
<p>在 stage 1 中會先經過一層 Linear Projection (SwinT 中稱 Linear
Embedding)，簡單說就是 1x1 conv，把 48 維轉換成 C 維 (C
會依照網路設計的大小而改變)</p>
<h4 id="swin-transformer-block">Swin Transformer Block</h4>
<p>接著會經過 Swin Transformer Block</p>
<p><img src="https://i.imgur.com/kdOG9Ve.png" alt="Image" /></p>
<p>由上圖可以發現 SwinT 與 ViT 最大的差別就在於，把 ViT 中的 MSA 改成
W-MSA (Window-based MSA) 與 SW-MSA (Shifted Window-based MSA)</p>
<p>其餘部份與 ViT 大部份相同，有一不同的地方在 MLP 的 activation
function 從 ReLU 改為 GELU (嗯…可能受到了 BERT 的啟發吧…)</p>
<h4 id="window-based-msa">Window-based MSA</h4>
<p>主要設計是為了解決原 self-attention 計算複雜度為 <span
class="math inline">\(O(N^2)\)</span> 的問題</p>
<p>以下簡單介紹原 self-attention 計算量之算法</p>
<p>計算出 <span class="math inline">\(QKV\)</span> 的公式：<span
class="math inline">\(x\times W^Q\)</span>、<span
class="math inline">\(x\times W^K\)</span>、<span
class="math inline">\(x\times W^V\)</span> 一個需要 <span
class="math inline">\(hwC^2\)</span>，三個就為 <span
class="math inline">\(3hwC^2\)</span></p>
<p>計算 <span class="math inline">\(QK^T\)</span> 需要 <span
class="math inline">\((hw)^2C\)</span></p>
<p>再計算乘以 <span class="math inline">\(V\)</span> 完整公式：<span
class="math inline">\((QK^T)V\)</span> 也需要 <span
class="math inline">\((hw)^2C\)</span></p>
<p>最後得到的 Multi-Head 還要再乘上一個 <span
class="math inline">\(W^Z\)</span> 需要 <span
class="math inline">\(hwC^2\)</span></p>
<p>所以總得來說原版 MSA 的計算量為</p>
<p><span class="math display">\[
\Omega(MSA) = 4hwC^2 + 2(hw)^2C
\]</span></p>
<p>在 SwinT 中 self-attention 只會在一個 window 中做</p>
<p>所以 <span class="math inline">\(QK^T\)</span> 變成 <span
class="math inline">\(\frac{h}{M}\frac{w}{M}\)</span> 再乘上 <span
class="math inline">\((M^2)^2C\)</span> ，得到</p>
<p><span class="math display">\[
\Omega(W-MSA) = 4hwC^2 + 2M^2hwC
\]</span></p>
<p>而一個 window 所含的 patch size
遠小於圖片大小，所以計算量就可以與圖片大小呈線性的關系了</p>
<h4 id="shifted-window-based-msa">Shifted Window-based MSA</h4>
<p><img src="https://i.imgur.com/cYvXzn0.png" alt="Image" /></p>
<p>先前有提到為了使不同 window 間也能有關系，所以會把 window
往斜上方移動，但移動後會產生幾個問題：</p>
<ol type="1">
<li>window 的數量變多了</li>
<li>每個 window 的大小還不一樣</li>
</ol>
<p>因此我們沒辨法直接對移動過的 window 做self-attention。</p>
<h5 id="cyclic-shift">cyclic shift</h5>
<p>而作者提出了 cyclic shift
來解決這個問題，把因位移而多出來的右上角，把它用搬的方法搬到了左下角，使得一張圖片中的
window 數量維持一致，如下圖</p>
<p><img src="https://i.imgur.com/Sx5Q2ya.png" alt="Image" /></p>
<p>參考了知乎大神上更詳細的圖片，從左邊移成右邊</p>
<p><img src="https://i.imgur.com/zb57ex9.png" alt="Image" /></p>
<h5 id="masked-msa">masked MSA</h5>
<p>但這又沿生出另一個問題，一個 window
內有來自不同地方的區塊阿，像是上圖的右上角，一個 window 裡同時包含了 6
和 4，如果直接做 self-attention 會…非常的不合理…，於是作者又提出了
masked MSA，通過適當的遮罩使得來自不同區塊不會互相運算到</p>
<p><img src="https://i.imgur.com/kYiySYr.png" alt="Image" /></p>
<p>再次參考知乎大神的詳解，舉個例子來說明會更清楚。</p>
<p><img src="https://i.imgur.com/3AXzEGu.png" alt="Image" /></p>
<p>我們再次以右上角為例子，這個 window 內同時有 6 和 4。要怎麼設計 mask
使得計算 attention 不會發生交疊呢？</p>
<p>答案如下圖：</p>
<p><img src="https://i.imgur.com/OiKCryC.png" alt="Image" /></p>
<p>以此類推右下角的例子，有 4 個同時存在呢？</p>
<p><img src="https://i.imgur.com/fhJUFja.png" alt="Image" /></p>
<p>答案如下：</p>
<p><img src="https://i.imgur.com/jn2x2Um.png" alt="Image" /></p>
<p>按照上面的邏輯可以推出所有 window 內的狀況所對應的 mask 設計。透過 0
1 的 mask 設計，可以使不同的區塊不會相互計算 attention 而影響。</p>
<p>最後附上完整流程圖</p>
<p><img src="https://i.imgur.com/HQLqm3c.png" alt="Image" /></p>
<h3 id="stage-2-4">stage 2 ~ 4</h3>
<h4 id="patch-merging">Patch Merging</h4>
<p>接著到了新的 stage，在 stage 2 ~ 4 中都做著重複的動作，首先會經過一個
Patch Merging，把剛剛 stage 1 所產生的 <span
class="math inline">\(\frac{H}{4}\times \frac{W}{4}\times
C\)</span>，以每一個 patch 階與相鄰的其它 2x2 patch concat
起來，得到新的 <span class="math inline">\(\frac{H}{8}\times
\frac{W}{8}\times 4C\)</span> ，如下圖</p>
<p><img src="https://i.imgur.com/VTqWFKm.png" alt="Image" /></p>
<p>再經過一層線性轉換，把 4C 變為 2C，得到 <span
class="math inline">\(\frac{H}{8}\times \frac{W}{8}\times
2C\)</span>，就是 stage 2 的輸入了，如下圖</p>
<p><img src="https://i.imgur.com/SmBBECI.png" alt="Image" /></p>
<p>會做 Patch Merging 的理由是模仿 U-Net 或是一般 CNN
中的多重解析度，藉由不斷的合併相鄰 patch 使得越深的網路，patch
的視野越大。</p>
<p>最後再做跟 stage 1 一樣的 Swin Transformer
Block，就是完整的網路了。</p>
<h3 id="relative-position-bias">Relative position bias</h3>
<p>Swin Transformer 在設計 Self-Attention 時，參考了 ViT
的設計之外，還額外加入了一個 Bias <span
class="math inline">\(B\)</span>，使得式變為以下：</p>
<p><span class="math display">\[
\mathrm{Attention}(Q,K,V) = \mathrm{SoftMax}(\frac{QK^T}{\sqrt{d}}+B)V
\]</span></p>
<p>作者稱這個 Bias 為 relative position
bias，為網路填加了額外的值置資訊</p>
<p>而這個 <span class="math inline">\(B\)</span>
不是隨機生成出來的，而是透過一系列算法生出來的，詳細可見以下知乎大神的
source code 解釋：</p>
<p><a
href="https://blog.csdn.net/weixin_42364196/article/details/119954379">https://blog.csdn.net/weixin_42364196/article/details/119954379</a></p>
<p>後面作者有用實驗比較一些 position 加入的方法，包含 absolute
position、reletive position</p>
<p>發現： * 有 shifted window 比沒有 shifted 效果來得好 * 加了 absolute
position 效果不是最好的 * 使用 reletive position bias 效果是最好的</p>
<p>(下圖中的 w/o app. 指的是第一個 Attention 公式沒有 Attenion，只有
B)</p>
<p><img src="https://i.imgur.com/jmUlfwe.png" alt="Image" /></p>
<h2 id="experiments">4. Experiments</h2>
<h3 id="分類-imagenet-上的實驗">分類 ImageNet 上的實驗</h3>
<p><img src="https://i.imgur.com/Vz82yqF.png" alt="Image" /></p>
<h3 id="偵測-coco-上的實驗">偵測 COCO 上的實驗</h3>
<p><img src="https://i.imgur.com/cm1G7Bj.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/zWJO8Dd.png" alt="Image" /></p>
<h3 id="語意分割-ade20k-上的實驗">語意分割 ADE20K 上的實驗</h3>
<p><img src="https://i.imgur.com/YOaTVpZ.png" alt="Image" /></p>
<p>可看到不管在哪一個領域上 SwinT 皆打敗了所有目前的 Transformer
網路，而與傳統 CNN 網路相比 (EfficientNet) 效果我認為是不相上下</p>
<p>值得注意的是 SwinT 在語義分割的題目表現特別好，好超過 3 個
ticks，連在 SOTA 的網站中都可以看到明顯的差距</p>
<p><img src="https://i.imgur.com/AkLOwMM.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>我自己讀完乍看之下，好像與直接前幾篇論文的 proposal
差不多，不外乎就是模仿 CNN
或是減少運算量。但如果真的照著論文給的實驗結果，SwinT
的效果也好太多，真的有點神奇。</p>
<p>如果現在在 Google 上搜尋 SwinT
也常常會找到什麼屠榜之類的標題，我自己是覺得有點過頭了啦，論文實驗歸實驗，真正要在現實實作上發揮功能才是最重要的部份。(例如測資多寡的問題)</p>
<p>不過這個論文倒是開了一個「Swin」風潮，希望再過個半年到一年，能把「Swin」的觀念再發揮的成熟一些，讓大家知道
Transformer 的厲害哈哈</p>
<h2 id="reference">Reference</h2>
<p>https://www.youtube.com/watch?v=SndHALawoag</p>
<p>https://zhuanlan.zhihu.com/p/360513527</p>
<p>https://zhuanlan.zhihu.com/p/404001918</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Rethink：重新思考 Transformer 倒底學到了什麼東西？倒底與 CNN 差在哪裡？</title>
    <url>/2021/11/11/Rethink%EF%BC%9A%E9%87%8D%E6%96%B0%E6%80%9D%E8%80%83-Transformer-%E5%80%92%E5%BA%95%E5%AD%B8%E5%88%B0%E4%BA%86%E4%BB%80%E9%BA%BC%E6%9D%B1%E8%A5%BF%EF%BC%9F%E5%80%92%E5%BA%95%E8%88%87-CNN-%E5%B7%AE%E5%9C%A8%E5%93%AA%E8%A3%A1%EF%BC%9F/</url>
    <content><![CDATA[<p>前面看了這麼多不同的 Transformer
網路架構，不仿現在稍微停下腳步，回頭看看一些最基本的概念及問題：<strong>倒底
Transformer 比 CNN 好在哪裡？</strong>。究竟是什麼原因使得現在
Transformer 可以在各大題目上刷新 SOTA，而究竟 Transformer
創新的地方在哪裡？</p>
<p>keywords: Transformer、CNN <span id="more"></span></p>
<h2 id="回過頭來看看-vit">回過頭來看看 ViT</h2>
<p><img src="https://i.imgur.com/abtdhSj.png" alt="Image" /></p>
<p>總的來說 ViT 的架構流程可以用以下方式來表達： 1. 把 <span
class="math inline">\(x\in H\cdot W\cdot C\)</span> 圖片依照 Patch
大小切塊成 <span class="math inline">\(N\cdot (P\cdot P \cdot
C)\)</span> 的三維 Patch 2. 再把三維 Patch reshape 成二維向量 <span
class="math inline">\(N\cdot (P^2 \cdot C)\)</span> 3. 再把 <span
class="math inline">\(x_p\in N \cdot(P^2 \cdot C)\)</span> 經可學習
Linear <span class="math inline">\(E\)</span> 後得到 <span
class="math inline">\(N\cdot D\)</span>，這一步叫 Patch Embedding 4.
再加上 Positional Embedding 5. 再加上 Class token 6. 經過好幾層
Transformer 7. 輸出分類結果</p>
<h3 id="self-attention-layer-代表著什麼">Self-Attention Layer
代表著什麼？</h3>
<p>首先我們來回顧始組 CNN</p>
<p>在 CNN 中我們都知道會有 kernel 去一層層的提取特徵，同時我們也會加入
pooling 或是 stride 去縮小解析度，提取出來的 feature map
會一層層大縮小解析度並增加特徵。</p>
<p>這一步的用意除了減少運算量，還可以使 CNN 能有更大的感知域 (receptive
field)，如下圖所示。使得 CNN 除了在一個 kernel size
中有很強的關聯性外，也能夠與 kernel size 外的點，也有關聯性</p>
<p><img src="https://i.imgur.com/g1LIhgr.png" alt="Image" /></p>
<p>同時因為卷積的特性使 CNN 還有 Locality 與 Spatial Invariance
兩種特性，也就是局部注意力與空間不變性。因為只會加強關注 kernel
內的訊息而有 Locality 特性、因為 kernel size
會平移，使得同樣特徵不管出現在圖片的任何地方都可以偵測出，而有了 Spatial
Invariance 的特性</p>
<p>以上這些特性又可合併成為 Inductive Bias
(歸納偏置)，意思為綜合以上特性，使得 CNN
在尋找圖片中的特徵有非常強的歸納能力
(Inductive)，同時面對沒看過的資料也能作出正確的推論 (Bias)</p>
<hr />
<p>接下來來看看 Transformer</p>
<p>Transformer 與 CNN 最大的不同在於 1. 沒有卷積，而改用 Self-Attention
2. 圖片解析度不會隨層數縮小</p>
<p>Self-Attention 的核心公式如下所示：</p>
<p><span class="math display">\[
A=softmax(\frac{QK^T}{\sqrt{d}})V
\]</span></p>
<p><img src="https://i.imgur.com/Bmql5qV.png" alt="Image" /></p>
<p>如果以瀏覽器為例子的話：Q 代表 Query，可代表使用者下的關鍵字，K 代表
Key，可代表瀏覽器後台的 Database，V 代表 Value，可代表結果的權重值。</p>
<p>因此 Self-Attention，可看成使用者下關鍵字
Q，瀏覽器會去資料庫中與每個資料 K
做相關性運算，得出最有相關的資料後，再乘上 V
權重值，使結果想搜尋列一樣越有關的排在越前面。</p>
<p>如果換成影像也是同理，Self-Attention 會去計算 每個 Patch 與 Patch
中的每個像素與像素之間的關系，最後得出的值再乘上權重。最後得到一個特徵圖
(feature maps)</p>
<p><img src="https://i.imgur.com/vAzIUSO.png" alt="Image" /></p>
<p>因為 Self-Attention 沒有使用 kernel，因此它的 receptive field
是整張圖片。同時也因沒有使用 kernel 因而失去了 Locality 與 Spatial
Invariance 兩種特性，進而導致 Transformer 沒有 Inductive Bias
的能力。</p>
<p>沒有 Inductive Bias 的重點是沒有了「Bias
(偏置)」的能力，什麼意思呢？意思就是少了遇到沒見過的資料的選擇能力。在相同資料訓練的前提下
CNN 看到沒見過的資料有更強的推論能力，而 Transformer 無法做到這一點</p>
<p>為了彌補 Transformer
這個缺點，需要使用大量的資料去訓練它，使它看過更多的資料，直接把可能會遇到沒看過的圖片的可能性去掉，就是再也不需要
Bias(推論) 這項能力。</p>
<hr />
<p>最後的結論可整理為下列表格：</p>
<p>CNN 的優點： * Locality * Spatial Invariance * Inductive Bias</p>
<p>Transformer 的優點： * Global Receptive Field</p>
<h3 id="self-attention-學到了什麼">Self-Attention 學到了什麼？</h3>
<p>由上面結論可以看出，Transformer 少了一些 CNN 的優點，同時多了一個
Global Receptive Field，接下來來仔細看看到底 Transformer
的特徵圖中學到了什麼？</p>
<p>根據 ViT 論文後面實驗的部份，作者有把 Patch Embedding 中可學習的向量
<span class="math inline">\(E\)</span> 拿來做可視化，如下圖：</p>
<p><img src="https://i.imgur.com/PHXp4bp.png" alt="Image" /></p>
<p>可以發現經過一層 MLP 後，裡面學習到的東西與 CNN
非常的類似，都是一個像特徵域的東西。</p>
<p>以下是我的想法：MLP 是 Transformer 最主要提取特徵的主要來源</p>
<p><img src="https://i.imgur.com/DSnccCE.png" alt="Image" /></p>
<p>不管是 Patch Embedding 中的 MLP 或是 Encoder 中的
MLP，它們都含有類似像 CNN 一樣找特徵的能力，找到特徵後再送至
Self-Attention 做特徵的強化，隨著層數的增加，在 MLP 與 Self-Attention
的共同努下找到的特徵就越來越清晰了</p>
<p>其實在 CV 最一開始發展的時候，那時候還沒有 CNN，大家都是使用 MLP
(多層感知機) (或是稱全連接層 FFN)，但是因為 MLP
的運算量太大，因而才有了後續 CNN 的誕生。而且在 CNN
發展的過程中，最後用來輸出分類結果的全連接層也慢慢被淘汰掉了</p>
<p>雖然 MLP 計算量大但是它更
General、網路更有彈性，能計算到的特徵數更多。而 CNN 算是 MLP
的一個特例，透過 kernel
的限制使得網路計算量少且更好訓練，但也因為限制範圍而使 CNN
的效果有一定的上限。</p>
<p>但隨著科技的進步、計算力的增加，這些計算量比較大的方法有慢慢回歸的趨勢，大家慢慢使用更多的資料集去訓練，更多的計算力去計算。</p>
<hr />
<p>接著來看看 Global Receptive Field 的部份</p>
<p>同樣是根據 ViT 的論文作者分析了在 self-attention layer 各層中各個
attention head 之間的關系，以 Mean attention distance 作為分析目標</p>
<p>Mean attention distance 的意思指的是，一個 pixel 能最遠與附近的其它
pixel 做相關性運算，也可以理解為就是 CNN 中的 receptive field
(空間感知域)</p>
<p>依據實驗結果可看到在網路第一層，假設網路中有 16 個 head，這 16 個
head 它們的 receptive field 有的大有的小，有些 head 天生就可以有比較
Global 的感知域，而有些則是比較 Local 的感知域。</p>
<p>隨著層數的增加，每個 head 的 receptive field
也隨之增加，意謂著層數越深越能看到更全局 Global 的資訊</p>
<p>與 CNN 不一樣的是，CNN
在一開始並不會出現全局的感知域，而是像底下藍線一樣，隨著層數而呈線性關性，但
Transformer 能做到的是紅色圈圈部份，這些早期全局資訊是 CNN
所沒有的。</p>
<p><img src="https://i.imgur.com/spEkO2q.png" alt="Image" /></p>
<hr />
<p>參考 Google 在 2021 年 8 月發表的論文</p>
<p><a
href="https://arxiv.org/pdf/2108.08810.pdf">https://arxiv.org/pdf/2108.08810.pdf</a></p>
<p>論文裡面使用了實驗數據來比較 Transformer 與 CNN 的差別</p>
<p>下圖也是一個 Transformer Global Receptive Field
很好的例子，橫縱軸分別代表為網路兩兩層之間相互的關系，座標值越大代表層數越深，可以看到
Transformer 在每個層上兩兩都有關系，但 ResNet50
最淺的層與最深的層兩層的關系非常弱，由圖可知 ResNet50 暗色的部份正是
Transformer 所彌補的強項</p>
<p><img src="https://i.imgur.com/BlozGy1.png" alt="Image" /></p>
<p>下圖則是 Receptive Field 的比較，可看到Transformer
則是在第六層就有全局的資訊出來了，而 CNN 要等到 16
層才慢慢有全局的資訊出來</p>
<p>不過也可以在這張圖中發現，CNN
的局部關注力真的很強，非常大且非常黑</p>
<p><img src="https://i.imgur.com/zhNvCxd.png" alt="Image" /></p>
<hr />
<p>結論：</p>
<p>Transformer 所謂的 Global Receptive Field
指的是：「在網路淺層時期就能有全局的概念」，並非指 CNN
就沒有全局的概念，而是 CNN 需要到網路非常深的時候才能顯現出這項特點</p>
<h3 id="為什麼一定要-reshape">為什麼一定要 Reshape？</h3>
<p>也許你也注意到了，Transformer 的輸入是二維的，在輸入到 Transformer
前會做分 Patch 以及 reshape 這兩步，那不禁讓人懷疑</p>
<ol type="1">
<li>直接把圖片分塊，塊與塊之間彼此互不相關，合理嗎？</li>
<li>直接把三維圖片 reshape 成二維，在數學的角度上意義為？</li>
</ol>
<p>在目前為止我認為這部份的解答只有一個</p>
<p><strong>大家就只是很直覺把 Transformer 好奇的拿到 CV
上來試試看效果，沒想到效果竟然還不錯</strong></p>
<p>所以最一開始的 ViT，基本上都是從 NLP
的觀點來出發的，網路輸入要二維？那我就直接 reshape
你；網路輸入是字詞？那我就直接把圖片切塊</p>
<p>以上這些操作目前我認為沒有任何實際上的意義，但是！實驗證明
Transformer
就是有它厲害的地方！它好是一定有道理的，我們不仿試著去想想看這些操的合理性，以及想一下文字與圖片倒底差在哪裡？</p>
<p>在以下的影片中有提到觀念：</p>
<p><a
href="https://www.youtube.com/watch?v=aH7s6qXEUcc">https://www.youtube.com/watch?v=aH7s6qXEUcc</a></p>
<p>在 NLP 中一串句字中的每一個字都有相對應的編碼，可能是 one-hot
encoding 也可能是 token based
encoding，總之句子中的每個字都有對應的編碼用來表示特徵向量</p>
<p><img src="https://i.imgur.com/l3huXd8.png" alt="Image" /></p>
<p>而在圖片中資訊是用長寬來表示的，但是如果我們換個角度想：想想電腦是怎麼理解一個二維矩陣的？是從左上角為原點一列一列的往右掃，直到最右下角的點。</p>
<p><img src="https://i.imgur.com/6A0TNnn.png" alt="Image" /></p>
<p>這時我們再以每一列拿出來排成一排，像是把圖片中的一列當成是句字中的一個詞，圖片中的行代表句字的長度，我們就可以得到用文字的方式來表達的圖片了。如下圖所示：</p>
<p><img src="https://i.imgur.com/x8PLYi7.png" alt="Image" /></p>
<p>可以想像一列就是一個詞，每一列都會送到 Transformer 中與其它的每一列做
Self-Attention
相關性運算。到目前為止皆與文字相同，但是圖片有一個最大不同的點</p>
<p>除了每一列之間要計算相關性外，列之間的每一個像素也應該要計算相關性，因為文字中的向量表示是這個「詞」的特徵，但是在圖片中一個列的向量的函意不只是一個特徵，更是其中的一個個像素。</p>
<p>而這正是 Transformer 擅長與其它人做相關性計算 (全局
Global)，而比較不擅長與自己內部做相關性計算 (局部 Local)
的另一個方面的解釋</p>
<h3 id="transformer-有什麼缺點">Transformer 有什麼缺點？</h3>
<p>綜觀以上與 CNN 之間的比較，我們接下來來看看除了與 CNN
之間的差異，Transformer 本身有什麼樣的問題？</p>
<p><a
href="https://bbs.huaweicloud.com/blogs/298123">https://bbs.huaweicloud.com/blogs/298123</a></p>
<p>參考以上文章 Transformer 的缺點可分為以下五大類：</p>
<ol type="1">
<li>資料需求量大</li>
<li>計算量大</li>
<li>堆疊的層數的限制</li>
<li>模型本身無位置編碼</li>
<li>局部注意力較弱</li>
</ol>
<h4 id="資料需求量大">1. 資料需求量大</h4>
<p>這點在上面有仔細分析過了，Transformer 一是缺少了 Inductive Bias
需要更多資料去彌補。而造成沒有 Inductive Bias 的主要原因是不使用 CNN
而使用 MLP 做為特徵提取器，MLP 本身更 General
也因此需要使用到更大的資料集去訓練</p>
<h4 id="計算量大">2. 計算量大</h4>
<p>假設 Batch 為一，圖片經 CNN 後的維度會變成 <span
class="math inline">\((H \cdot W \cdot C)\)</span> 的特徵向量，後經
reshape 變成 <span class="math inline">\((HW \cdot C)\)</span> 再加上
Positional Enbedding 後放進 Transformer。其中 <span
class="math inline">\((HW \cdot C)\)</span> 可看成長度為 <span
class="math inline">\(HW\)</span> 大小為 <span
class="math inline">\(C\)</span> 的 sequence</p>
<p><img src="https://i.imgur.com/6pAB8h3.png"
alt="image-20210709115659980" /></p>
<p>以下 <span class="math inline">\(N_q N_k\)</span> 其實就是 <span
class="math inline">\(HW\)</span>，則輸入向量 <span
class="math inline">\((N \cdot C)\)</span>，乘上一個 <span
class="math inline">\(W\)</span> 轉換矩陣 <span class="math inline">\((C
\cdot 1)\)</span> 則計算 self attention 的時間複雜度為：</p>
<p><span class="math display">\[
O(N_qC^2 + N_kC^2 + N_qN_kC)
\]</span></p>
<p>分別對應</p>
<p><span class="math inline">\(O(N_qC^2)\)</span> 計算 Query
的複雜度</p>
<p><span class="math inline">\(O(N_kC^2)\)</span> 計算 key 的複雜度</p>
<p><span class="math inline">\(O(N_qN_kC)\)</span> Attention 的複雜度
<span class="math inline">\((N_qC \cdot CN_k) = (N_qN_k)\)</span></p>
<p><img src="https://i.imgur.com/gr4y6sI.png"
alt="image-20210709120129060" /></p>
<p>透過以上可以發現當圖片的解析度越大，Attention
的計算複雜度為所有像素數量的平方，也就是 <span
class="math inline">\((HW)^2 = N^2\)</span> ，這就導致 Transformer
參數使用量及計算量特高的原因</p>
<h4 id="堆疊的層數的限制">3. 堆疊的層數的限制</h4>
<p>根據以下這篇論文的實驗 (DeepViT) <a
href="https://arxiv.org/abs/2103.11886">https://arxiv.org/abs/2103.11886</a></p>
<p>發現隨著網路層數的增加，各個 attention head
所關注的資料也漸漸靠攏全局，而之間的相關性也會漸漸上升。</p>
<p><img src="https://i.imgur.com/nG54UWa.png" alt="Image" /></p>
<p>因此如果單純的疊加層數，效果反而不一定會變更好</p>
<h4 id="模型本身無位置編碼">4. 模型本身無位置編碼</h4>
<p>可以參考我之前寫過的文章</p>
<p><a
href="https://mushding.space/2021/07/30/CNN-%E8%88%87%E4%BD%8D%E7%BD%AE%E8%B3%87%E8%A8%8A-CNN-%E5%80%92%E5%BA%95%E5%AD%B8%E5%88%B0%E4%BA%86%E4%BB%80%E9%BA%BC%EF%BC%9F-Position-Padding-and-Predictions-A-Deeper-Look-at-Position-Information-in-CNNs-CNN-%E8%88%87%E7%B5%95%E5%B0%8D%E4%BD%8D%E7%BD%AE%E8%B3%87%E8%A8%8A/">CNN
與絕對位置資訊 - CNN 倒底學到了什麼？</a></p>
<p><a
href="https://mushding.space/2021/07/28/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Conditional-Positional-Encodings-for-Vision-Transformers-%E5%8F%AF%E8%AE%8A%E5%BA%8F%E5%88%97%E9%95%B7%E7%9F%AD%E7%9A%84-Positional-Encoding/">Vision
Transformer 演化史: Conditional Positional Encodings for Vision
Transformers - 可變序列長短的 Positional Encoding</a></p>
<p>Transformer 因為解決了 RNN
訓練時間過長的問題提出平行化訓練，但同時也捨棄了 RNN 的 Inductive Bias
也就是時間上的假設。因此 Transformer 無法了解各字詞之間的順序關系。</p>
<p>而 CNN 根據實驗，因 kernel 及 padding
的關系，使網路學習到了相對的位置資訊。</p>
<p>為了補足 Transformer 沒有如同 CNN 一樣的位置資訊，因此加上了
Positional Encoding。</p>
<p>但也有一些實驗試著結合 CNN 使 Positional Encoding
用更自然的方式實現。</p>
<h4 id="局部注意力較弱">5. 局部注意力較弱</h4>
<p>一個 Patch 內部像素之間的相關性，不如 CNN 來得強
未來實驗可以朝前半段使用 Transformer 後半段使用 CNN 來互補</p>
<p><img src="https://i.imgur.com/HoSmP7V.png" alt="Image" /></p>
<h2 id="reference">Reference</h2>
<h3 id="vit">ViT</h3>
<p><a href="https://www.youtube.com/watch?v=DVoHvmww2lQ">(Youtube) An
image is worth 16x16 words: ViT | Is this the extinction of CNNs? Long
live the Transformer?</a></p>
<p><a href="https://www.youtube.com/watch?v=TrdevFK_am4">(Youtube) An
Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
(Paper Explained)</a></p>
<p><a href="https://www.youtube.com/watch?v=j6kuz_NqkG0">(Youtube)
Vision Transformer (ViT) - An image is worth 16x16 words | Paper
Explained</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/342261872">Vision Transformer
超详细解读 (原理分析+代码解读) (二)</a></p>
<p><a href="https://bbs.huaweicloud.com/blogs/298123">目前Vision
Transformer遇到的问题和克服方法的相关论文汇总</a></p>
<h3 id="vit-vs-cnn">ViT vs CNN</h3>
<p><a href="https://arxiv.org/pdf/2108.08810.pdf">(arxiv) Do Vision
Transformers See Like Convolutional Neural Networks?</a></p>
<p><a href="https://arxiv.org/pdf/2101.01169.pdf">(arxiv) Transformers
in Vision: A Survey</a></p>
<h3 id="vit-vs-mlp">ViT vs MLP</h3>
<p><a
href="https://www.gushiciku.cn/pl/gXcq/zh-tw">歸納偏置多餘了？靠“資料堆砌”火拼Transformer，MLP架構可有勝算？</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/405295929">CNN vs
Transformer、MLP，谁更胜一筹？</a></p>
<h3 id="other">Other</h3>
<p><a href="https://www.youtube.com/watch?v=aH7s6qXEUcc">(Youtube)
Transformers can do both images and text. Here is why.</a></p>
<p><a href="https://www.zhihu.com/question/264264203">如何理解Inductive
bias？</a></p>
<p><a
href="https://peltarion.com/blog/data-science/self-attention-video">(超猛視覺化
Self-Attention) Getting meaning from text: self-attention step-by-step
video</a></p>
<p>[Attention and
Transformers](https://theaisummer.com/topics/attention/</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>SwinIR 讀原始碼心得</title>
    <url>/2021/12/02/SwinIR-%E8%AE%80%E5%8E%9F%E5%A7%8B%E7%A2%BC%E5%BF%83%E5%BE%97/</url>
    <content><![CDATA[<p>SwinIR 讀原始碼心得</p>
<p>Github 連結：<a
href="https://github.com/JingyunLiang/SwinIR">https://github.com/JingyunLiang/SwinIR</a></p>
<p>keywords: <span id="more"></span> ## 網路架構</p>
<h3 id="swinir">SwinIR</h3>
<blockquote>
<p>網路主進入點</p>
</blockquote>
<h4 id="參數">參數</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">img_size              輸入圖片的大小</span><br><span class="line">patch_size              </span><br><span class="line">in_chans              輸入時的 channel 為 3</span><br><span class="line">embed_dim             Patch embedding 的大小，為 96</span><br><span class="line">depths                每一個階段由幾個 Swin Transformer 組今，為 (6, 6, 6, 6)</span><br><span class="line">num_heads             Attention 中的 head 數量，為 (6, 6, 6, 6)</span><br><span class="line">window_size           window 的大小，為 7</span><br><span class="line">mlp_ratio             Transformer 中的 MLP 層放大倍率 (invert bottleneck)，為 4</span><br><span class="line">qkv_bias              在 Attention 中加入 B Bias，目的是加入 relevent positional encoding</span><br><span class="line">qk_scale              把 QK 後的結果縮小二倍 </span><br><span class="line">drop_rate             dropout 設定比率</span><br><span class="line">attn_drop_rate        Attention 也可設 dropout 比率</span><br><span class="line">drop_path_rate</span><br><span class="line">norm_layer            normalization 層設定，為 Layer Normalization</span><br><span class="line">ape                   加入決對位置資訊</span><br><span class="line">patch_norm            在 patch embedding 後加一層 normalization</span><br><span class="line">use_checkpoint        把訓練到一半的網路參數存起來</span><br><span class="line">upscale               要把圖片放大幾倍 (2/3/4/8)</span><br><span class="line">img_range             圖片的「範圍」，1. or 255.</span><br><span class="line">upsampler             使用什麼方法上採樣，為 pixelshuffle</span><br><span class="line">resi_connection       在一個 RSTB 中，會有一個 residual connection，設定要加上一個 3x3 conv 還是一個 inverted-bottleneck 的 3x3 conv</span><br></pre></td></tr></table></figure>
<h4 id="第-0-步---初始變數">第 0 步 - 初始變數</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, img_size=<span class="number">64</span>, patch_size=<span class="number">1</span>, in_chans=<span class="number">3</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             embed_dim=<span class="number">96</span>, depths=[<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>], num_heads=[<span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>],</span></span></span><br><span class="line"><span class="params"><span class="function">             window_size=<span class="number">7</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>, drop_path_rate=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             norm_layer=nn.LayerNorm, ape=<span class="literal">False</span>, patch_norm=<span class="literal">True</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             use_checkpoint=<span class="literal">False</span>, upscale=<span class="number">2</span>, img_range=<span class="number">1.</span>, upsampler=<span class="string">&#x27;&#x27;</span>, resi_connection=<span class="string">&#x27;1conv&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             **kwargs</span>):</span></span><br><span class="line">    <span class="built_in">super</span>(SwinIR, self).__init__()</span><br><span class="line">    num_in_ch = in_chans</span><br><span class="line">    num_out_ch = in_chans</span><br><span class="line">    num_feat = <span class="number">64</span></span><br><span class="line">    self.img_range = img_range</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 做 Mean Shift 處理，方法從 EDSR 這篇論文開始的</span></span><br><span class="line">    <span class="keyword">if</span> in_chans == <span class="number">3</span>:</span><br><span class="line">        rgb_mean = (<span class="number">0.4488</span>, <span class="number">0.4371</span>, <span class="number">0.4040</span>)</span><br><span class="line">        self.mean = torch.Tensor(rgb_mean).view(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.mean = torch.zeros(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    self.upscale = upscale</span><br><span class="line">    self.upsampler = upsampler</span><br><span class="line">    self.window_size = window_size</span><br></pre></td></tr></table></figure>
<h4 id="第-1-步---淺層特徵提取">第 1 步 - 淺層特徵提取</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># in channel 為 3</span></span><br><span class="line"><span class="comment"># out channel 為 96</span></span><br><span class="line">self.conv_first = nn.Conv2d(num_in_ch, embed_dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="第-2-步---深層特徵提取">第 2 步 - 深層特徵提取</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 圖片轉 Patch</span></span><br><span class="line"><span class="comment"># split image into non-overlapping patches</span></span><br><span class="line">self.patch_embed = PatchEmbed( <span class="comment">#<span class="doctag">TODO:</span></span></span><br><span class="line">    img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,</span><br><span class="line">    norm_layer=norm_layer <span class="keyword">if</span> self.patch_norm <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">num_patches = self.patch_embed.num_patches</span><br><span class="line">patches_resolution = self.patch_embed.patches_resolution</span><br><span class="line">self.patches_resolution = patches_resolution</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Patch 轉圖片</span></span><br><span class="line"><span class="comment"># merge non-overlapping patches into image</span></span><br><span class="line">self.patch_unembed = PatchUnEmbed(</span><br><span class="line">    img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,</span><br><span class="line">    norm_layer=norm_layer <span class="keyword">if</span> self.patch_norm <span class="keyword">else</span> <span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加入絕對資訊 (可選擇)</span></span><br><span class="line"><span class="comment"># absolute position embedding</span></span><br><span class="line"><span class="keyword">if</span> self.ape:</span><br><span class="line">    self.absolute_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches, embed_dim))</span><br><span class="line">    trunc_normal_(self.absolute_pos_embed, std=<span class="number">.02</span>)</span><br><span class="line">self.pos_drop = nn.Dropout(p=drop_rate)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 隨機深度</span></span><br><span class="line"><span class="comment"># stochastic depth</span></span><br><span class="line">dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))]  <span class="comment"># stochastic depth decay rule</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立超多的 RSTB 層</span></span><br><span class="line"><span class="comment"># build Residual Swin Transformer blocks (RSTB)</span></span><br><span class="line">self.layers = nn.ModuleList()</span><br><span class="line"><span class="comment"># depth 為 [6, 6, 6, 6]</span></span><br><span class="line"><span class="comment"># 4 個 num_layers</span></span><br><span class="line"><span class="comment"># 每一個 num_layers 有 6 個 SwinIR Layer</span></span><br><span class="line"><span class="keyword">for</span> i_layer <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">    layer = RSTB(dim=embed_dim,</span><br><span class="line">                 input_resolution=(patches_resolution[<span class="number">0</span>],</span><br><span class="line">                                   patches_resolution[<span class="number">1</span>]),</span><br><span class="line">                 depth=depths[i_layer],</span><br><span class="line">                 num_heads=num_heads[i_layer],</span><br><span class="line">                 window_size=window_size,</span><br><span class="line">                 mlp_ratio=self.mlp_ratio,</span><br><span class="line">                 qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                 drop=drop_rate, attn_drop=attn_drop_rate,</span><br><span class="line">                 drop_path=dpr[<span class="built_in">sum</span>(depths[:i_layer]):<span class="built_in">sum</span>(depths[:i_layer + <span class="number">1</span>])],  <span class="comment"># no impact on SR results</span></span><br><span class="line">                 norm_layer=norm_layer,</span><br><span class="line">                 downsample=<span class="literal">None</span>,</span><br><span class="line">                 use_checkpoint=use_checkpoint,</span><br><span class="line">                 img_size=img_size,</span><br><span class="line">                 patch_size=patch_size,</span><br><span class="line">                 resi_connection=resi_connection</span><br><span class="line">                 )</span><br><span class="line">    self.layers.append(layer)</span><br><span class="line">self.norm = norm_layer(self.num_features)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 建立最後一個 CNN 特徵提取層</span></span><br><span class="line"><span class="comment"># build the last conv layer in deep feature extraction</span></span><br><span class="line"><span class="keyword">if</span> resi_connection == <span class="string">&#x27;1conv&#x27;</span>:</span><br><span class="line">    self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">elif</span> resi_connection == <span class="string">&#x27;3conv&#x27;</span>:</span><br><span class="line">    <span class="comment"># to save parameters and memory</span></span><br><span class="line">    self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">                                         nn.LeakyReLU(negative_slope=<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">                                         nn.Conv2d(embed_dim // <span class="number">4</span>, embed_dim // <span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">                                         nn.LeakyReLU(negative_slope=<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">                                         nn.Conv2d(embed_dim // <span class="number">4</span>, embed_dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h3 id="patchembed">PatchEmbed</h3>
<blockquote>
<p>把影像轉換成 Patch Embedding</p>
</blockquote>
<h4 id="參數-1">參數</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">img_size              圖片大小，為 224</span><br><span class="line">patch_size            patch 的大小，為 4</span><br><span class="line">in_chans              輸入 channel，為 3</span><br><span class="line">embed_dim             輸出 channel，為 96</span><br><span class="line">norm_layer            做完 patch embedding 後要不要做 normalization，為 None</span><br></pre></td></tr></table></figure>
<h4 id="程式">程式</h4>
<blockquote>
<p>直接用 flatten 的方式把圖片從 <span class="math inline">\(B\times
H\times W\times C\)</span> 變成 <span class="math inline">\(B\times
P^2\times C\)</span> 並且生出 patches_resolution (PxP 的大小) 還有
num_patches (patch 數量) (為什麼沒有 conv，stride=kernel_size)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    img_size = to_2tuple(img_size)</span><br><span class="line">    patch_size = to_2tuple(patch_size)</span><br><span class="line">    patches_resolution = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">    </span><br><span class="line">    self.img_size = img_size</span><br><span class="line">    self.patch_size = patch_size</span><br><span class="line">    <span class="comment"># 兩個後面會用到的參數</span></span><br><span class="line">    self.patches_resolution = patches_resolution</span><br><span class="line">    self.num_patches = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    self.in_chans = in_chans</span><br><span class="line">    self.embed_dim = embed_dim</span><br><span class="line">    <span class="keyword">if</span> norm_layer <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.norm = norm_layer(embed_dim)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.norm = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># B Ph*Pw C # 就這裡，為什麼沒有用 conv？</span></span><br><span class="line">    <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 還有計算 flops 的 function 呢！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flops</span>(<span class="params">self</span>):</span></span><br><span class="line">    flops = <span class="number">0</span></span><br><span class="line">    H, W = self.img_size</span><br><span class="line">    <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        flops += H * W * self.embed_dim</span><br><span class="line">    <span class="keyword">return</span> flops</span><br></pre></td></tr></table></figure>
<h3 id="patchunembed">PatchUnEmbed</h3>
<blockquote>
<p>把 patch embedding 改回原圖 個人覺得意義不明</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    img_size = to_2tuple(img_size)</span><br><span class="line">    patch_size = to_2tuple(patch_size)</span><br><span class="line">    patches_resolution = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">    self.img_size = img_size</span><br><span class="line">    self.patch_size = patch_size</span><br><span class="line">    self.patches_resolution = patches_resolution</span><br><span class="line">    self.num_patches = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    self.in_chans = in_chans</span><br><span class="line">    self.embed_dim = embed_dim</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, x_size</span>):</span></span><br><span class="line">    B, HW, C = x.shape</span><br><span class="line">    <span class="comment"># 在這裡把二維向量轉回三維影像</span></span><br><span class="line">    x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).view(B, self.embed_dim, x_size[<span class="number">0</span>], x_size[<span class="number">1</span>])  <span class="comment"># B Ph*Pw C</span></span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flops</span>(<span class="params">self</span>):</span></span><br><span class="line">    flops = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> flops</span><br></pre></td></tr></table></figure>
<h3 id="rstb">RSTB</h3>
<blockquote>
<p>負責深層特徵的提取 由 BasicLayer (一堆 Swin Transformer) 以及一層 CNN
所組成</p>
</blockquote>
<h4 id="參數-2">參數</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dim    輸入維度</span><br><span class="line">與 SwinIR 差不多，大都份都是直接傳下來的</span><br></pre></td></tr></table></figure>
<h4 id="程式-1">程式</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 設</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, input_resolution, depth, num_heads, window_size,</span></span></span><br><span class="line"><span class="params"><span class="function">             mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             drop_path=<span class="number">0.</span>, norm_layer=nn.LayerNorm, downsample=<span class="literal">None</span>, use_checkpoint=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             img_size=<span class="number">224</span>, patch_size=<span class="number">4</span>, resi_connection=<span class="string">&#x27;1conv&#x27;</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>(RSTB, self).__init__()</span><br><span class="line">    self.dim = dim</span><br><span class="line">    self.input_resolution = input_resolution</span><br><span class="line">    self.residual_group = BasicLayer(dim=dim,</span><br><span class="line">                                     input_resolution=input_resolution,</span><br><span class="line">                                     depth=depth,</span><br><span class="line">                                     num_heads=num_heads,</span><br><span class="line">                                     window_size=window_size,</span><br><span class="line">                                     mlp_ratio=mlp_ratio,</span><br><span class="line">                                     qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                                     drop=drop, attn_drop=attn_drop,</span><br><span class="line">                                     drop_path=drop_path,</span><br><span class="line">                                     norm_layer=norm_layer,</span><br><span class="line">                                     downsample=downsample,</span><br><span class="line">                                     use_checkpoint=use_checkpoint)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 選擇一個 3x3 還是 bottlenect 3x3</span></span><br><span class="line">    <span class="keyword">if</span> resi_connection == <span class="string">&#x27;1conv&#x27;</span>:</span><br><span class="line">        self.conv = nn.Conv2d(dim, dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">elif</span> resi_connection == <span class="string">&#x27;3conv&#x27;</span>:</span><br><span class="line">        <span class="comment"># to save parameters and memory</span></span><br><span class="line">        self.conv = nn.Sequential(nn.Conv2d(dim, dim // <span class="number">4</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>), nn.LeakyReLU(negative_slope=<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">                                  nn.Conv2d(dim // <span class="number">4</span>, dim // <span class="number">4</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>),</span><br><span class="line">                                  nn.LeakyReLU(negative_slope=<span class="number">0.2</span>, inplace=<span class="literal">True</span>),</span><br><span class="line">                                  nn.Conv2d(dim // <span class="number">4</span>, dim, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 變 patch 以及變回圖片的方法</span></span><br><span class="line"><span class="comment"># 迷之 in_chans = 0，class 內跟本沒有用到這個參數…</span></span><br><span class="line">    self.patch_embed = PatchEmbed(</span><br><span class="line">        img_size=img_size, patch_size=patch_size, in_chans=<span class="number">0</span>, embed_dim=dim,</span><br><span class="line">        norm_layer=<span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 這個也同理，也沒有用到 in_chans…</span></span><br><span class="line">    self.patch_unembed = PatchUnEmbed(</span><br><span class="line">        img_size=img_size, patch_size=patch_size, in_chans=<span class="number">0</span>, embed_dim=dim,</span><br><span class="line">        norm_layer=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># forward 函數</span></span><br><span class="line"><span class="comment"># 流程：</span></span><br><span class="line"><span class="comment"># Swin Transformer 群 -&gt; 變回三維影像 -&gt; 做一層卷積 -&gt; 變回二維向量 -&gt; 加上 Residual connection (identity)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, x_size</span>):</span></span><br><span class="line">    <span class="keyword">return</span> self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 貼心的計算 flops ！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flops</span>(<span class="params">self</span>):</span></span><br><span class="line">    flops = <span class="number">0</span></span><br><span class="line">    flops += self.residual_group.flops()</span><br><span class="line">    H, W = self.input_resolution</span><br><span class="line">    flops += H * W * self.dim * self.dim * <span class="number">9</span></span><br><span class="line">    flops += self.patch_embed.flops()</span><br><span class="line">    flops += self.patch_unembed.flops()</span><br><span class="line">    <span class="keyword">return</span> flops</span><br></pre></td></tr></table></figure>
<h3 id="basiclayer">BasicLayer</h3>
<blockquote>
<p>在此建立 6 層 Swin Transformer #### 參數</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">與 RSTB 差不多</span><br></pre></td></tr></table></figure>
<h4 id="程式-2">程式</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定義一些變數</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, input_resolution, depth, num_heads, window_size,</span></span></span><br><span class="line"><span class="params"><span class="function">             mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             drop_path=<span class="number">0.</span>, norm_layer=nn.LayerNorm, downsample=<span class="literal">None</span>, use_checkpoint=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.dim = dim</span><br><span class="line">    self.input_resolution = input_resolution</span><br><span class="line">    self.depth = depth</span><br><span class="line">    self.use_checkpoint = use_checkpoint</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 堆疊 Swin Transformer Block</span></span><br><span class="line">    <span class="comment"># build blocks</span></span><br><span class="line">    self.blocks = nn.ModuleList([</span><br><span class="line">        SwinTransformerBlock(dim=dim, input_resolution=input_resolution,</span><br><span class="line">                             num_heads=num_heads, window_size=window_size,</span><br><span class="line">                             <span class="comment"># shift size 為 0 表示不動</span></span><br><span class="line">                             <span class="comment"># 當到下一個 Swin Block 時，移動 window size 的一半 (7 // 2 = 3)</span></span><br><span class="line">                             shift_size=<span class="number">0</span> <span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>) <span class="keyword">else</span> window_size // <span class="number">2</span>,</span><br><span class="line">                             mlp_ratio=mlp_ratio,</span><br><span class="line">                             qkv_bias=qkv_bias, qk_scale=qk_scale,</span><br><span class="line">                             drop=drop, attn_drop=attn_drop,</span><br><span class="line">                             drop_path=drop_path[i] <span class="keyword">if</span> <span class="built_in">isinstance</span>(drop_path, <span class="built_in">list</span>) <span class="keyword">else</span> drop_path,</span><br><span class="line">                             norm_layer=norm_layer)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 這裡做 patch merging，把 H/4 W/4 C 轉變成 H/8 W/8 2C</span></span><br><span class="line">    <span class="comment"># patch merging layer</span></span><br><span class="line">    <span class="keyword">if</span> downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.downsample = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定義 forward 函數</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, x_size</span>):</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">        <span class="keyword">if</span> self.use_checkpoint:</span><br><span class="line">            x = checkpoint.checkpoint(blk, x, x_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = blk(x, x_size)</span><br><span class="line">    <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        x = self.downsample(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 印出變數用的</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extra_repr</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;dim=<span class="subst">&#123;self.dim&#125;</span>, input_resolution=<span class="subst">&#123;self.input_resolution&#125;</span>, depth=<span class="subst">&#123;self.depth&#125;</span>&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 貼心算 flop ！</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flops</span>(<span class="params">self</span>):</span></span><br><span class="line">    flops = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">        flops += blk.flops()</span><br><span class="line">    <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        flops += self.downsample.flops()</span><br><span class="line">    <span class="keyword">return</span> flops</span><br></pre></td></tr></table></figure>
<h3 id="swin-transformer-block">Swin Transformer Block</h3>
<blockquote>
<p>Swin Transformer 的主流程</p>
</blockquote>
<h4 id="參數-3">參數</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">比較不一樣的是：</span><br><span class="line">shift_block        window 下一個位置要移動幾格</span><br></pre></td></tr></table></figure>
<h4 id="程式-3">程式</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定義一些初始變數</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, input_resolution, num_heads, window_size=<span class="number">7</span>, shift_size=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>, drop_path=<span class="number">0.</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">             act_layer=nn.GELU, norm_layer=nn.LayerNorm</span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.dim = dim</span><br><span class="line">    self.input_resolution = input_resolution</span><br><span class="line">    self.num_heads = num_heads</span><br><span class="line">    self.window_size = window_size</span><br><span class="line">    self.shift_size = shift_size</span><br><span class="line">    self.mlp_ratio = mlp_ratio</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果輸入影像的大小小於 window size 的話，就不會分割 windows 了</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">min</span>(self.input_resolution) &lt;= self.window_size:</span><br><span class="line">        <span class="comment"># if window size is larger than input resolution, we don&#x27;t partition windows</span></span><br><span class="line">        self.shift_size = <span class="number">0</span></span><br><span class="line">        self.window_size = <span class="built_in">min</span>(self.input_resolution)</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= self.shift_size &lt; self.window_size, <span class="string">&quot;shift_size must in 0-window_size&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Attention 中的第一個 Norm</span></span><br><span class="line">self.norm1 = norm_layer(dim)</span><br><span class="line"><span class="comment"># 這裡傳入 window attention</span></span><br><span class="line">self.attn = WindowAttention(</span><br><span class="line">    dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,</span><br><span class="line">    qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line"><span class="comment"># 如果有使用 Schotistic depth 的話，就用 dropPath</span></span><br><span class="line">self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"><span class="comment"># Attention 中的第二個 Norm</span></span><br><span class="line">self.norm2 = norm_layer(dim)</span><br><span class="line"><span class="comment"># bottleneck 的 MLP，放大四倍</span></span><br><span class="line">mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加入不會隨著網路更新的參數 (buffer) attention mask，用來蓋住 cyclic cycle 後的計算</span></span><br><span class="line"><span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">    attn_mask = self.calculate_mask(self.input_resolution)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    attn_mask = <span class="literal">None</span></span><br><span class="line">self.register_buffer(<span class="string">&quot;attn_mask&quot;</span>, attn_mask)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 產生給 SW-MSA 的 Mask (有點複雜 XD)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_mask</span>(<span class="params">self, x_size</span>):</span></span><br><span class="line">    <span class="comment"># calculate attention mask for SW-MSA</span></span><br><span class="line">    H, W = x_size</span><br><span class="line">    img_mask = torch.zeros((<span class="number">1</span>, H, W, <span class="number">1</span>))  <span class="comment"># 1 H W 1</span></span><br><span class="line">    h_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">    w_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">    cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> h <span class="keyword">in</span> h_slices:</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> w_slices:</span><br><span class="line">            img_mask[:, h, w, :] = cnt</span><br><span class="line">            cnt += <span class="number">1</span></span><br><span class="line">    mask_windows = window_partition(img_mask, self.window_size)  <span class="comment"># nW, window_size, window_size, 1</span></span><br><span class="line">    mask_windows = mask_windows.view(-<span class="number">1</span>, self.window_size * self.window_size)</span><br><span class="line">    attn_mask = mask_windows.unsqueeze(<span class="number">1</span>) - mask_windows.unsqueeze(<span class="number">2</span>)</span><br><span class="line">    attn_mask = attn_mask.masked_fill(attn_mask != <span class="number">0</span>, <span class="built_in">float</span>(-<span class="number">100.0</span>)).masked_fill(attn_mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">    <span class="keyword">return</span> attn_mask</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, x_size</span>):</span></span><br><span class="line">    H, W = x_size</span><br><span class="line">    B, L, C = x.shape</span><br><span class="line">    <span class="comment"># assert L == H * W, &quot;input feature has wrong size&quot;</span></span><br><span class="line">    shortcut = x</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第一步：先過一個 LN</span></span><br><span class="line">    x = self.norm1(x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第二步：轉成三維影像做 如果 window 有移動過 -&gt; 做 cyclic shift，把影像拼回正常 windows 分佈</span></span><br><span class="line">    x = x.view(B, H, W, C)</span><br><span class="line">    <span class="comment"># cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shifted_x = x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第三步：</span></span><br><span class="line"><span class="comment"># 把影像又從三維 N*H*W*C 轉變成，有 N 個 window，長寬為 M 的一堆 windows </span></span><br><span class="line">    <span class="comment"># partition windows</span></span><br><span class="line">    x_windows = window_partition(shifted_x, self.window_size)  <span class="comment"># nW*B, window_size, window_size, C</span></span><br><span class="line"><span class="comment"># 再把它轉回二維向量 nW*B, window_size*window_size, C</span></span><br><span class="line">    x_windows = x_windows.view(-<span class="number">1</span>, self.window_size * self.window_size, C)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第四步：</span></span><br><span class="line"><span class="comment"># 經過 W-MSA 層，或是 SW-MSA 層</span></span><br><span class="line"><span class="comment"># 因為 window 的特性，只要圖片大小是 window size 的倍數，都可以放進網路中訓練 / 測試</span></span><br><span class="line">    <span class="comment"># W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size</span></span><br><span class="line">    <span class="keyword">if</span> self.input_resolution == x_size:</span><br><span class="line">        attn_windows = self.attn(x_windows, mask=self.attn_mask)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第五步：</span></span><br><span class="line"><span class="comment"># 把二維向量轉回三維影像</span></span><br><span class="line">    <span class="comment"># merge windows</span></span><br><span class="line">    attn_windows = attn_windows.view(-<span class="number">1</span>, self.window_size, self.window_size, C)</span><br><span class="line">    shifted_x = window_reverse(attn_windows, self.window_size, H, W)  <span class="comment"># B H&#x27; W&#x27; C</span></span><br><span class="line"><span class="comment"># 第六步：</span></span><br><span class="line"><span class="comment"># 把剛剛 cyclic shift 給拼回去</span></span><br><span class="line">    <span class="comment"># reverse cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = shifted_x</span><br><span class="line">    x = x.view(B, H * W, C)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 第七步：</span></span><br><span class="line"><span class="comment"># 首先是 Attention 的 shortcut</span></span><br><span class="line">    x = shortcut + self.drop_path(x)</span><br><span class="line"><span class="comment"># 再來是 FFN (LN + MLP) + shortcut</span></span><br><span class="line">    x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extra_repr</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;dim=<span class="subst">&#123;self.dim&#125;</span>, input_resolution=<span class="subst">&#123;self.input_resolution&#125;</span>, num_heads=<span class="subst">&#123;self.num_heads&#125;</span>, &quot;</span> \</span><br><span class="line">           <span class="string">f&quot;window_size=<span class="subst">&#123;self.window_size&#125;</span>, shift_size=<span class="subst">&#123;self.shift_size&#125;</span>, mlp_ratio=<span class="subst">&#123;self.mlp_ratio&#125;</span>&quot;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flops</span>(<span class="params">self</span>):</span></span><br><span class="line">    flops = <span class="number">0</span></span><br><span class="line">    H, W = self.input_resolution</span><br><span class="line">    <span class="comment"># norm1</span></span><br><span class="line">    flops += self.dim * H * W</span><br><span class="line">    <span class="comment"># W-MSA/SW-MSA</span></span><br><span class="line">    nW = H * W / self.window_size / self.window_size</span><br><span class="line">    flops += nW * self.attn.flops(self.window_size * self.window_size)</span><br><span class="line">    <span class="comment"># mlp</span></span><br><span class="line">    flops += <span class="number">2</span> * H * W * self.dim * self.dim * self.mlp_ratio</span><br><span class="line">    <span class="comment"># norm2</span></span><br><span class="line">    flops += self.dim * H * W</span><br><span class="line">    <span class="keyword">return</span> flops</span><br></pre></td></tr></table></figure>
<h3 id="windowattention">WindowAttention</h3>
<blockquote>
<p>定義 Attenion 的部份</p>
</blockquote>
<h4 id="參數-4">參數</h4>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">都差不多</span><br></pre></td></tr></table></figure>
<h4 id="程式-4">程式</h4>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化變數</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, qk_scale=<span class="literal">None</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span></span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    self.dim = dim</span><br><span class="line">    self.window_size = window_size  <span class="comment"># Wh, Ww</span></span><br><span class="line">    self.num_heads = num_heads</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 我現在才知道…，原來 Attention 中的特徵數要除上 head 的數量，才是一個 head 的特徵數</span></span><br><span class="line">    <span class="comment"># 為了要與其它 head 特徵相加時維持數量相等</span></span><br><span class="line">    head_dim = dim // num_heads</span><br><span class="line">    self.scale = qk_scale <span class="keyword">or</span> head_dim ** -<span class="number">0.5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定義相對位置表 (Parameter)，等等會用來做對應用</span></span><br><span class="line"><span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">    torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義論文中提到的 relative position bias </span></span><br><span class="line"><span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  <span class="comment"># 2, Wh, Ww</span></span><br><span class="line">coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># 2, Wh*Ww</span></span><br><span class="line">relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># 2, Wh*</span></span><br><span class="line">relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># Wh*Ww, Wh*Ww, 2</span></span><br><span class="line">relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># Wh*Ww, Wh*Ww</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 稱作 relative_position_index，會用這個當 index 去對應上面的表</span></span><br><span class="line">self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以下為定義 Self-Attention 的變數們</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一口氣用 Linear 生出三倍的特徵量，分別代表 QKV 之後會再分開來</span></span><br><span class="line">self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">self.proj = nn.Linear(dim, dim)</span><br><span class="line">self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Truncated normal distribution 截斷常態分佈</span></span><br><span class="line"><span class="comment"># 簡單來說就是根據一個範圍，只選擇一定範圍的常態分佈</span></span><br><span class="line"><span class="comment"># ex 標準差為 2</span></span><br><span class="line">trunc_normal_(self.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: input features with shape of (num_windows*B, N, C)</span></span><br><span class="line"><span class="string">        mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 在這裡把 QKV 分家</span></span><br><span class="line">    B_, N, C = x.shape</span><br><span class="line">    qkv = self.qkv(x).reshape(B_, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">    q, k, v = qkv[<span class="number">0</span>], qkv[<span class="number">1</span>], qkv[<span class="number">2</span>]  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># QK^T</span></span><br><span class="line">    q = q * self.scale</span><br><span class="line">    attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">		</span><br><span class="line">    <span class="comment"># QK^T + B</span></span><br><span class="line">    relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">        self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)  <span class="comment"># Wh*Ww,Wh*Ww,nH</span></span><br><span class="line">    relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># nH, Wh*Ww, Wh*Ww</span></span><br><span class="line">    attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SoftMax(QK^T + B)</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        nW = mask.shape[<span class="number">0</span>]</span><br><span class="line">        attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        attn = attn.view(-<span class="number">1</span>, self.num_heads, N, N)</span><br><span class="line">        attn = self.softmax(attn)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">    attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># SoftMax(QK^T + B) V</span></span><br><span class="line">    x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># SoftMax(QK^T + B) V W^V</span></span><br><span class="line">    x = self.proj(x)</span><br><span class="line">    x = self.proj_drop(x)</span><br><span class="line">    <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extra_repr</span>(<span class="params">self</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&#x27;dim=<span class="subst">&#123;self.dim&#125;</span>, window_size=<span class="subst">&#123;self.window_size&#125;</span>, num_heads=<span class="subst">&#123;self.num_heads&#125;</span>&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flops</span>(<span class="params">self, N</span>):</span></span><br><span class="line">    <span class="comment"># calculate flops for 1 window with token length of N</span></span><br><span class="line">    flops = <span class="number">0</span></span><br><span class="line">    <span class="comment"># qkv = self.qkv(x)</span></span><br><span class="line">    flops += N * self.dim * <span class="number">3</span> * self.dim</span><br><span class="line">    <span class="comment"># attn = (q @ k.transpose(-2, -1))</span></span><br><span class="line">    flops += self.num_heads * N * (self.dim // self.num_heads) * N</span><br><span class="line">    <span class="comment">#  x = (attn @ v)</span></span><br><span class="line">    flops += self.num_heads * N * N * (self.dim // self.num_heads)</span><br><span class="line">    <span class="comment"># x = self.proj(x)</span></span><br><span class="line">    flops += N * self.dim * self.dim</span><br><span class="line">    <span class="keyword">return</span> flops</span><br></pre></td></tr></table></figure>
<h3 id="window_partition">window_partition</h3>
<blockquote>
<p>把 BxHxWxC 變成</p>
<p>(Bx window 數量) x window 長 x window 寬 x C</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">window_partition</span>(<span class="params">x, window_size</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">        window_size (int): window size</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows</span><br></pre></td></tr></table></figure>
<h3 id="window_reverse">window_reverse</h3>
<blockquote>
<p>(Bx window 數量) x window 長 x window 寬 x C</p>
<p>變成 BxHxWxC</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">window_reverse</span>(<span class="params">windows, window_size, H, W</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">        window_size (int): Window size</span></span><br><span class="line"><span class="string">        H (int): Height of image</span></span><br><span class="line"><span class="string">        W (int): Width of image</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B = <span class="built_in">int</span>(windows.shape[<span class="number">0</span>] / (H * W / window_size / window_size))</span><br><span class="line">    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -<span class="number">1</span>)</span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(B, H, W, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
        <tag>Source Code</tag>
      </tags>
  </entry>
  <entry>
    <title>Stochastic depth 隨機深度</title>
    <url>/2021/12/01/Stochastic-depth-%E9%9A%A8%E6%A9%9F%E6%B7%B1%E5%BA%A6/</url>
    <content><![CDATA[<p>論文地址：</p>
<p><a
href="https://arxiv.org/pdf/1603.09382v3.pdf">https://arxiv.org/pdf/1603.09382v3.pdf</a></p>
<p>Stochastic depth 這篇論文是在 ECCV 2016 所出的方向，這個時候是介於
ResNet 提出後，及 DenseNet 之前</p>
<p>而提出的作者 Gao Huang 也正是 ResNet 同一個作者</p>
<p>keywords: Stochastic depth、ResNet <span id="more"></span></p>
<h2 id="目的">目的</h2>
<p>ResNet 提出 shortcut
的目的就是為了解決當網路過深時，可以有效的學習特徵，把每一個 block 加上
Residual line 使得每個 block 只學到網路上下的「差值」而已</p>
<p>而 Stochastic depth 則是進一步拓展這個想法，除了跳過一個 block
之外，直接跳過網路中的一層</p>
<p>利用一個隨機變數來控制網路中的某一層，是不是要直接省略不訓練，其機率會隨著網路越深而越大</p>
<p>作者發現利用這個方法可以進一步提高 ResNet 的 Generalization
的能力，並使網路更 robust</p>
<h2 id="架構">架構</h2>
<p><img src="https://i.imgur.com/Y95rNJS.png" /></p>
<p>實際的公式如下：</p>
<p><span class="math display">\[
H_l = \mathrm{ReLU}(b_lf_l(H_{l-1})+id(H_{l-1}))
\]</span> <span class="math inline">\(H_l\)</span> <span
class="math inline">\(H_{l-1}\)</span> 代表 Residual block
的結果，以及前一層的結果</p>
<p><span class="math inline">\(b\)</span> 的值只有 0 或
1，是一個隨機變數，代表這一個 block 是不是要 activate</p>
<p><span class="math inline">\(f\)</span> 代表經過 conv 層、BN、ReLU…
等的運算方向</p>
<p><span class="math inline">\(id\)</span> 代表 identity line，也就是
shortcut</p>
<p>架構如下圖：最後兩個方向的分流會合併，並且再經過一層 ReLU</p>
<p>當 <span class="math inline">\(b = 0\)</span> 時，公式就會變成：</p>
<p><span class="math display">\[
H_l = \mathrm{ReLU}(id(H_{l-1}))
\]</span> 公式中的 <span class="math inline">\(b\)</span>
有一定的「生存機率」，使得 <span class="math inline">\(b\)</span>
在此機率下為 1，也就是「通過 Block」</p>
<p>生存機率依以下公式生成：</p>
<p><span class="math display">\[
p_l = 1-\frac{l}{L}(1-p_L)
\]</span> <img src="https://i.imgur.com/Oja6Aqt.png" /></p>
<p><span class="math inline">\(p_l\)</span> 為第 <span
class="math inline">\(l\)</span> 層的機率</p>
<p><span class="math inline">\(L\)</span> 為 block (或稱層數)
的總數量</p>
<p><span class="math inline">\(p_L\)</span> 就代表最後一層的機率</p>
<p>注意的是 <span class="math inline">\(p_L\)</span>
為自定變數，以及第一層 <span class="math inline">\(l\)</span> 為
0，代表第一層一定不會省略掉，機率照著一定比例下降到最後一層</p>
<p>這樣子做的目的是因為淺層的網路所抓取到的特徵會一直被深層網路所使用，相比之下重要性大得許多，所以這些淺層不應該有太大的機率跳過</p>
<h2 id="結論">結論</h2>
<p>經由隨機的把 ResNet
中的一些層省略後，實驗證明，效果竟然變好一點點了</p>
<p>實驗結果如下圖：</p>
<p><img src="https://i.imgur.com/3Z4Tx2r.png" /></p>
<p>個人結論為，相較於 ResNet 多提供一個 shortcut
可以走的解法，Stochastic depth 更像是強制網路選擇 shortcut 的方法</p>
<p>同時經過實驗也證實，在 ResNet
中眾多層的網路中，有一些層數是沒學到任何東西、是多餘的</p>
<h2 id="transformer">Transformer</h2>
<p>主要會寫這一篇的原因是，在 timm 開源的程式中，ViT 及 Swin Transformer
都使用到了這個方法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))]  <span class="comment"># stochastic depth decay rule</span></span><br></pre></td></tr></table></figure>
<p>且在下面這一篇 2019 年的論文中 <a
href="https://arxiv.org/pdf/1909.11556.pdf">REDUCING TRANSFORMER DEPTH
ON DEMAND WITH STRUCTURED DROPOUT</a> 同時也提出類似的 LayerDrop
架構，透過實驗來證明 stochastic depth 的方法同樣可以應用在深層的
Transformer 上面</p>
<h2 id="reference">Reference</h2>
<h3 id="arxiv">arxiv</h3>
<p><a href="https://arxiv.org/pdf/1603.09382v3.pdf">Deep Networks with
Stochastic Depth</a></p>
<p><a href="https://arxiv.org/pdf/1909.11556.pdf">REDUCING TRANSFORMER
DEPTH ON DEMAND WITH STRUCTURED DROPOUT</a></p>
<h3 id="其餘心得文章">其餘心得文章</h3>
<p><a
href="https://www.cnblogs.com/zyxxmu/p/12788051.html">论文阅读：Reducing
Transformer Depth On Demand With Structured Dropout</a></p>
<p><a
href="https://blog.csdn.net/comway_Li/article/details/82228348">深度学习模型之——Stochastic
depth（随机深度）</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>網路模組</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: CoAtNet: Marrying Convolution and Attention for All Data Sizes - 使用 Depthwise Conv 來結合 CNN 與 Transformer</title>
    <url>/2021/10/21/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-CoAtNet-Marrying-Convolution-and-Attention-for-All-Data-Sizes-%E4%BD%BF%E7%94%A8-Depthwise-Conv-%E4%BE%86%E7%B5%90%E5%90%88-CNN-%E8%88%87-Transformer/</url>
    <content><![CDATA[<p>Google 繼提出 BotNet 後又提出新的 Transformer 網路
CoAtNet，並且在數學的公式上發現，Depthwise Convolution 是一個很好結合
CNN 與 Transformer 的點，將兩者公式結合得到刷新「分類」項目上的
SOTA，值得注意的是這篇論文目前並未開源。</p>
<p><a
href="https://arxiv.org/pdf/2106.04803.pdf">https://arxiv.org/pdf/2106.04803.pdf</a></p>
<p>keywords: CoAtNet、Depthwise Convolution <span id="more"></span></p>
<h2 id="introduction">1. Introduction</h2>
<p>作者發現在<strong>相同資料量及運算量下</strong>所有 Transformer based
的方法都不及 CNN 的效果，也可以換句話說 Transformer
只有在資料量多的前提下才能發揮它的強處。</p>
<p>作者認為這個問題最關鍵的點在於 Transformer 缺乏 inductive bias
的能力，因此才需要使用大量的資料來補足這個問題。</p>
<hr />
<p>什麼是 inductive
bias？下面這個討論區大家的回答都蠻可以參考的，而以下是我個人的想法</p>
<p><a
href="https://www.zhihu.com/question/264264203">https://www.zhihu.com/question/264264203</a></p>
<p>Inductive Bias 這個詞可以分成兩個部份來看：Induction
(歸納、推理)，指的是在資料中尋找共同性、尋找一個通用的規則。Bias
(偏見、誤差) 指的是對規則的偏好</p>
<p>在日常生活中的 Inductive
Bias，以颱風路徑預報為例子可為：觀察雲向、氣壓、衛星圖等資料來<strong>歸納</strong>出明颱風最有可能的路徑為何，但每個不同國家的氣象局對於每個因素都會有不同的<strong>偏好</strong>，進而使得每個國家預報出來的路徑都不相同。</p>
<p>對於深度學習 CNN 來說，透過 kernel 可以使 CNN
有<strong>歸納</strong>出 locality、spatial invariance
的特性，也就是局部特徵提取，和空間不變性
(特徵不管在圖的哪個地方都是同個特徵)。而對於找出來的不同特徵，會再給它們<strong>權重</strong>來選擇重要及不重要的特徵</p>
<p>因此 Inductive Bias 可以簡單的說是找特徵的能力。</p>
<hr />
<p>回到論文，作者認為之前論文提出結合的方法 (像是 CvT、Levit、T2T-ViT)
都過於生硬，都有點像直接把 CNN 的某個區塊直接併上
Transformer，因此作者提出
CoAtNet，試著從深度學習的兩個角度來考量：Generalization
(歸化能力)、Model Capacity
(模組的擬合能力)，看能不能找一個平衡點使合併後的網路最佳化。</p>
<h2 id="網路架構">2. 網路架構</h2>
<p>作者把如果最佳化合併兩網路分成兩個問題：</p>
<ol type="1">
<li>要怎麼最佳化合併</li>
<li>要怎麼最佳化堆疊合併後的網路</li>
</ol>
<h3 id="最佳化合併">最佳化合併</h3>
<p>作者要合併的 CNN 網路選擇同為 Google 提出的 MobileNet，理由有二：</p>
<ol type="1">
<li>MobileNet 與 Trasnformer 中的 FFN 都是使用了 inverted
bottleneck，也就是會先把維度放大再縮回原 size</li>
<li>MobileNet 中使用到了 Depthwise Conv，與 Transformer
相同的部份，兩者皆是<strong>一層一層的在定義空間中找出經權重的加總</strong>，只是
Transformer 定義為整張圖，而 Depthwise Conv 定義為一個 kernel
size。原文如下： &gt;&gt; a per-dimension weighted sum of values in a
pre-defined receptive field</li>
</ol>
<p><img src="https://i.imgur.com/K6d7uoC.png" alt="Image" /></p>
<p>MobileNet 公式可表式如下：</p>
<p><span class="math display">\[
y_i = \sum_{j\in\mathcal{L}(i)}w_{i-j}\odot x_j
\]</span></p>
<p><span class="math inline">\(x_i\)</span> <span
class="math inline">\(y_i\)</span> 代表第 <span
class="math inline">\(i\)</span> 個位置的輸入和輸出，<span
class="math inline">\(\mathcal{L}(i)\)</span> 表示一個 kernel size
的大小，<span class="math inline">\(\odot\)</span>
表示在定義空間中的內積</p>
<p>Self Attention 的公式如下：</p>
<p><span class="math display">\[
y_i = \sum_{j\in\mathcal{G}}
\underbrace{\frac{\mathrm{exp}(x_i^Tx_j)}{\sum_{k\in\mathcal{G}}\mathrm{exp}(x_i^Tx_k)}}_{A_{i,j}}x_j
\]</span></p>
<p><span class="math inline">\(\mathcal{G}\)</span> 代表整張圖。把
MobileNet 中的 <span class="math inline">\(w_{i-j}\)</span> 替換成 <span
class="math inline">\(A_{i,j}\)</span>，意義為找出 <span
class="math inline">\(x_i, x_y\)</span> 之間的相關性 (co-relation)</p>
<p>在融合兩公式前，來對比一下各自的優缺點：</p>
<ol type="1">
<li>Input-adaptive Weighting 輸入權重比較
<ul>
<li>MobileNet 中的 Depthwise Conv 權重計算是用 kernel (<span
class="math inline">\(w_{i-j}\)</span>)，特色是 kernel
中的值不會隨著不同層數的圖片而改變，也可說 kernel 是靜態的
(static)，與輸入圖片無關</li>
<li>Self-attention 是根據整張圖的 <span
class="math inline">\(QK^T\)</span>
做計算，每一個特徵層中的權重都不一樣，也可說 Self-Attention 是動態的
(dynamically) 尋找特徵。正是因為比 kernel 還要自由的原因，Self-Attention
更適合尋找空間中彼此的關系，同時也需要比較大的資料才能發揮，不然會很容易
overfitting</li>
</ul></li>
<li>Translation Equivariance 平移不變性
<ul>
<li>在 Conv 中，只關心 kernel 中的局部特徵，因此 Conv 有 translation
equivalence 平移不變性，而這個特性可以幫助 CNN 在小資料集中有更好的
Generalization 泛化能力，也就是尋找特徵的能力</li>
<li>而在 Transformer 中，以 ViT 為例子，ViT 使用了 absolution positional
embedding 絕對位置編碼，平移不變性消失了，這也是 Transformer
需要更大資料集的其中一個原因</li>
</ul></li>
<li>Global Receptive Field 全局感知野
<ul>
<li>CNN 中的感知野只限於 kernel
中，也可以說是局部感知野，或是也有人說這是 CNN 的 locality 特色</li>
<li>而 Transformer
一次看一整張圖片，屬於全局感知野，可以更有彈性的去尋找特徵，但代價就是運算量高，與圖片的大小呈平方關系</li>
</ul></li>
</ol>
<p>下圖是上述各優點的整理</p>
<p><img src="https://i.imgur.com/pgCjUSs.png" alt="Image" /></p>
<p>因此作者要把上面三點優點相互結合成一個新的網路，提出的方法為把 CNN
以及 Transformer 的局部感知野與全局感知野相加，也就是 kernel 以及
attention matrix 兩個部份相加，又分為在 softmax
前後相加得到下列兩個式子：</p>
<p><span class="math display">\[
y_i^{post} =
\sum_{j\in\mathcal{G}}(\frac{\mathrm{exp}(x_i^Tx_j)}{\sum_{k\in\mathcal{G}}\mathrm{exp}(x_i^Tx_k)}+w_{i-j})x_j
\]</span></p>
<p><span class="math display">\[
y_i^{pre} =
\sum_{j\in\mathcal{G}}\frac{\mathrm{exp}(x_i^Tx_j+w_{i-j})}{\sum_{k\in\mathcal{G}}\mathrm{exp}(x_i^Tx_k+w_{i-j})}x_j
\]</span></p>
<p>式子中的 <span
class="math inline">\(\sum_{k\in\mathcal{G}}\mathrm{exp}(x_i^Tx_k)\)</span>
指的就是 softmax，而來自 Conv 的 kernel <span
class="math inline">\(w_{i-j}\)</span>，分別加在 softmax 後，及 softmax
中</p>
<p>作者最後選擇 <span class="math inline">\(y_{pre}\)</span>
做為網路架構，原因是在 softmax 前加上 <span
class="math inline">\(w_{i-j}\)</span>
的意思更能符合，<strong>Self-attneion 除了考慮全局感知野外，還加上了
<span class="math inline">\(w_{i-j}\)</span>
局部感知野</strong>的感覺，也可以看成在 Self-attention 中加入了來自
<span class="math inline">\(w_{i-j}\)</span> 的平移不變特性。</p>
<h3 id="最佳化堆疊合併後的網路">最佳化堆疊合併後的網路</h3>
<p>設計好網路核心後，接下來要討論如何有效的堆疊 CNN 與 Transformer。由於
Self-Attention
的計算量偏大，要在網路效果及效能間做出取捨，因此作者提出了以下三種解決方案</p>
<ol type="1">
<li>先用 CNN 做幾次 downsampling，再把比較小的特徵圖丟給
Transformer</li>
<li>只使用 local attention，把 Self-Attention 中的 <span
class="math inline">\(\mathcal{G}\)</span> 改成跟 kernel 一樣大小</li>
<li>把原本的 Self-Attention 改成線性
Self-Attention，使時間複雜度變為線性</li>
</ol>
<p>作者經實驗證實，2 3 的方法會影響到網路效能，因此最終方案採用
1，詳細流程如下：</p>
<p>downsampling 的做法可分為兩種：</p>
<ol type="1">
<li>像 ViT 一樣直接切成 16x16，記做 <span
class="math inline">\(ViT_{REL}\)</span></li>
<li>使用 CNN 的 stride 2 兩倍兩倍往下</li>
</ol>
<p>整個網路分為 4 個 stage，又 Conv 找特徵的能力比較強一定要在
Transformer 之前，所以一共有以下五種情況：</p>
<ol type="1">
<li><span class="math inline">\(ViT_{REL}\)</span></li>
<li><span class="math inline">\(C-C-C-C\)</span></li>
<li><span class="math inline">\(C-C-C-T\)</span></li>
<li><span class="math inline">\(C-C-T-T\)</span></li>
<li><span class="math inline">\(C-T-T-T\)</span></li>
</ol>
<p>比較以上五種網路的指標分別為</p>
<ol type="1">
<li>歸化能力 (generalization)
<ul>
<li>在比較訓練損失 (training loss) 與驗證集正確率 (evaluation accuracy)
之間的差距，在兩模型有相同訓練損失的前提下，有比較高的驗證集正確率代表有更好的歸化能力</li>
<li>可理解成網路遇到<strong>未看過資料</strong>尋找重點特徵的能力</li>
</ul></li>
<li>模型擬合能力 (model capacity)
<ul>
<li>給一個超大的訓練集，確保網路絕對不會出現 overfitting
的現象，看哪一個網路<strong>收斂</strong>的速度最快，也就是學習力最好的網路</li>
</ul></li>
</ol>
<h4 id="歸化能力-generalization-實驗">歸化能力 (generalization)
實驗</h4>
<p><img src="https://i.imgur.com/TkpnkrN.png" alt="Image" /></p>
<p>直接用實驗得出以下結果</p>
<p><span class="math display">\[
C\textrm{-}C\textrm{-}C\textrm{-}C \approx
C\textrm{-}C\textrm{-}C\textrm{-}T \geq
C\textrm{-}C\textrm{-}T\textrm{-}T \gt
C\textrm{-}T\textrm{-}T\textrm{-}T \gg ViT_{REL}
\]</span></p>
<h4 id="模型擬合能力-model-capacity-實驗">模型擬合能力 (model capacity)
實驗</h4>
<p><img src="https://i.imgur.com/Nr0QVt4.png" alt="Image" /></p>
<p>直接用實驗得出以下結果</p>
<p><span class="math display">\[
C\textrm{-}C\textrm{-}T\textrm{-}T \approx
C\textrm{-}T\textrm{-}T\textrm{-}T \gt ViT_{REL} \gt
C\textrm{-}C\textrm{-}C\textrm{-}T \gt
C\textrm{-}C\textrm{-}C\textrm{-}T
\]</span></p>
<p>綜合以上兩個實驗結果發現 <span
class="math inline">\(C\textrm{-}C\textrm{-}T\textrm{-}T \approx
C\textrm{-}T\textrm{-}T\textrm{-}T\)</span>
兩個結果相當，於是作者最後再把 ImageNet-1K 加上 30 個 epochs
看看誰比較好</p>
<p><img src="https://i.imgur.com/67DFOUq.png" alt="Image" /></p>
<p>最後選擇 <span
class="math inline">\(C\textrm{-}C\textrm{-}T\textrm{-}T\)</span> 作為
CoAtNet 的主架構</p>
<h3 id="網路架構-1">網路架構</h3>
<p><img src="https://i.imgur.com/ucvJj3k.png" alt="Image" /></p>
<p>網路架構包括 5 個 stage</p>
<ul>
<li>stage S0：兩層簡單的 CNN 做低階特徵選取</li>
<li>stage S1：使用 MobileNet with SE (Squeeze-Excitation)</li>
<li>stage S1-S4：照 <span
class="math inline">\(C\textrm{-}C\textrm{-}T\textrm{-}T\)</span>
依序堆疊</li>
</ul>
<h2 id="experiments">3. Experiments</h2>
<h3 id="實驗一coatnet-家族">實驗一、CoAtNet 家族</h3>
<p>依照每個 stage 重複層數不同來區分</p>
<p><img src="https://i.imgur.com/PEvvF1s.png" alt="Image" /></p>
<h3 id="實驗二與-sota-的比較">實驗二、與 SOTA 的比較</h3>
<p>較大型的 CoAtNet 有超越 NFNet 一點點</p>
<p><img src="https://i.imgur.com/BRGLUys.png" alt="Image" /></p>
<h3 id="實驗三flops-運算量的比較">實驗三、FLOPs 運算量的比較</h3>
<p><img src="https://i.imgur.com/X0RNPtu.png" alt="Image" /></p>
<h3 id="實驗四params-參數量的比較">實驗四、Params 參數量的比較</h3>
<p><img src="https://i.imgur.com/owvIlTA.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>本文提出 Self-Attention 可以自然的與 Depthwise Conv
結合在一起，以更數學的角度來結合兩公式。</p>
<p>其次就是找到適合的堆疊方法，大概上就是 Conv 與 Transformer
各占一半是效果最好的，且 Conv 要先於 Transformer 做運算</p>
<p>注意！這篇論文目前沒開源，網路上找到的這個 github <a
href="https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/model/attention/CoAtNet.py">https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/model/attention/CoAtNet.py</a>
是有一些 issue 的，畢竟不是官方的 code…，在使用前可能要多多留意一下
XD</p>
<h2 id="reference">Reference</h2>
<p>https://www.zhihu.com/question/264264203</p>
<p>https://zhuanlan.zhihu.com/p/385106095</p>
<p>https://jishuin.proginn.com/p/763bfbd5eae9</p>
<p>https://github.com/xmu-xiaoma666/External-Attention-pytorch/blob/master/model/attention/CoAtNet.py</p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: SwinIR: Image Restoration Using Swin Transformer</title>
    <url>/2021/12/02/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-SwinIR-Image-Restoration-Using-Swin-Transformer/</url>
    <content><![CDATA[<p>論文網址：<a
href="https://arxiv.org/pdf/2108.10257.pdf">https://arxiv.org/pdf/2108.10257.pdf</a></p>
<p>這是基於 Swin Transformer 應用在 Super Resolution 的研究，網路稱
SwinIR，實驗證明 Backbone 使用 Transformer 也能達到不錯的效果</p>
<p>最後效果甚至成為當時的 SOTA，改進了
0.14∼0.45dB，且參數使用量相較下少了 67% (拜層數不深所賜)</p>
<p>keywords: Swin Transformer、SwinIR <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<h3 id="cnn-vs-transformer">CNN vs Transformer</h3>
<p>以前 CNN-based 的 SR 網路，重點常常聚焦在 Residual connection
上的改良，以及深層網路的堆疊</p>
<p>代價就是參數使用量偏高</p>
<p>而已 Transformer-based 的
SwinIR，因層數偏少的因素，相同效果下參數使用量明顯小了一些，如下圖：</p>
<p><img src="https://i.imgur.com/9is3YjZ.png" alt="Image" /></p>
<p>除了參數變少外，當然一定要提一下的…就是 Transformer 的 Global
Recepitve Filed，當然有了這個資訊一定是補足了 CNN 一些比較不足的地方</p>
<h3 id="vit-vs-swint">ViT vs SwinT</h3>
<p>如果直接把 ViT 拿來做 SR 會發生什麼事？由於 ViT 各 Patch
之間互相獨立，互相不做運算，因此在 patch 邊邊的像素會出現邊界現象</p>
<p>而如果 patch 彼此有 overlaping 的話，運算量會增加</p>
<p>要怎麼在不增加運算量的前提下解決這個問題呢？解答就是 Swin Transformer
所提出的 Shifted windows 方法</p>
<h3 id="swinir">SwinIR</h3>
<p>因此作者提出以 SwinT 為基準的 SR 網路</p>
<p>分為三個階段：</p>
<ol type="1">
<li>淺層特徵提取</li>
<li>深層特徵提取</li>
<li>影像 upsampling 至高解析度 (image reconstruction)</li>
</ol>
<h2 id="網路架構">網路架構</h2>
<p><img src="https://i.imgur.com/Y7GR1Oi.png" alt="Image" /></p>
<h3 id="淺層特徵提取">淺層特徵提取</h3>
<p><span class="math display">\[
F_0 = H_{SF} (I_{LQ})
\]</span></p>
<p><span class="math inline">\(I_{LQ}\)</span> 代表 input 一張 Low
Quality 的影像</p>
<p><span class="math inline">\(H_{SF}\)</span> 代表一個 3x3 conv 負責
Shallow Feature</p>
<h3 id="深層特徵提取">深層特徵提取</h3>
<p><span class="math display">\[
F_{DF} = H_{DF}(F_0)
\]</span></p>
<p><span class="math inline">\(H_{DF}\)</span> RSTB Block 負責 Deep
Feature</p>
<h3 id="rstb">RSTB</h3>
<p>RSTB 的全名是 residual Swin Transformer blocks，由 <span
class="math inline">\(K\)</span> 個 Swin Transformer 以及一個 3x3 conv
所組成</p>
<p><span class="math display">\[
\begin{gathered}
    F_i = H_{RSTBi}(F_{i-1}), i=1,2,...,K\\
    F_{DF} = H_{CONV}(F_K)
\end{gathered}
\]</span></p>
<p><img src="https://i.imgur.com/qOjA61X.png" alt="Image" /></p>
<p>STL 代表 Swin Transformer Layer，與原論文架構相同，這邊就不多講了</p>
<h3 id="image-reconstruction">image reconstruction</h3>
<p><span class="math display">\[
I_{RHQ} = H_{REC}(F_0+F_{DF})
\]</span></p>
<p><span class="math inline">\(I_{RHQ}\)</span> 代表 reconstruct
high-quality image</p>
<p><span class="math inline">\(H_{REC}\)</span>
會接兩個參數：淺層特徵與深層特徵，兩個不同特徵一起當 input</p>
<p>而 upsampling 使用的方法則是 pixelshuffle</p>
<p>另外對於一些圖片不需要 upsampling
的應用，像是去噪、去雨…公式改成以下：</p>
<p><span class="math display">\[
I_{RHQ} = H_{SwinIR}(I_{LQ}) + I_{LQ}
\]</span></p>
<h3 id="loss-function">Loss function</h3>
<p>損失函數則是簡單的 L1 loss</p>
<p><span class="math display">\[
\mathcal{L} = ||I_{RHQ} - I_{HQ}||_1
\]</span></p>
<h2 id="experiments">Experiments</h2>
<h3 id="channel-數rstb-層數stl-層數數量實驗">channel 數、RSTB 層數、STL
層數數量實驗</h3>
<p><img src="https://i.imgur.com/6OIFF9o.png" alt="Image" /></p>
<p>最後選擇 channel 180 個 (Source code 上好像是 96 個)</p>
<p>RSTB、STL 各 6 層，使得網路相對小</p>
<h3 id="patch-size-的影響訓練集大小的影響">patch size
的影響、訓練集大小的影響</h3>
<p>可發現 patch size 越大，SwinIR 效果越好</p>
<p><img src="https://i.imgur.com/6XiySR4.png" alt="Image" /></p>
<h3 id="rstb-中的-residual-connection-以及-最後一個-cnn-的選擇">RSTB
中的 residual connection 以及 最後一個 CNN 的選擇</h3>
<p><img src="https://i.imgur.com/TAg6zhx.png" alt="Image" /></p>
<p>有 residual 比沒 residual 好、3x3 比 1x1 來得好</p>
<p>普通 3x3 與 inverted-bottlenect 3x3 差不多，後者參數少，效果差一些些
(合理)</p>
<h3 id="sota-比較">SOTA 比較</h3>
<p><img src="https://i.imgur.com/SXPUFfs.png" alt="Image" /></p>
<h3 id="一些實驗結果">一些實驗結果</h3>
<p><img src="https://i.imgur.com/HhoTifa.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/Lo8cuiy.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/12DaV9G.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>Transformer 應用在 SR 上，因著 Swin Transformer
的成功，也應用的非常順利</p>
<p>本篇論文其實沒什麼特別的貢獻，大概就是側面證明了 Swin 的厲害</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://arxiv.org/pdf/2108.10257.pdf">https://arxiv.org/pdf/2108.10257.pdf</a></p>
<p><a
href="https://arxiv.org/pdf/2103.14030.pdf">https://arxiv.org/pdf/2103.14030.pdf</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Vision Transformer 演化史: CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows</title>
    <url>/2021/12/03/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-CSWin-Transformer-A-General-Vision-Transformer-Backbone-with-Cross-Shaped-Windows/</url>
    <content><![CDATA[<p>論文網址：<a
href="https://arxiv.org/pdf/2107.00652.pdf">https://arxiv.org/pdf/2107.00652.pdf</a></p>
<p>Swin 原班人馬在 2021 7 月提出更進一步的網路架構 CSWin
Transformer，提出全新的 <strong>C</strong>ross-<strong>S</strong>haped
<strong>Win</strong>dow self-attention
有著更好的特徵截取能力，以及更少的網路運算量</p>
<p>更提出新的位置資訊架構 LePE (Locally-enhanced Positional
Encoding)，相較於原本的絕對位置 (APE) 或是相對位置 (RPE)
有著更好的表現</p>
<p>keywords: CSwin、LePE <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p>Self-Attention 的運算量過大，這是眾所皆知的事實，因此 Swin
Transformer 藉由把 Patch 再切成更小的 Window 嘗試減少運算量，同時為了使
window 與 window 之間有關聯，Swin 把整個流程切成兩步 W-MSA 與
SW-MSA，藉由<strong>兩次</strong>不同位置的 window 來達成像素的關聯</p>
<p>而 CSwin
再進一步減少運算量的同時還加強了截取特徵的能力，使用有別於原本
Self-Attention 的 Cross-Shaped Window Self-Attention</p>
<p><img src="https://i.imgur.com/74I4gl5.png" alt="Image" /></p>
<p>如上圖，CSwin 分成垂直、水平 Attention
來取得像素間的關聯，且是利用<strong>把 multi head
分成兩半</strong>來達成，一半負責垂直部份，一半負責水平部份。這樣做的好處是可以在<strong>一步</strong>就完成不同
patch 像素間的關聯，而作者後續的實驗也證明 CSwin 相比 Swin
可以在使用更少的層達到相同的效果</p>
<p>上圖 b 則是類似 ViT 的方法全部圖片都做 Self-Attention，c 則是 Swin
的方法，e 與本文的 CSwin 有點類似，不同的點在於 e
是先做水平再做垂直的，與本文利用 head 一次做兩步有些許的差別</p>
<h2 id="網路架構">網路架構</h2>
<p>網路架構圖如下圖所示：</p>
<p><img src="https://i.imgur.com/h6GmDDy.png" alt="Image" /></p>
<p>與 Swin 架構類似，首先會經過 convolutional token
embedding，也就是利用 7x7 conv stride 4 來得到 W/4 H/4 個 Patch。其實
ViT 也是利用 conv 來達來劃分 Patch 的目的，但是 ViT 的 conv 沒有
overlap，而 CSwin 這邊則有，有 overlap 的效果比沒有要好上一些</p>
<p>網路主架構分為四個 Stage，每個 Stage 會使用 3x3 conv stride 2 像 CNN
一樣不斷的減少圖片大小，同時增加特徵圖數量</p>
<p>本論文最特別的地方提出了 CSwin Self-Attetion，與傳統的 Self-Attetion
有著以下兩點的不同：</p>
<ol type="1">
<li>把 Self-Attention 換成了 Cross-Shaped Windows Self-Attention</li>
<li>為了增強 local inductive bias (局部的歸納偏置能力)，提出了全新的
LePE 架構</li>
</ol>
<h3 id="cross-shaped-window-self-attention">Cross-Shaped Window
Self-Attention</h3>
<p><img src="https://i.imgur.com/iSPamsH.png" alt="Image" /></p>
<p>為了提高局部像素之間的關系 (增加 Window
的大小)，同時顧及到運算量不要過大 (像 ViT
那樣與圖片大小呈平方關系)，CSWin 所使用的方法是<strong>利用水平及垂直的
stripe window 來做 Self-Attention</strong></p>
<p>先來看水平的 stripe</p>
<p>每個 window 可表示成 <span class="math inline">\(X\)</span>，而 <span
class="math inline">\(X\)</span> 的大小定義為 <span
class="math inline">\(sw \times W\)</span>，<span
class="math inline">\(sw\)</span> 代表為水平 window 的寬度，<span
class="math inline">\(W\)</span> 即為圖片的總寬度</p>
<p>每張圖片可以分割成相同大小的 <span class="math inline">\(M\)</span>
個 <span class="math inline">\(X\)</span>，且每個 <span
class="math inline">\(X\)</span> 不重疊，所以 <span
class="math inline">\(M=H/sw\)</span></p>
<p><span class="math display">\[
\begin{gathered}
X=[X^1,X^2,...,X^M] \quad \mathrm{where}\quad X^i\in
\mathbb{R}^{(sw\times W)\times C}\quad \mathrm{and} \quad M=H/sw
\end{gathered}
\]</span></p>
<p>同時假設這些特徵來自第 <span class="math inline">\(k\)</span> 個
head</p>
<p>接著把每個 <span class="math inline">\(X\)</span> 也就是每個 window
彼此之間做 Self-Attention。</p>
<p><span class="math display">\[
\begin{gathered}
Y^i_k = \mathrm{Attention}(X^iW^Q_k,X^iW^K_k,X^iW^V_k),\quad
\mathrm{and} \quad i=q,...,M\\
W^Q_k,W^K_k,W^V_k \in\mathbb{R}^{C\times d_k}
\end{gathered}
\]</span></p>
<p>最後就得到的水平 (Horizontal) 方向的 CSwin 了</p>
<p><span class="math display">\[
\mathrm{H-Attention_k}(X)= [Y^1_k,T^2_k,...,T^M_k]
\]</span></p>
<p>而垂直 (Vertical) 方向也是同理，公式與上面基本一樣，只有 <span
class="math inline">\(M\)</span> 的部份改為 <span
class="math inline">\(M=W/sw\)</span></p>
<p>把 multi-head 的數量 <span class="math inline">\(K\)</span>
分成兩半，一半給水平，一半給垂直，得到最後下列式子：</p>
<p><span class="math display">\[
\begin{gathered}
\mathrm{CSWin-Attention}(X) =
\mathrm{Concat}(\mathrm{head}_1,...,\mathrm{head}_K)W^O
\end{gathered}
\]</span> <span class="math display">\[
\mathrm{where} \quad \mathrm{head}_k =\left\{
  \begin{aligned}
    \mathrm{H-Attention}_k(X) \quad k &amp;= 1,...,K/2\\
    \mathrm{V-Attention}_k(X) \quad k &amp;= K/2+1,...,K
  \end{aligned}
\right.
\]</span></p>
<h3 id="計算複雜度與-sw-的變化">計算複雜度與 sw 的變化</h3>
<p>CSwin 的計算複雜度如下：</p>
<p><span class="math display">\[
\Omega(\mathrm{CSWin-Attention}=HWC\times(4C+sw\times H+sw\times W))
\]</span></p>
<p>詳細推導過程可以參考以下這個網頁：</p>
<p><a
href="https://zhuanlan.zhihu.com/p/388165447">https://zhuanlan.zhihu.com/p/388165447</a></p>
<p>而複雜度的結論為：複雜度與 sw 有關，如果 sw 遠小於
HW，則呈一次方關系，如果 sw 大，則呈兩次方關系</p>
<p>因此，CSwin 一共分為四個階段，每當網路越來越深的時候，sw
的值也會隨之變化：<strong>淺層的 sw 比較小，深層的 sw 的比較大</strong>
(論文中提出的變化為：[1, 2, 7, 7] 皆為輸入圖片 224 的倍數)</p>
<p>會這麼做的用意是原圖的解析度大，如果 sw 大的話，計算量會非常大，而在
CSwin 中每個 Stage 結束後都會用 conv
來解少圖片的解析度，因此到了深增時圖片解析度相對小，就可以用比較大的 sw
來做計算了</p>
<p>這麼做的第二個優點是淺層的關注度比較偏局部，而深層的關注度就比較全局，這一點與
CNN 非常類似，但與 ViT 的想法相反。</p>
<p>我覺得…自從 Transformer 從關注一個 Patch 到關注一個 Window
後，Transformer 的初始關注並沒像 ViT 的那麼全局了，轉而像 CNN
一樣從局部再慢慢的到全局</p>
<h3 id="lepe">LePE</h3>
<p><img src="https://i.imgur.com/C1dtNEi.png" /></p>
<p>作者比較了 APE (絕對位置)、RPE (相對位置) 整理如上表，APE 是加在
Self-Attention 前，RPE 是加在 Self-Attention 之中</p>
<p>而作者提出的 LePE 如圖最右邊，將位置訊息加到 Value 中，再將結果加到
Self-Attention 的結果中，公式如下：</p>
<p><span class="math display">\[
\mathrm{Attention}(Q,K,V)=\mathrm{SoftMax}(QK^T/\sqrt{d})V+\mathrm{DWConv}(V)
\]</span></p>
<p>作者提到這邊使用 Depth-wise Conv 的原因有二：</p>
<ol type="1">
<li>相較於 Conv 計算量較少</li>
<li>位置編碼只會和當前同一張圖周圍有像素有關聯，不會與其它特徵圖之間有關聯</li>
</ol>
<p>結論來看 CSWin Transformer block 是由一個十字形的 Attention
window，以及一個 Depth-wise conv，兩個分支合併而成的</p>
<h3 id="cswin-transformer-block">CSWin Transformer Block</h3>
<p><img src="https://i.imgur.com/W6cJ8aF.png" /></p>
<p>一個 Block 與 ViT 相同，這邊就不再多解釋了</p>
<h2 id="experiments">Experiments</h2>
<h3 id="網路模型種類">網路模型種類</h3>
<p>一種分為 4 個不同大小的模型</p>
<p><img src="https://i.imgur.com/EuSDNIH.png" /></p>
<h3 id="相同模型大小比較">相同模型大小比較</h3>
<p>在參數量差不多的情況下做比較，發現當網路模型越大，Transformer-based
的效果比 CNN-based 好上一些些</p>
<p><img src="https://i.imgur.com/WbonUTn.png" /></p>
<h3 id="imagenet-1k-分類比較">ImageNet-1K 分類比較</h3>
<p>個人覺得分類的榜快刷不動了…大概也就好那一點點</p>
<p><img src="https://i.imgur.com/jZBUDW8.png" /></p>
<h3 id="coco-偵測比較">COCO 偵測比較</h3>
<p>偵測的結果主要是和 Swin 來比，可發現效果好上 1.5 個點，好上不少</p>
<p><img src="https://i.imgur.com/9nINvYM.png" /></p>
<h3 id="ade20k-語意分割比較">ADE20K 語意分割比較</h3>
<p>可發現 CSwin 在分割項目上超強，直接超過了 2 個點以上</p>
<p><img src="https://i.imgur.com/5HQaFQr.png" /></p>
<h3 id="其它一些技巧的相互比較實驗">其它一些技巧的相互比較實驗</h3>
<p>動態調整 <span
class="math inline">\(sw\)</span>、同時算平行垂直、網路初期卷積 kernel
重疊、Deep-Narrow</p>
<p>以上四個 Tricks 是 CSwin 效果好的主因</p>
<p><img src="https://i.imgur.com/RD5zQgy.png" /></p>
<h2 id="結論">結論</h2>
<p>CSwin 在 Swin 的成功下進一步增加效果且減少運算量，算是 Swin
家族的一個重大優化</p>
<p>提出的 LePE 也很值得讓人研究，倒底如何加入 PE 才是最好的做法呢？</p>
<p>水平垂直平行化處理的觀念也很創新，那是不是可以再把 head
多切分成更多塊呢？</p>
<p>不論如何，雖然分類的榜已經快刷不動了，但看起來 Transformer
的強項是在分割阿</p>
<h2 id="reference">Reference</h2>
<h3 id="arxiv">Arxiv</h3>
<p><a
href="https://arxiv.org/pdf/2107.00652.pdf">https://arxiv.org/pdf/2107.00652.pdf</a></p>
<h3 id="知乎大神們">知乎大神們</h3>
<p><a
href="https://bbs.cvmart.net/articles/5075">https://bbs.cvmart.net/articles/5075</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/388165447">https://zhuanlan.zhihu.com/p/388165447</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>nvidia driver、cuDNN、CUDA 安裝踩坑心得</title>
    <url>/2021/12/20/nvidia-driver-%E5%AE%89%E8%A3%9D%E8%B8%A9%E5%9D%91%E5%BF%83%E5%BE%97/</url>
    <content><![CDATA[<p>實驗室電腦 CUDA
版本各種跑掉，花了一個禮拜的時間，才上網找出下面的心得…</p>
<p>keywords: nvidia driver、docker <span id="more"></span></p>
<h3 id="移除舊版本的-driver">移除舊版本的 driver</h3>
<ul>
<li><p>當時用 apt-get 安裝驅動程式 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia*</span><br><span class="line">apt-get autoremove</span><br></pre></td></tr></table></figure></p></li>
<li><p>如果是用 CUDA 來安裝的話 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-uninstall</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h3 id="前置作業">前置作業</h3>
<ul>
<li><p>加入顯卡 ppt <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:graphics-drivers</span><br></pre></td></tr></table></figure></p></li>
<li><p>慣例的套件更新 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt upgrade</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h3 id="安裝-nvidia-driver">安裝 Nvidia driver</h3>
<ul>
<li><p>在開始前：重開機治百病 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure></p></li>
<li><p>找出目前支援的 GPU driver 版本 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ubuntu-drivers list</span><br></pre></td></tr></table></figure></p></li>
<li><p>安裝對應版本的 driver (2022/07/07 為 nvidia-driver-510)
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt install nvidia-driver-VERSION_NUMBER_HERE</span><br></pre></td></tr></table></figure></p></li>
<li><p>檢查是否安裝成功 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h3 id="安裝-cuda">安裝 CUDA</h3>
<ul>
<li><p>到下面的網址找到對應的版本</p></li>
<li><p>https://developer.nvidia.com/cuda-toolkit-archive</p></li>
<li><p>照著以下選項選擇 <img src="https://i.imgur.com/BddnTAH.png"
alt="Image" /></p></li>
<li><p>照著生出來的指令一行一行 copy paste <img
src="https://i.imgur.com/seCnIG7.png" alt="Image" /></p></li>
</ul>
<h3 id="安裝-cudnn">安裝 cuDNN</h3>
<ul>
<li><p>先去官網下載對應的版本 (麻煩的是還要註關 nvidia
帳號，並登入…)</p></li>
<li><p>https://developer.nvidia.com/rdp/cudnn-download</p></li>
<li><p>選對應的 cuDNN 版本，並選擇對應的作業系統 <img
src="https://i.imgur.com/rSH7ZKg.png" alt="Image" /></p></li>
<li><p>有桌面環境，下載完成後直接按兩下就安裝了</p></li>
<li><p>如果沒有，輸入指令 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i YOUR_VERSION.deb</span><br></pre></td></tr></table></figure></p></li>
</ul>
<h3 id="container-toolkit">container-toolkit</h3>
<ul>
<li>可以來在 docker container 中使用 CUDA <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - \</span><br><span class="line">   &amp;&amp; curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br></pre></td></tr></table></figure></li>
<li>安裝 nvidia-docker2 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y nvidia-docker2</span><br></pre></td></tr></table></figure></li>
<li>重新起動 docker <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure></li>
<li>來使用 nvidia/cuda 來測試是不是成功 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="在-docker-中使用-cuda">在 docker 中使用 cuda</h3>
<ul>
<li>先去 dockerhub pull 下來 pytorch/pytorch 的 image</li>
<li>通常以 xxx-runtime 為主</li>
<li>https://hub.docker.com/r/pytorch/pytorch/tags</li>
<li>需特別注意 cuda 及 cuDNN 的版本是否相符 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker pull pytorch/pytorch:...</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="container-command">container command</h3>
<ul>
<li>把 image run 起來 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -itd -v /home/ipvr/mushding:/workspace --name mushding_container --gpus all pytorch/pytorch:...</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">參數介紹：</span><br><span class="line">-d 背景執行</span><br><span class="line">-it attach mode</span><br><span class="line">-v 設定 volume</span><br><span class="line">--name container 名字</span><br><span class="line">--gpus all 重要！！！不然裡面不會有 CUDA</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-v volume 是可以把你主機本地的資料，鏡像到 docker 的虛擬環境上面</span><br><span class="line">分為兩個部分，由 「:」 冒號分開</span><br><span class="line">左邊是本地的資料夾名稱</span><br><span class="line">右邊是 docker 虛擬環境的資料夾名稱</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="給-docker-root-權限">給 docker root 權限</h3>
<ul>
<li>通常 docker 安裝完以後，如果想要下 docker 的指令都要搭配 sudo
才可使用</li>
<li>因為 docker 的服務基本上都是以 root
的身分在執行的，所以目前的使用者身分沒有權限去存取 docker engine</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo groupadd docker</span><br><span class="line">sudo usermod -aG docker $USER</span><br></pre></td></tr></table></figure>
<ul>
<li><p>最後需要退出重新登錄後才會生效</p></li>
<li><p>如果都還是不行：手動修改權限</p></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo chmod 777 /var/run/docker.sock</span><br></pre></td></tr></table></figure>
<h3
id="如果重開機遇到-nvidia-smi-has-failed-because-it-couldnt-communicate-with-the-nvidia-driver.-make-sure-that-the-latest-nvidia-driver-is-installed-and-running.">如果重開機遇到
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA
driver. Make sure that the latest NVIDIA driver is installed and
running.</h3>
<ul>
<li>安裝 DKMS <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get install dkms</span><br></pre></td></tr></table></figure></li>
<li>接著去通過以下方法找到對應的 nvidia 版本</li>
<li>(在最後一行 nvidia-...) <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /usr/src</span><br><span class="line">ls</span><br></pre></td></tr></table></figure></li>
<li>重新生成對應的 nvidia 驅動 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo dkms install -m nvidia -v &lt;你的版本路&gt;</span><br></pre></td></tr></table></figure></li>
<li>就成功可以下 nvidia-smi 囉</li>
</ul>
<h3 id="could-not-select-device-driver-with-capabilities-gpu">could not
select device driver "" with capabilities: [[gpu]]</h3>
<p>首先要重新安裝 nvidia-container-runtime 或
nvidia-docker2，版本跑掉了</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install -y nvidia-container-toolkit</span><br><span class="line">sudo apt-get install -y nvidia-docker2</span><br></pre></td></tr></table></figure>
<p>如果在安裝 nvidia-docker2 出現
<code>Unable to locate package nvidia-docker2 when installing using apt-get</code></p>
<p>跑以下指令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -s -L https://nvidia.github.io/nvidia-container-runtime/gpgkey | \</span><br><span class="line">  sudo apt-key add -</span><br><span class="line">distribution=$(. /etc/os-release;echo $ID$VERSION_ID)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-container-runtime/$distribution/nvidia-container-runtime.list | \</span><br><span class="line">  sudo tee /etc/apt/sources.list.d/nvidia-container-runtime.list</span><br><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
<p>記得要重新起動 docker</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure>
<p><a
href="https://github.com/NVIDIA/nvidia-docker/issues/1034#issuecomment-520282450">could
not select device driver "" with capabilities: gpu</a></p>
<p><a href="https://nvidia.github.io/nvidia-docker/">出現 Unable to
locate package nvidia-docker2 when installing using apt-get</a></p>
]]></content>
      <categories>
        <category>雜開發心得</category>
      </categories>
      <tags>
        <tag>nvidia driver</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 學習筆記</title>
    <url>/2021/12/20/Docker-%E5%AD%B8%E7%BF%92%E7%AD%86%E8%A8%98/</url>
    <content><![CDATA[<p>Docker
真是個好東西，網路前後端開發，實驗室電腦環境配置，都變得超好管理的</p>
<p>keywords: Docker <span id="more"></span></p>
<h2 id="下載-docker-docker-compose-on-rpi">下載 docker &amp;
docker-compose on Rpi</h2>
<ul>
<li>下載 docker <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">curl -sSL https://get.docker.com | sh</span><br></pre></td></tr></table></figure></li>
<li>把 docker 的權限提高 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo usermod -aG docker <span class="variable">$USER</span></span><br></pre></td></tr></table></figure></li>
<li>下載一些有的沒的 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libffi-dev libssl-dev</span><br></pre></td></tr></table></figure></li>
<li>下載 docker-compose <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo pip3 install docker-compose</span><br></pre></td></tr></table></figure></li>
<li>https://dev.to/rohansawant/installing-docker-and-docker-compose-on-the-raspberry-pi-in-5-simple-steps-3mgl</li>
</ul>
<h2 id="什麼是-dockerfile">什麼是 dockerfile</h2>
<ul>
<li>Dockerfile 是一個包含指令的文字檔，可以根據這個檔案去呼叫指令以及
assemble image 檔</li>
<li>Docker 可以根據 dockerfile 自動 build images</li>
</ul>
<h2 id="docker-三個名詞">docker 三個名詞</h2>
<ul>
<li>image
<ul>
<li>build 好的一個包</li>
<li>可以自己用 docker build 做一個自己的 image</li>
<li>也可以 docker pull 從 docker hub 上載別人整理好的 image</li>
</ul></li>
<li>container
<ul>
<li>在電腦上跑起來的一個環境，可以同時有多個 container 執行</li>
</ul></li>
<li>volume
<ul>
<li>在 container 中的儲存單位，可以和電腦中的資料夾共通</li>
</ul></li>
</ul>
<h2 id="dockerfile-格式詳解">dockerfile 格式詳解</h2>
<ul>
<li>做一個 dockerfile <figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Use a lighter version of Node as a parent image</span></span><br><span class="line"><span class="keyword">FROM</span> mhart/alpine-node:<span class="number">8.11</span>.<span class="number">4</span></span><br><span class="line"><span class="comment"># Set the working directory to /client</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /client</span></span><br><span class="line"><span class="comment"># copy package.json into the container at /client</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> package*.json /client/</span></span><br><span class="line"><span class="comment"># install dependencies</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> npm install</span></span><br><span class="line"><span class="comment"># Copy the current directory contents into the container at /client</span></span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . /client/</span></span><br><span class="line"><span class="comment"># Make port 3000 available to the world outside this container</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">3000</span></span><br><span class="line"><span class="comment"># Run the app when the container launches</span></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [<span class="string">&quot;npm&quot;</span>, <span class="string">&quot;start&quot;</span>]</span></span><br></pre></td></tr></table></figure></li>
<li>COPY 如果要連資料夾一起複製的話
<ul>
<li>後面要再加一次自己 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">COPY dir something/dir</span><br></pre></td></tr></table></figure> ## docker 指令 ### image 指令</li>
</ul></li>
<li>Dockerfile build 出 image <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker build -t simple-express-server . </span><br></pre></td></tr></table></figure></li>
<li>看有多少個 image <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker image ls</span><br></pre></td></tr></table></figure></li>
<li>把 docker run 起來 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -d -p 3000:8080 simple-express-server</span><br></pre></td></tr></table></figure> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">把 docker 中的 8080 port 打到外面的 3000 port</span><br><span class="line">-d -&gt; 在背景中執行</span><br></pre></td></tr></table></figure> <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -it &lt;image&gt; /bin/bash</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">進入 image 中</span><br></pre></td></tr></table></figure></li>
<li>看看背景中的 image 訊息 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker logs --tail 50 --follow --timestamps &lt;container&gt;</span><br></pre></td></tr></table></figure></li>
<li>remove docker image <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker rmi &lt;image_name&gt;</span><br></pre></td></tr></table></figure> ### container 指令</li>
<li>看所有正在執行中的 container <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker ps -a</span><br></pre></td></tr></table></figure></li>
<li>中斷 container <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker stop &lt;container_id&gt;</span><br></pre></td></tr></table></figure></li>
<li>啟動 container (在現有的 container 中開) <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker start &lt;container_id&gt;</span><br></pre></td></tr></table></figure></li>
<li>啟動 container (啟動一個新的 container) <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker run -idt &lt;image:name&gt;</span><br></pre></td></tr></table></figure></li>
<li>進入 container 中 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it &lt;image:name&gt; bash <span class="comment"># 進入 command line</span></span><br><span class="line">docker <span class="built_in">exec</span> -it &lt;image:name&gt; &lt;<span class="built_in">command</span>&gt; <span class="comment"># 也可以直接執行對應的指令</span></span><br></pre></td></tr></table></figure></li>
<li>清除 build 時的 cache <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker builder prune</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="什麼是-docker-compose">什麼是 docker-compose</h2>
<ul>
<li>Docker Compose
是一個工具可以讓你可以透過一個指令就可以控制所有專案（project）中所需要的
services</li>
<li>Docker Compose 是用 YAML 檔案格式來描述和定義 project 中 services
運作關係</li>
</ul>
<h2 id="docker-compose-格式詳解">docker-compose 格式詳解</h2>
<ul>
<li>docker-compose.yml 大概的長像</li>
<li>.yml 是有縮排的，跟 python 一樣 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3&#x27;</span> <span class="comment"># 目前使用的版本，可以參考官網：</span></span><br><span class="line"><span class="attr">services:</span> <span class="comment"># services 關鍵字後面列出 web, redis 兩項專案中的服務</span></span><br><span class="line">  <span class="attr">web:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">.</span> <span class="comment"># Build 在同一資料夾的 Dockerfile（描述 Image 要組成的 yaml 檔案）成 container</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;5000:5000&quot;</span> <span class="comment"># 外部露出開放的 port 對應到 docker container 的 port</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">.:/code</span> <span class="comment"># 要從本地資料夾 mount 掛載進去的資料 host: compose file</span></span><br><span class="line">    <span class="attr">links:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">redis</span> <span class="comment"># 連結到 redis，讓兩個 container 可以互通網路</span></span><br><span class="line">  <span class="attr">redis:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">redis</span> <span class="comment"># 從 redis image build 出 container</span></span><br></pre></td></tr></table></figure> ### version</li>
<li>一定要放在最上面，告訴 yml 是要使用哪一個版本的 docker-compose ###
service</li>
<li>放 docker image 執行方式的地方</li>
<li>服務可命名，也可以放多個服務 ### image</li>
<li>可以直接放從 docker hub pull 下來的 image</li>
<li>也可以指定版本的 tag ### build</li>
<li>如果是要自己 build 一個 image 的話</li>
<li>要用 build 而不是 image</li>
<li>context 是告訴 docker 你的 dockerfile 在哪裡</li>
<li>dockerfile 則是 告訴 docker 你的 dockerfile 叫什麼名字，預設就是
Dockerfile <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">build:</span></span><br><span class="line">    <span class="attr">context:</span> <span class="string">./flask</span></span><br><span class="line">    <span class="attr">dockerfile:</span> <span class="string">Dockerfile</span></span><br></pre></td></tr></table></figure> ### container_name</li>
<li>指定 container 的名稱</li>
<li>可以在打指令的時候不打 id 打名稱就可以了 ### command</li>
<li>container 啟動後立刻執行的指令</li>
<li>如果要一次執行多個指令 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">command: bash -c &quot;mongod --repair &amp;&amp; mongod --auth&quot;</span><br></pre></td></tr></table></figure> ### networks</li>
<li>container 要加入哪個網路 ### restart</li>
<li>指定如果起動失敗後要執行什麼</li>
<li>always
<ul>
<li>一失敗就執行</li>
</ul></li>
<li>unless-stopped
<ul>
<li>the containers will start automatically once the Docker Engine is
restarted or any error occurs. ### environment</li>
</ul></li>
<li>定義環境變數</li>
<li>可以隨便自行定義</li>
<li>如果值是布林，要用單引號包起來，如 ' true '、' false '。 ###
ports</li>
<li>格式是 host：container</li>
<li>把在 container 內的 port 打在主機上的一個 port</li>
<li>或是只指定 container，這時會隨機挑一個 host port 來用 ###
volume</li>
<li>格式為 host：container</li>
<li>是一個路徑</li>
<li>可以把 container 內的某個資料夾同布到主機上的某上的資料夾</li>
<li>在主機上修改時，container 內也同部修改 ### reference</li>
<li>https://tpu.thinkpower.com.tw/tpu/articleDetails/1377 ##
docker-compose 指令</li>
<li>把 docker-compose run 起來 (且 run 在背景) <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure></li>
<li>把 docker-compose 關掉 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker-compose down</span><br></pre></td></tr></table></figure></li>
<li>觀看 docker-compose process 狀況 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker-compose ps</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="使用-nginx-把-react-部署">使用 nginx 把 react 部署</h2>
<ul>
<li>docker-compose file <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3&#x27;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="comment"># 服务名称</span></span><br><span class="line">  <span class="attr">nginx:</span></span><br><span class="line">    <span class="comment"># 镜像:版本</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:latest</span> </span><br><span class="line">    <span class="comment"># 映射容器80端口到本地80端口</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">&quot;80:80&quot;</span></span><br><span class="line">    <span class="comment"># 数据卷 映射本地文件到容器</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">    <span class="comment"># 映射nginx.conf文件到容器的/etc/nginx/conf.d目录并覆盖default.conf文件</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">./nginx.conf:/etc/nginx/conf.d/default.conf</span></span><br><span class="line">    <span class="comment"># 映射build文件夹到容器的/usr/share/nginx/html文件夹</span></span><br><span class="line">     <span class="bullet">-</span> <span class="string">./build:/usr/share/nginx/html</span></span><br><span class="line">    <span class="comment"># 覆盖容器启动后默认执行的命令。</span></span><br><span class="line">    <span class="attr">command:</span> <span class="string">/bin/bash</span> <span class="string">-c</span> <span class="string">&quot;nginx -g &#x27;daemon off;&#x27;&quot;</span></span><br></pre></td></tr></table></figure></li>
<li>nginx.conf</li>
<li>為了要讓 react router 有作用要修改一些地方 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen       80;</span><br><span class="line">    server_name  localhost;</span><br><span class="line"></span><br><span class="line">    #charset koi8-r;</span><br><span class="line">    #access_log  /var/log/nginx/host.access.log  main;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        root   /usr/share/nginx/html;</span><br><span class="line">        index  index.html index.htm;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    #error_page  404              /404.html;</span><br><span class="line"></span><br><span class="line">    # redirect server error pages to the static page /50x.html</span><br><span class="line">    #</span><br><span class="line">    error_page   500 502 503 504  /50x.html;</span><br><span class="line">    location = /50x.html &#123;</span><br><span class="line">        root   /usr/share/nginx/html;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
<li>https://segmentfault.com/a/1190000010415158</li>
</ul>
<h2 id="reference">Reference</h2>
<ul>
<li>簡單的 docker 介紹
<ul>
<li>https://larrylu.blog/step-by-step-dockerize-your-app-ecd8940696f4</li>
</ul></li>
<li>flask + mongodb + nginx docker 部署
<ul>
<li>https://www.digitalocean.com/community/tutorials/how-to-set-up-flask-with-mongodb-and-docker</li>
</ul></li>
<li>docker 指令大全
<ul>
<li>https://philipzheng.gitbooks.io/docker_practice/content/</li>
</ul></li>
<li>docker &amp; docker-compose cheat sheet
<ul>
<li>https://devhints.io/docker-compose</li>
</ul></li>
</ul>
]]></content>
      <categories>
        <category>雜開發心得</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>vscode remote-ssh 問題踩坑心得</title>
    <url>/2021/12/22/vscode%20remote-ssh%20%E5%95%8F%E9%A1%8C%E8%B8%A9%E5%9D%91%E5%BF%83%E5%BE%97/</url>
    <content><![CDATA[<p>vscode 的 remote ssh
真的超好用的，但是就是有時候連線進去會有下面的問題：</p>
<p>keywords: remote ssh、vscode <span id="more"></span></p>
<h2 id="downloading-with-wget">Downloading with wget</h2>
<h3 id="問題原因">問題原因</h3>
<p>vscode 的 remote ssh
真的超好用的，但是就是有時候連線進去會有下面的問題：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[10:50:31.984] &gt; Acquiring lock on /home/remoteuser/.vscode-server/bin/9df03c6d6ce97c6645c5846f6dfa2a6a7d276515/vscode-remo</span><br><span class="line">&gt; te-lock.9df03c6d6ce97c6645c5846f6dfa2a6a7d276515</span><br><span class="line">&gt; Installing to /home/remoteuser/.vscode-server/bin/9df03c6d6ce97c6645c5846f6dfa2a6a7d276515...</span><br><span class="line">&gt; Downloading with wget</span><br></pre></td></tr></table></figure>
<p>然後就…卡住了…</p>
<p>經過了一陣爬文才知道，原來是 vscode 在連線前會去下載一個
vscode-server-linux-x64.tar.gz 包，目的…(我不太想知道 XD)</p>
<p>但是！重點來了！</p>
<p>不知道大家有沒有下載過 vscode 的經驗，有時候去 microsoft
的官網下載，會常常出現網路錯誤中斷下載，而且下載的速度超慢</p>
<p>這裡的問題是一樣的</p>
<p>當 vscode 要去 microsoft 下載 vscode-server-linux-x64.tar.gz
時也出現了網路錯誤，所以才常常卡在 <code>Downloading with wget</code>
不動</p>
<h3 id="解決辨法">解決辨法</h3>
<p>既然 vscode 的載點不能用，那我們就自己手動下載吧</p>
<p>首先去剛剛 terminal 的錯誤畫面中，把 <code>.vscode-server/bin</code>
後面的亂碼 copy 下來</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// 也就是 9df03c6d6ce97c6645c5846f6dfa2a6a7d276515，注意每個人都不一樣</span><br><span class="line">Installing to /home/remoteuser/.vscode-server/bin/9df03c6d6ce97c6645c5846f6dfa2a6a7d276515...</span><br></pre></td></tr></table></figure>
<p>接著把下面的 commit_id 修改成剛剛 copy 的亂碼</p>
<p>這個就是手動下載 vscode-server-linux-x64.tar.gz 包的載點了
(看網址好像是中國的 azure 來的！？)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">https://vscode.cdn.azure.cn/stable/$&#123;commit_id&#125;/vscode-server-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>
<blockquote>
<p>在 Google 的時候也有發現另一個載點： 但這個好像就是原本 vscode
官方的載點，超極慢…千萬不要用它
<code>https://update.code.visualstudio.com/commit:$&#123;commit_id&#125;/server-linux-x64/stable</code></p>
</blockquote>
<p>下載後，把 vscode-server-linux-x64.tar.gz 利用 scp 或是隨身碟 copy
到遠端 server 中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp -i /Users/user/.ssh/$&#123;your_key&#125; ~/vscode-server-linux-x64.tar.gz $&#123;your_server&#125;:/home/user/...</span><br></pre></td></tr></table></figure>
<p>進入 ~/.vscode-server/bin 資料夾</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd ~/.vscode-server/bin</span><br></pre></td></tr></table></figure>
<p>創立與亂數同名的資料夾，並進去裡面</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir $&#123;你的亂數&#125;</span><br><span class="line">cd $&#123;你的亂數&#125;</span><br></pre></td></tr></table></figure>
<p>把剛剛 copy 過來的 vscode-server-linux-x64.tar.gz 包 mv 過來</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mv ~/vscode-server-linux-x64.tar.gz .</span><br></pre></td></tr></table></figure>
<p>解壓縮它，並且把裡面的檔案全部 copy 到現在的位置</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -zxf vscode-server-linux-x64.tar.gz</span><br><span class="line">mv vscode-server-linux-x64/* .</span><br></pre></td></tr></table></figure>
<p>最後再新增一個 vscode-scp-done.flag 檔案</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch vscode-scp-done.flag</span><br></pre></td></tr></table></figure>
<p>最後就可以重新整理 remote-ssh ，按 retry 就可以正常連線進去囉！</p>
<p>當然以上超極複雜的做法，只有在以下兩個條件下滿足才會用到它</p>
<ol type="1">
<li>你第一次連到遠端 server 去</li>
<li>今天 microsoft 網路連線超不好</li>
</ol>
<p>如果你很幸運的兩個條件都符合，恭禧你要全都照做一邊，或是…隔天等網路比較順後再處理吧
XD</p>
<h2 id="cat-homeuser.vscode-server...log-permission-denied">cat
/home/user/.vscode-server/...log: Permission denied</h2>
<h3 id="問題原因-1">問題原因</h3>
<p>.vscode-server 連線時會去看一份 log 檔，有時候 log
的權限設錯就會出現問題</p>
<h3 id="解決方法">解決方法</h3>
<p>去 terminal 中看看連線的 commit id 是多少</p>
<p>接著去到 .vscode-server 中</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd ~/.vscode-server</span><br></pre></td></tr></table></figure>
<p>把所有 commit id 相關的檔案權限都改成最大 777
(應該有更安全的做法…)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod 777 $&#123;commit_id&#125;.*</span><br></pre></td></tr></table></figure>
<h2 id="flock-99-錯誤的檔案敘述項">flock: 99: 錯誤的檔案敘述項</h2>
<p>完整錯誤訊息：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[14:19:42.047] &gt; main: 列 243: /home/user/.vscode-server/bin/899d46d82c4c95423fb7e10e68eba52050e3</span><br><span class="line">0ba3/vscode-remote-lock.user.899d46d82c4c95423fb7e10e68eba52050e30ba3: 拒絕不符 </span><br><span class="line">權限的操作</span><br><span class="line">[14:19:42.061] &gt; Acquiring lock on /home/user/.vscode-server/bin/899d46d82c4c95423fb7e10e68eba520</span><br><span class="line">50e30ba3/vscode-remote-lock.user.899d46d82c4c95423fb7e10e68eba52050e30ba3</span><br><span class="line">flock: 99: 錯誤的檔案敘述項</span><br><span class="line">Installation already in progress...</span><br><span class="line">If you continue to see this message, you can try toggling the remote.SSH.useFloc</span><br><span class="line">k setting</span><br><span class="line">2da976fb3cde: start</span><br></pre></td></tr></table></figure>
<h3 id="問題原因-2">問題原因</h3>
<p>好像是在 /bin 資料夾有一個 vscode-remote-lock.user.${commit_id}
的檔案，是它權限設定不對</p>
<h3 id="解決方法-1">解決方法</h3>
<p>一個治標不治本的方法是刪掉這個檔案，然後重連，可以一次性的解決，但是下一次要再連的時候又不行了</p>
<p>更外一個解法是</p>
<ol type="1">
<li>進入 vscode 後，按下 ctrl+, 進入偏好設定頁面</li>
<li>上面搜尋列打 useFlock</li>
<li>把選項關掉</li>
<li>再重新連線</li>
</ol>
<h2 id="reference">Reference</h2>
<h3 id="問題一">問題一</h3>
<p><a
href="https://github.com/microsoft/vscode-remote-release/issues/4743">Github
上的正解</a></p>
<p><a
href="https://blog.csdn.net/zhuzixiangshui/article/details/103680328">csdn
上的誤解</a></p>
<p><a
href="https://stackoverflow.com/questions/56671520/how-can-i-install-vscode-server-in-linux-offline">stackoverflow
上的誤解</a></p>
<h3 id="問題三">問題三</h3>
<p><a
href="https://github.com/microsoft/vscode-remote-release/issues/2518">Github
上的解法</a></p>
]]></content>
      <categories>
        <category>雜開發心得</category>
      </categories>
      <tags>
        <tag>remote ssh</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>AutoML - MobileNet v1~v3 與 EfficientNet v1 回頭閱讀</title>
    <url>/2022/01/23/AutoML-EfficientNet-%E5%9B%9E%E9%A0%AD%E9%96%B1%E8%AE%80/</url>
    <content><![CDATA[<p>時間來到 2017 ~ 2019 年，在這期間 Google
依序提出基於「輕量化」的神經網路 MobileNet
v1~v3，在相同效果的條件下，運算量少了非常之多。而 2019 年 EfficientNet
則繼承了這項重責大任，把 NAS 應用在 MobileNet
上，找出最佳的排列組合。結果是非常驚人的，在效率及效果均刷新 SOTA
好幾個百分點，並為 CNN 的發展打下了非常牢固的基礎。</p>
<p>keywords: EfficientNet、NAS、MobileNet <span id="more"></span></p>
<h2 id="什麼是-mobilenet">什麼是 MobileNet ？</h2>
<p>在介紹 EfficientNet 之前，先來很簡單的說一下什麼是 MobileNet。</p>
<p>MobileNet 是 Google 2017
首次提出的網路架構，目的是在降低網路的運算量及參數使用量，使得深度學習可以應用在日漸發展的物聯網、移動平台上</p>
<p>以下依時間順序，簡單講解：改進了什麼？以及為什麼這樣改？</p>
<p>如果有想要更進一步了解更多東西的話，可以參考下列文章：</p>
<p><a
href="https://chihangchen.medium.com/%E8%AB%96%E6%96%87%E7%AD%86%E8%A8%98-mobilenetv3%E6%BC%94%E8%AE%8A%E5%8F%B2-f5de728725bc">MobileNet
演變史</a></p>
<h3 id="mobilenet-v1">2017 MobileNet v1</h3>
<p><a
href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a></p>
<p>MobileNet v1 主要應用 <strong>Depth-wise Separable
Convolution</strong>，把一個 Convolution 運算拆解成 Depthwise
Convolution 以及 Pointwise Convolution。雖然這個概念不是 MobileNet
原創，而是由 <a href="https://arxiv.org/abs/1610.02357">Xception</a>
這篇論文提出的，但 MobileNet 仍把它發揮得淋漓盡致。網路架構圖如下：</p>
<p><img src="https://i.imgur.com/zjQuFO3.png"
alt="image-20220124145825257" /></p>
<p>而為什麼 Depthwise Separable Convolution
可以降低運算量呢？假設我們要把一張大小為 32x32x3 的圖片，經過一個 3x3
kernel 特徵圖放大為 64，則原 Convolution 總運算為： <span
class="math display">\[
(32\times32)\times3\times64\times(3\times3)=1769472
\]</span> 而使用 Depthwise Separable Convolution 的運算則為： <span
class="math display">\[
\begin{gather}
[(32\times32)\times3\times1\times(3\times1)]+
[(32\times32)\times3\times64\times(1\times1)]=205824
\end{gather}
\]</span> 兩個運算相差近 10 倍，可說 Depthwise Separable Convolution
省下了非常非常多的運算，然後省下運算也代表著網路的「彈性」變小，理論上限效果會變差，但根據這篇論文其實效果並為減少太多。</p>
<h3 id="mobilenet-v2">2018 MobileNet v2</h3>
<p><a
href="https://arxiv.org/abs/1801.04381">https://arxiv.org/abs/1801.04381</a></p>
<p>MobileNet v2 與 v1 最大的差別在於，v2 多引入了 bottleneck block 與
inverted-bottleneck block 架構，再運算量又進一步減少。架構如下：</p>
<p><img src="https://i.imgur.com/kn5AjKz.png" alt="image-20210426130521052"/></p>
<p>bottleneck block 的核心在於，輸入特徵圖會先經過一個 1x1 conv
做放大/縮小的運算，接著做一 3x3 的 Depthwise Convolution，再用一個 1x1
conv 變回原本維度。這種把特徵圖放大再縮小就是 bottleneck 的特色了。</p>
<p>因為每一個 conv 後都會做 ReLU 的關系，作者經實驗發現 inverted
bottleneck 的效果最好。因為如果 3x3
特徵圖數太小，很有可能大部份的特徵值都會被 ReLU 化為
0，網路就學不到東西了。</p>
<p>除了使用 inverted bottleneck block 外，作者也在最後一個 1x1 conv 使用
linear activation 線性的函數，避免太多的 ReLU 非線性 block
破壞了網路的結構</p>
<p>另外在 MobileNet v2 中，開始以 stride 2 取代 2x2 pooling
達成降維操作</p>
<h3 id="mobilenet-v3">2019/5 MobileNet v3</h3>
<p><a
href="https://arxiv.org/abs/1905.02244">https://arxiv.org/abs/1905.02244</a></p>
<p>MobileNet v3 與 v2 最大的差別在於加入了 SENet 以及使用 NAS。</p>
<p>SENet 全名 Squeeze and Excitation 是一個類似 Attention
想法的網路，放大重要的特徵，縮小不重要特徵，並加入了 GAP Global Average
Pooling 計算每個 Feature Map 的權重。</p>
<p><img src="https://i.imgur.com/yMSANwl.png"
alt="image-20220124155129171" /></p>
<p>並且透過 NAS 找出了一個最佳排列組合的網路</p>
<h2 id="什麼是-efficientnet">什麼是 EfficientNet ？</h2>
<p>就在 MobileNet v3 提出的同月，Google 發表了 EfficientNet v1，照抄了
MobileNet v3 的架構，在修改 NAS 的 Search Strategy 後，結果好到直接把
MobileNet v3 甩到牆上</p>
<h3 id="efficientnet-v1">2019/5 EfficientNet v1</h3>
<p><a
href="https://arxiv.org/abs/1905.11946">https://arxiv.org/abs/1905.11946</a></p>
<p>EfficientNet
的核心想法認為：在以往設計網路時，常常加強網路的三個面向以得到更好的效果：深度、寬度、解析度，如下圖所示：</p>
<p><img src="https://i.imgur.com/qX2iVPC.png"
alt="image-20220124204200111" /></p>
<p>加寬代表 (圖 b)：增加 Feature Map 也就是 Channel
的數量，可以得到更多的特徵組合</p>
<p>加深代表 (圖 c)：使網路學習到更多更複雜的特徵</p>
<p>加解析度代表 (圖 d)：在做 Object Detection
時，有時影像中的小物件效果不好，可以增加解析度來得到更好的效果</p>
<p>但是作者認為這三個東西並非是三個獨立的參數，不應該每次只調整其中一個而已
(如深度)，應該是三個參數一起找一個最佳組合才對，而作者稱這種方法叫
Compound Scaling (圖 e)</p>
<p>如果以上想法用數學公式來表達的話，如下式：</p>
<p>假設 input 是 <span class="math inline">\(X\)</span> 經一層卷積運算
<span class="math inline">\(\mathcal{F}_i()\)</span> 得到 output <span
class="math inline">\(Y\)</span> ，而 <span
class="math inline">\(i\)</span> 代表的是第 <span
class="math inline">\(i\)</span> 層卷積運算</p>
<p>如果今天網路有很多卷積運算，則可得到下列表示： <span
class="math display">\[
\begin{align}
\mathcal{N}&amp;=\mathcal{F}_k\odot...\odot\mathcal{F}_2\odot\mathcal{F}_1(X_1)\\
&amp;=\bigodot_{i=1...s}\mathcal{F}_i^{L_i}(X_{(H_i,W_i,C_i)})
\end{align}
\]</span> 在以上式為基準之下，調整 <span
class="math inline">\(d,w,r\)</span> 參數，使得準確率為最大： <span
class="math display">\[
\begin{align}
\max_{d,w,r}&amp;\quad\mathrm{Accuracy}(\mathcal{N}(d,w,r))\\
s.t.&amp;\quad\mathcal{N}(d,w,r)=\bigodot_{i=1...s}\hat{\mathcal{F}}_i^{d\cdot
\hat{L}_i}(X_{(r\cdot \hat{H}_i,r\cdot \hat{W}_i,w\cdot \hat{C}_i)})
\end{align}
\]</span> 並且額外加入兩個條件式，在記憶體使用量及運算量都要小於一定值：
<span class="math display">\[
\begin{align}
&amp;\mathrm{Memory}(\mathcal{N})\leq\mathrm{target\_memory}\\
&amp;\mathrm{FLOPS}(\mathcal{N})\leq\mathrm{target\_flops}
\end{align}
\]</span> 作者在規劃調整參數有時兩個發現：</p>
<p>發現一：各個參數在加大時，準確率「提升程度」越來越小，白話說：付出的計算成本與效果
cp 值越來越低</p>
<p><img src="https://i.imgur.com/A5E4Z6o.png"
alt="image-20220125151315306" /></p>
<p>發現二：參數不能只調整單個，要整體來考慮。下圖為固定 w 下調整 d, r
的結果，發現調哪個參數對網路影響最大都不是一定的</p>
<p><img src="https://i.imgur.com/A3fF45y.png"
alt="image-20220125151722668" /></p>
<p>根據上列發現，作者設計了以下限制式： <span class="math display">\[
\begin{align}
\mathrm{depth}&amp;:d=\alpha^\phi\\
\mathrm{width}&amp;:w=\beta^\phi\\
\mathrm{resolution}&amp;:r=\gamma^\phi\\
\mathrm{s.t.}&amp;\quad\alpha\cdot\beta^2\cdot\gamma^2 \approx2^\phi\\
&amp;\quad\alpha\ge1,\beta\ge1,\gamma\ge1
\end{align}
\]</span> 透過調整 compound coefficient <span
class="math inline">\(\phi\)</span> 設計出不同大小的網路，且要找出 <span
class="math inline">\(\alpha\beta\gamma\)</span> 三者相乘後最接近 <span
class="math inline">\(2^\phi\)</span> 的組合。</p>
<p>至於為什麼 <span class="math inline">\(\beta\gamma\)</span>
要加個平方項呢？因為當我們放大網路寬度、解析度時，是同時對圖片的「長寬」同時放大，因此運算量也呈平方關系。而深度因圖片數量一樣，只是多做幾次而已，與運算量程線性倍數關系。</p>
<p>至於為什麼是要小於 <span class="math inline">\(2^\phi\)</span>
呢？嗯…可能是作者經實驗或經驗得來的吧，論文中並未明確給出解答，是個神奇的
magic number 呢。但總之作者經上述式子令 <span
class="math inline">\(\phi\)</span> 時，找到當 <span
class="math inline">\(\alpha=1.2\)</span> <span
class="math inline">\(\beta = 1.1\)</span> <span
class="math inline">\(\gamma = 1.15\)</span>
時效果最好，並把此倍大倍率套回 MobileNet v3 的架構中，得到
EfficientNet-B0 架構</p>
<p><img src="https://i.imgur.com/nxWry7B.png"
alt="image-20220125155242575" /></p>
<p>最後放大 <span class="math inline">\(\phi\)</span> 得到
EfficientNet-B1~B7 不同大小的架構</p>
<p><img src="https://i.imgur.com/ehgVs2g.png"
alt="image-20220125155355722" /></p>
<p>最後這是 EfficientNet 與 SOTA 的比較，在相同效果下，運算量少了近 5
倍以上</p>
<p><img src="https://i.imgur.com/wCldkxc.png"
alt="image-20220125155500013" /></p>
<h2 id="reference">Reference</h2>
<p><a
href="https://blog.csdn.net/qq_37541097/article/details/114434046">cdsn
文章</a></p>
<p><a href="https://www.youtube.com/watch?v=qeCi-Qo1OcA">講得很棒的
EfficienDet Youtube</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title>Swin Transformer V2: Scaling Up Capacity and Resolution</title>
    <url>/2021/12/23/Swin-Transformer-V2-Scaling-Up-Capacity-and-Resolution/</url>
    <content><![CDATA[<p>論文網址：<a
href="https://arxiv.org/pdf/2111.09883.pdf">https://arxiv.org/pdf/2111.09883.pdf</a></p>
<p>Swin 原班人馬在 2021 11 月提出 Swin Transformer 的改良版 Swin
Transformer V2。主要是優化 Swin 在 scale up 大參數模型上的能力</p>
<p>改進了 Swin 架構中的三個小地方：</p>
<ul>
<li>post normalization：在 self-attention layer 和 MLP block 後做 layer
normalization</li>
<li>scaled cosine attention approach：使用 cosine 相似度來計算 token
pair 之間的關系</li>
<li>log-spaced continuous position bias：設計全新的相對位置編碼</li>
</ul>
<p>keywords: Swin v2 <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p>在 NLP 的領域中，自 Transformer
提出以來，一路提出更多新架構：BERT、GPT-3，而使用的參數量也呈指數上升。這個現象叫做
scaling up 是 NLP 領域為了提升更好的效能所做的方法
(白話的說叫：巨量資料集、瘋狂疊參數)。</p>
<p>但是在 CV 領域中，很少聽到有人用 scaling up
達到很好的效果，而且實作經驗也告訴我們，一昧的增加參數效果不見得好，所以目前
CNN 最多的參數量 (1B 億)，與 NLP 相比單位完全在不同的量級上 (GPT-3
的參數可是到 1700B 億了…)</p>
<p>那為什麼會有這樣的現象？這篇作者認為是 CNN 的 inductive bias
限制了效果，而最近流行的 Transformer 並沒有這個限制</p>
<p>因此本篇作者提出 Swin V2 是為了之後 Scaling up
做準備，並且同時實驗分類任務與分割任務，看看效果如何</p>
<h2 id="網路架構">網路架構</h2>
<p><img src="https://i.imgur.com/3iR0LCB.png" /></p>
<p>作者為了把 Swin Transformer Scaling up 做了以下三個小技巧</p>
<h3 id="post-normalization">Post normalization</h3>
<p>作者第一個小技巧是把 LN 放到 Self-Attention Block 後</p>
<p>作者經由下圖實驗發現當 Swin 做 Scale 後，越深層的 activate function
之間的差就越大，使得網路變得非常難以練訓</p>
<p>紅色是最大的網路架構，有 658M 個參數量，可發現上下相差非常大</p>
<p><img src="https://i.imgur.com/6d3m9YE.png" /></p>
<p>會使得 activate function 極端化的原因是：在經過超多次的
Self-Attention 後，兩像素之間，相似會變超相似，不相關的會超不相關</p>
<p>作者還提出 Scaling up 後 Pre-Norm 與 Post-Norm 的差別，可看到
Pre-Norm 甚至還跑到一半就爆了</p>
<p><img src="https://i.imgur.com/7qenyIF.png" /></p>
<p>作者還每 6 個 Transformer Block 又額外加一個 LN
層，為了使網路更穩定</p>
<h3 id="scaled-cosine-attention-approach">Scaled cosine attention
approach</h3>
<p>在最原本的 Transformer 論文中，query 與 key 的運算子是使用 dot
product (內積運算)</p>
<p>作者發現當把模形做 Scaling up 後，Attention map 中的某些 Patch 某些
Head，權重往往會變過大，變成只有它最重要，特徵不平衡了</p>
<p>於是作者改使用 Scaled cosine attention (cosine 相似度) 來代替</p>
<p><span class="math display">\[
\mathrm{Sim}(q_i,k_i)=cos(q_i,k_i)\tau+B_{ij}
\]</span></p>
<p><span class="math inline">\(\tau\)</span> 是一個可學習參數，head
layer 之間不共享</p>
<p><span class="math inline">\(B_{ij}\)</span> 是指相對位置</p>
<p>因為 cosine
本身的取值範圍本身就相當於是被正歸化後的結果，因此可以平均差距的問題</p>
<h3 id="log-spaced-contiguous-position-biaslog-spaced-cpb">Log-spaced
contiguous position bias（log spaced CPB)</h3>
<p>作者直接把模形 Scaling up
發現效果越來越差，推論可能是因為相對位置沒有一併放大的問題，因此提出來
Log-spaced contiguous position bias 來減少因放大而產生的差距</p>
<p>舉個例子，假設我們要把 8×8 window size fine-tuned 到 16 × 16 window
size，使用原本 Swin 定義，相對位置座標會從 [−7, 7] × [−7, 7] 到 [−15,
15]×[−15, 15]，放大倍率約為 1.14x</p>
<p>因此作者試著轉換相對位置的座標，把單位從整數，改為以 log
為單位，公式如下：</p>
<p><span class="math display">\[
\begin{gathered}
\hat{\Delta x} = \mathrm{sign}(x) + log(1+ |\Delta x|)\\
\hat{\Delta y} = \mathrm{sign}(y) + log(1+ |\Delta y|)
\end{gathered}
\]</span></p>
<p>經由上面的座標轉換從 [−2.079, 2.079] × [−2.079, 2.079] 變成 [−2.773,
2.773] × [−2.773, 2.773]，放大倍率為 0.33x</p>
<p>相比之前的方法差距小了不少</p>
<p>作者在把座標換成 log，又新增了個叫 Continuous relative position
bias，簡單來說就是把上面得出的相對位置座標，再經 2 層 MLP 層</p>
<p>加入 2 層可學習的 MLP 後，使得未來在 Scaling up
輸入圖片大小不同時，網路彈性大一些，而不是像以前一樣固定的死死的</p>
<p>結論可看下圖，最上面使用 ViT 的原作法，中間則是 Swin
的整數做法，最下面是 Swin v2 的 log 座標做法。可發現 ViT 與 Swin
的效果相差最多，Swin v2 提出的 log 只好一些些</p>
<p><img src="https://i.imgur.com/5GRgOEM.png" /></p>
<h2 id="experiment">Experiment</h2>
<p>實驗方面鐵定頂級，這裡就不再多說了，有興趣可自己去看原論文，下面就放一張
SOTA 的比較表</p>
<p><img src="https://i.imgur.com/8e4vFQs.png" /></p>
<h2 id="結論">結論</h2>
<p>這篇論文並未提出什麼新架構，僅僅是把 Swin 改成更好 Scaling up
模形的工程報告書而已</p>
<p>不過我們也可以從中看到一個趨勢：CV 開始往 Scaling up 方向前進了</p>
<p>由 NLP
成功的例子我們知道：「大力出奇蹟」，更多的資料，更大的模形，勢必是 CV
界下一步的方向</p>
<p>因此 Self-Supervised Learning、Trasfer Learning、Scaling up
想必是末來研究的重點</p>
<p>而 CV 是否真的能 copy paste NLP
的經歷並成功打出一片天？我們就靜觀其變吧！</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://zhuanlan.zhihu.com/p/435210138">https://zhuanlan.zhihu.com/p/435210138
知乎大神筆記</a></p>
<p><a href="https://www.zhihu.com/question/500004483">知乎大神們對 CV
未來的爭論，裡面有很有趣的觀點，大推</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>Twins: Revisiting the Design of Spatial Attention in Vision Transformers</title>
    <url>/2022/01/21/Twins-Revisiting-the-Design-of-Spatial-Attention-in-Vision-Transformers/</url>
    <content><![CDATA[<p>在 2021 年 3 月提出 Swin Transformer 後，4 月澳洲 Adelaide 大學提出
Twins-PCPVT 以及 Twins-SVT 兩個新架構來改進 Swin Transformer Backbone
上的一些問題。</p>
<p>本篇論文比較像一個工程報告書，比較 PVT、Swin 以及作者提出的 Twins
之間的優缺點</p>
<p><a
href="https://arxiv.org/abs/2104.13840">https://arxiv.org/abs/2104.13840</a></p>
<p>keywords: Twins-PCPVT、Twins-SVT <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p>作者認為 Swin Transformer 有以下的優缺點：</p>
<p>pros：</p>
<ul>
<li>提出 window 單位，解決了 Transformer 運算量過大的問題</li>
</ul>
<p>cons：</p>
<ul>
<li>shifted window 設計，雖然解決了 windows
之間缺少相關性的問題，但程式其中所使用的
<code>torch.roll()</code>，對運算量非常不友好。一些部署優化的規範
(如：ONNX、TensorRT)，並不支援這一指令</li>
</ul>
<p>因此提出了 Twins-SVT 來改善 Swin 的缺點</p>
<p>另外作者認為 PVT 與 Swin 的網路架構想法相近，皆是在 Transformer
架構中加入多重解析度的概念。</p>
<p>也提出了 Twins-PCPVT 類似技術報告的方法，來使 PVT 的效果更好一些</p>
<h2 id="網路架構">網路架構</h2>
<h3 id="twins-pcpvt">Twins-PCPVT</h3>
<p>作者比較 PVT 與 Swin 的網路架構的差別，發現：</p>
<ul>
<li>PVT 沒有使用 window 為單位，不同解析度的特徵圖整張做 Self-Attention
運算，在運算量上比 Swin 大</li>
<li>PVT 中的 Positional Encoding 是使用如同 ViT 中的 APE (Absoute
Positional Encoding 絕對位置)，而 Swin 則是使用 RPE (Reletive Positional
Encoding 相對位置)</li>
</ul>
<p>明明兩個網路架構都使用到了多重解析度的概念，那為什麼 PVT 的效果不及
Swin 呢？作者認為問題是出在位置編碼上</p>
<p>因此作者參考了 CPVT 這篇論文所提出的 CPE (Conditional Position
Encoding)，並把原本 PVT 的 APE 替換成 CPE。PVT 與 CPVT
兩篇論文相互結合，作者稱這個新的混合方法為 Twins-PCPVT</p>
<p><img src="https://i.imgur.com/r65k5FV.png" alt="Image" /></p>
<p>PEG (Positional Encoding Generator) 為 CPVT
中提出的架構，詳細可以參考我之前寫過的文章：</p>
<p><a
href="https://mushding.space/2021/07/28/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Conditional-Positional-Encodings-for-Vision-Transformers-%E5%8F%AF%E8%AE%8A%E5%BA%8F%E5%88%97%E9%95%B7%E7%9F%AD%E7%9A%">Vision
Transformer 演化史: Conditional Positional Encodings for Vision
Transformers - 可變序列長短的 Positional Encoding</a></p>
<h3 id="twins-svt">Twins-SVT</h3>
<p>本篇架構目標是把 Swin 改進，Twins-SVT 與 Swin 相同，同樣使用 window
作為一個計算單位，但是不使用 shifted window 來解決各 windos
不相關的問題，而是提出了 SSSA (Spatially Separable Self-Attention)
架構</p>
<h4 id="sssa-spatially-separable-self-attention">SSSA (Spatially
Separable Self-Attention)</h4>
<p>模仿 CNN 中的 Separable-Convolution 將卷積運算分成 Depth-wise +
Point-wise，目的為了減少運算量</p>
<p>而 SSSA 也把 Self-Attention 分成兩個步驟：LSA (Locally-grouped
self-attention) 與 GSA (Global sub-sampled attention)</p>
<h4 id="lsa">LSA</h4>
<p>LSA 簡單說與 Swin 一模一樣，就是把張圖片分成 window 們，每一個
Self-Attention 只會發生在一個 window 內</p>
<p>設定 window 大小為 <span class="math inline">\(mn\)</span>，則 window
個數為</p>
<p><span class="math display">\[
k_1k_2=\frac{H}{m}\frac{W}{n}
\]</span></p>
<h4 id="gsa">GSA</h4>
<p>與 Shifted window 不同，作者使用的方法更直接一些，就是在 LSA
後直接再做一次整張圖的 Self-Attention</p>
<p>也可以說先做一次 LSA 代表局部資訊，再做 GSA
代表全局資訊，兩兩結合就是全部圖片的相關性了</p>
<p>但如果是這樣，計算量就又一樣了，又是整張圖片去做運算。</p>
<p>作者的解法為：把每個 window 中選一個最重要的值，代表這個 window
的主要特徵，於是我們可以拼出一個 <span class="math inline">\(mn\)</span>
大小的新 window。我們將這個新 window 看成是 Key 一般，去對原圖中每一個
window 做 Self-Attention</p>
<p>換句話說：LSA 是 window 中自己與自己計算相關性，而 GSA 是 window
中自己與「全局重要特徵」計算相關性</p>
<p>詳細流程可參考下圖：</p>
<p><img src="https://i.imgur.com/eqJueht.png" alt="Image" /></p>
<p>每一個 Transformer Block
的流程為：<code>LSA -&gt; FFN -&gt; GSA -&gt; FFN</code></p>
<h2 id="experiments">Experiments</h2>
<h3 id="imagenet-分類上的結果">ImageNet 分類上的結果</h3>
<p><img src="https://i.imgur.com/DEYK7L4.png" alt="Image" /></p>
<p>Twins-PCPVT 實驗證實，PVT 架構是很有潛力的，將其中 APE 替換成 CPE
後，效果比原 PVT 好 1.4%。</p>
<p>同時在效果與 Swin 差不多的前提下，Twins-PCPVT 運算量比 Swin 少了
18%，而 Twins-SVT 更是少了 35%</p>
<h3 id="ade20k-分割上的結果">ADE20K 分割上的結果</h3>
<p>為了公平比較，分割方法皆為使用 UpperNet</p>
<p><img src="https://i.imgur.com/9nUPKrx.png" alt="Image" /></p>
<p>Twins-PCPVT 比 PVT 高 4.3% mIoU</p>
<p>Twins-SVT 比 Swin 高大約 1.7% mIoU</p>
<h2 id="結論">結論</h2>
<p>這篇論文提出了兩個新架構，一是改進 PVT 的位置資訊編碼，加入了
CPVT，側向證明位置編碼的重要性</p>
<p>二是改進 Swin <code>torch.roll()</code>
的問題，在效果參數量不變的前提下，運算量又再一步下降</p>
<h2 id="reference">Reference</h2>
<p><a href="https://arxiv.org/abs/2104.13840">Twins 論文</a></p>
<p><a href="https://arxiv.org/abs/2102.12122">PVT 論文</a></p>
<p><a href="https://arxiv.org/abs/2102.10882">CPVT 論文</a></p>
<p><a
href="https://mushding.space/2021/08/17/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Pyramid-Vision-Transformer-A-Versatile-Backbone-for-Dense-Predictionwithout-Convolutions-%E6%8A%8A%E9%87%91%E5%AD%97%E5%A1%94%E7%B6%B2%E8%B7%AF%E6%87%89%E7%94%A8%E5%9C%A8-Transformer/">我的
PVT 筆記</a></p>
<p><a
href="https://mushding.space/2021/07/28/Vision-Transformer-%E6%BC%94%E5%8C%96%E5%8F%B2-Conditional-Positional-Encodings-for-Vision-Transformers-%E5%8F%AF%E8%AE%8A%E5%BA%8F%E5%88%97%E9%95%B7%E7%9F%AD%E7%9A%84-Positional-Encoding/">我的
CPVT 筆記</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer</title>
    <url>/2022/01/27/MobileViT-Light-weight-General-purpose-and-Mobile-friendly-Vision-Transformer/</url>
    <content><![CDATA[<p>2021 10 月 Apple 基於 Transformer 提出 MobileViT 架構，其主要目的是把
Transformer 輕量化，以達到能在移動設備上部署。</p>
<p>本篇最主要的方法為結合 MobileNet 與
Transformer，得到效果好、效率也不錯的架構</p>
<p><a
href="https://arxiv.org/abs/2110.02178">https://arxiv.org/abs/2110.02178</a></p>
<p>keywords: MobileViT <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p>作者開頭就提到，CNN 有 Inductive bias，ViT
則沒有，因此需要更多的資料或是用 L2Norm 來達到類似效果，在 ViT 使用
Self-Attention
運算量大以及資料量需求大下，自然是沒辨法輕易的部署到移動設備上了。</p>
<p>而本篇作者借鏡了 2019 年的 MobileNet v3 架構，提出 MobileViT
架構。MobileViT 試著結合了 CNN 與 Transformer
各自的優點，達成在相同低參數量下效果比 CNN 好</p>
<p>MobileViT 在 ImageNet 上 top-1 準確率為 78.4%，參數使用量為 6M，比
MobileNet v3 高出 3.2%，比 DeiT 高出 6.2%。也在偵測 MS-COCO 上比
MobileNet v3 高出 5.7%</p>
<h2 id="網路架構">網路架構</h2>
<p>MobileViT 有三個目標：Light-weight 輕量化、Genral-purpose
歸納能力強、Low latency 低延遲</p>
<p>而作者在設計架構時認為：CNN 的特色為：有 Inductive Bias
歸納能力強，Transformer
的特色為：可以關注到全局的資訊，但計算量大，因此採用：「以 CNN 為主，把
Transformer 融合到 CNN 架構中」</p>
<p>首先來上整體架構圖：</p>
<p><img src="https://i.imgur.com/W2Q0mH6.png"
alt="image-20220127160654393" /></p>
<p>整體架構圖不難發現，粉紅色的地方為 MV2 (MobileNet v2) 塊，是 CNN
的部份，而且占網路大多數，而綠色才是 MobileViT Transformer
的部份，只占了三格而已</p>
<p>借著 MobileNet v2 Block 減少圖片解析度，獲得多重解析度，再經過
MobileNet v2 達到類似 Attention 的效果</p>
<h3 id="unfold---matmul---fold">Unfold -&gt; Matmul -&gt; Fold</h3>
<p>在介紹一個 MobileViT Block 內部架構前，先來了解一下 CNN
的運算，通常我們在 pytorch 中設計 CNN 會使用到許多卷積運算 (Conv)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.functional.conv2d(inp, w)</span><br></pre></td></tr></table></figure>
<p>但根據這篇作者以及 <a
href="https://pytorch.org/docs/stable/generated/torch.nn.Unfold.html">pytorch
官網</a>上的敘述，一個 Conv 可以拆分成三個部份：Unfold、Matmul、Fold</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inp = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">12</span>)</span><br><span class="line">w = torch.randn(<span class="number">32</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># Unfold</span></span><br><span class="line">inp_unf = torch.nn.functional.unfold(inp, (<span class="number">4</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># Matmul</span></span><br><span class="line">out_unf = inp_unf.transpose(<span class="number">1</span>, <span class="number">2</span>).matmul(w.view(w.size(<span class="number">0</span>), -<span class="number">1</span>).t()).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># Fold</span></span><br><span class="line">out = torch.nn.functional.fold(out_unf, (<span class="number">7</span>, <span class="number">8</span>), (<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>上面那三個東西是什麼呢？下面這張圖可以清楚的表示：圖片來源：<a
href="https://blog.csdn.net/u010087338/article/details/113666140">https://blog.csdn.net/u010087338/article/details/113666140</a></p>
<p><img src="https://i.imgur.com/NsWeQJX.png"
alt="image-20220127162459106" /></p>
<p>我們一般認識的卷積運算就是一張圖片 <span
class="math inline">\(\mathbb{R}^{w_0\times h_0 \times c_0}\)</span>
對一個 kernel <span class="math inline">\(\mathbb{R}^{w_k\times
h_k\times c_1}\)</span> 做矩陣乘法後，得到結果 <span
class="math inline">\(\mathbb{R}^{w_1\times h_1\times c_1}\)</span>
的結程。但是我們也可以手動的設計上面這一系列步驟。</p>
<p>首先是 Unfold，它可以將輸入「切成」對應 kernel
大小的塊，並把塊「轉換維度」至序列。程式如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.nn.Unfold(kernel_size, dilation=<span class="number">1</span>, padding=<span class="number">0</span>, stride=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>可以發現跟 Conv 很像，它也有 kernel_size、stride …等等參數設定，但與
Conv 最大的不同在於，Unfold 只有「切」而已，不負責「運算」。</p>
<p>可以參考上圖中下部份，假設原圖 1x3x10x12 (Batch, Channel, H,
W)，kernel 大小 32x3x4x5 (Channel_out, Channel_in, H, W)，在 stride 為 1
下，我們可以「切」出 (10-4+1)x(12-5+1) = 56 個 3x4x5
塊，再把這個塊「轉化維度」至序列得到 1x56x(3x4x5) = 1x56x60</p>
<p>再來是 Matmul 也就是矩陣乘法，把得到的 1x56x60 乘上 kernel (3x4x5)x32
= 60x32，最後會得到 1x56x32 的結果</p>
<p>再來是 Fold，這一步的用意是把序列「轉換維度」回塊，把 1x56x32 轉回成
1x32x7x8，當然也可以直接用 view 直接設定維度來達成</p>
<p>根據 pytorch 官網，CNN 與 Unfold 三部曲的運算是等價的。而本篇
MobileViT 正是利用這個特性，把原本的 CNN 拆成三個步驟，並且把中間的
Matmul 核心運算層，更改為 Transformer
運算。就是這麼剛好，中間那層是一個序列維度的資料，正好適合放進
Transformer 中。</p>
<h3 id="mobilevit-block">MobileViT Block</h3>
<p>先上圖</p>
<p><img src="https://i.imgur.com/GX6A0uO.png"
alt="image-20220127160530959" /></p>
<p>流程為：</p>
<ol type="1">
<li>首先會做一個 nxn conv 運算 (論文 n=3) 得到局部特徵</li>
<li>再來做一個 1x1 conv 放大特徵圖數量</li>
<li>接著進行：Unfold -&gt; Transformer -&gt; Fold 得到全局特徵</li>
<li>再用一個 1x1 conv 回到原特徵圖數量</li>
<li>用一個 shortcut 把原輸入與剛剛經 Transformer 的結果相加</li>
<li>最後用一個 nxn conv 調整回原圖大小，使得輸入與輸出維度不變</li>
</ol>
<p>理論上「單層」 MobileViT 的運算複雜度為 <span
class="math inline">\(O(N^2Pd)\)</span> 而 ViT 的則是 <span
class="math inline">\(O(N^2d)\)</span>，看起來運算量反而變高了，但是作者解釋，MobileViT
因有 1 2 步的 CNN 得到局部特徵，加上模仿 CNN 的 Unfold Fold 架構，使得
MobileViT 有更強的 Inductive Bias
能力，在網路設計上可以使用效少的層數得到相同的效果。</p>
<p>以 ViT based 的 DeiT 為對照組，DeiT 需要 L=12, d=192，而 MobileViT
L={2, 4, 3}, d={96, 120, 144} 均少於 DeiT</p>
<h3 id="mobilevit-vs-vit">MobileViT vs ViT</h3>
<p>ViT 網路中有一步 Patch Embedding 實質上就是在把一影像，分成一個 Patch
一個 Patch 彼此不 overlap 的序列，而 MobileViT 套用了 CNN stride
的概念，每個 Patch 之間是會 overlap 的，並且彼此間距為 stride 1</p>
<p>但是這樣看起來 MobileViT 分 Patch 的數量比 ViT
多上不少，運算量應該會更大，但 MobileViT 藉由優秀的特徵提取，可以比 ViT
少了非常多層，變向減少計算量</p>
<p>另外因 MobileViT 結合了 CNN 與 Transformer
的優點，可以達到全域局部特徵都可觀察到的特色，如下圖所示，中心紅色點會達距離的藍色點計算
(Transformer)，藍色點也會和周邊其它的點計算 (CNN)</p>
<p><img src="https://i.imgur.com/89weffQ.png"
alt="image-20220129003704991" /></p>
<h2 id="experiments">Experiments</h2>
<h3 id="網路家族">網路家族</h3>
<p>作者設計了三個不同大小的網路，特別的是，網路是設計的越來越小</p>
<p><img src="https://i.imgur.com/X8NtFGR.png"
alt="image-20220127172148824" /></p>
<p>以及三種不同大小網路的效果</p>
<p><img src="https://i.imgur.com/uRodGGI.png"
alt="image-20220129012458883" /></p>
<h3 id="參數量-vs-分類效果">參數量 vs 分類效果</h3>
<p>作者與 Transformer 做比較，不清楚為什麼沒有與 Swin 作比較</p>
<p><img src="https://i.imgur.com/pUnohgN.png"
alt="image-20220129012108102" /></p>
<h3 id="偵測上的結果">偵測上的結果</h3>
<p><img src="https://i.imgur.com/FgcgUeb.png"
alt="image-20220129012312539" /></p>
<h3 id="分割上的結果">分割上的結果</h3>
<p><img src="https://i.imgur.com/WLRikce.png"
alt="image-20220129012434838" /></p>
<h2 id="結論">結論</h2>
<p>MobileViT 又是一篇整合了 CNN 與 Transformer
的論文，比較創新的地方在是以「減少運算量為目標」</p>
<p>網路架構主要還是以 MobileNet 為主，以 Transformer 為輔，並且利用
Unfold -&gt; Matmul -&gt; Fold 的方法巧妙融合 CNN 與
Transformer。這種方法使得與 MobileNet 在相同參數下效果好上了不少</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://aijishu.com/a/1060000000243736">網路上參考的筆記1</a></p>
<p><a
href="https://blog.csdn.net/u014546828/article/details/120741293">網路上參考的筆記2</a></p>
<p><a
href="https://blog.csdn.net/u010087338/article/details/113666140">图解卷积计算原理与pytorch中fold和unfold函数的使用
(圖)</a></p>
<p><a
href="https://blog.csdn.net/LoseInVain/article/details/88139435">pytorch手动实现滑动窗口操作，论fold和unfold函数的使用
(解說)</a></p>
<p><a
href="https://blog.csdn.net/xinjieyuan/article/details/105232802">pytorch
transpose() 和 permute()</a></p>
<p><a
href="https://blog.csdn.net/york1996/article/details/81949843">pytorch
view()</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>AutoML - NAS 與 NASNet 回頭閱讀</title>
    <url>/2022/01/22/AutoML-NAS-%E8%88%87-NASNet-%E5%9B%9E%E9%A0%AD%E9%96%B1%E8%AE%80/</url>
    <content><![CDATA[<p>在深度學習發展的今天，要設計一個網路要一件非常簡單的事情，但是要設計出「符合硬體需求」的網路非常困難，要怎麼在硬體的限制下求得最佳效能的網路呢？我們可以加深網路深度、加寬網路…等等技巧，但要怎麼在效能與效果間取一個最佳平衡？於是
NAS 誕生了。NAS
旨在透過一個「非人工」「自動化」的方法去尋找最佳的網路組合。</p>
<p>keywords: NAS、NASNet <span id="more"></span></p>
<h2 id="什麼是-nas">什麼是 NAS ？</h2>
<p>那倒底什麼是 NAS 呢？NAS
的目的就是希望以一套演算法能<strong>自動的根據我們的需求找到效果最好的網路架構</strong>，而一個
NAS 一共可以包含以下三個部份：Search Space、Search Strategy、Performance
Estimation Strategy</p>
<h3 id="search-space">Search Space</h3>
<p>也就是我們在尋找網路架構時由「基本單位」所組成的尋找空間。一個「基本單位」可以是一個
conv 層、一個 kernel size、chennel size、一個
normalization…等等，透過像堆積木的方式把所有在尋找空間中的「基本單位」<strong>拼</strong>成一個網路</p>
<h3 id="search-strategy">Search Strategy</h3>
<p>在一定的 Search Space
中，我們總不可能所有排列組合都試一邊，可能性太多種了，因此我們需要一個演算法來找出最好的排列組合。可以是
Greedy Search、Evolution Algorithm 等等…</p>
<h3 id="performance-estimation-strategy">Performance Estimation
Strategy</h3>
<p>透過 Search Strategy
<strong>拼</strong>出來的網路架構後需要一個評斷這個網路好壞的方法，可能是分類的
Accuracy、偵測的 mIoU 等等…，最高的 Performance Estimation
即為我們要的網路</p>
<h2 id="google-nas">2016 Google NAS</h2>
<p>在 2016 年 11 月 Google 是最先提出 NAS (Neural Architecture Search)
的想法的論文 <a
href="https://arxiv.org/abs/1611.01578">https://arxiv.org/abs/1611.01578</a>，整個流程如下：</p>
<p><img src="https://i.imgur.com/oIyFEH7.png"
alt="image-20220123151846095" /></p>
<p>使用一個由 RNN 構成的 controller (左粉紅色區塊)，會以一個機率 <span
class="math inline">\(p\)</span> 隨機找出一個網路 <span
class="math inline">\(A\)</span> (上線流程)，把網路 <span
class="math inline">\(A\)</span> 在 CIFAR-10 小資料夾上做預訓練並得到
Accuracy <span class="math inline">\(R\)</span>，接著把 <span
class="math inline">\(R\)</span> 當作是 Backbropagation 去更新 RNN
中的參數權重，重覆以上動作直接 RNN
網路收斂。以上的做法是使用到強化式學習 (Reinforcement Learning)
的精神，把 RNN 當成是 controller 去不斷自我訓練。</p>
<p>在最原始的 NAS 中，Search Space 中都是一些超基本的參數：CNN Filter
(Kernel) 的長寬、數量、Stride 數量：等等，每一個變量都當做是 RNN
中的一個神經元，透過強化式學習找出效果最好的組合</p>
<p><img src="https://i.imgur.com/hzO2Z5m.png"
alt="image-20220123155242514" /></p>
<p>至於 Search Strategy
這邊就不細講，有興趣的人可以自己去參考下面這篇知乎的文章：</p>
<p><a href="https://zhuanlan.zhihu.com/p/52471966">NAS 知乎詳解</a></p>
<h2 id="nasnet">NASNet</h2>
<p>以上 NAS 會有一個問題：付出的運算成本太大了，又要在一大堆 Search
Space 中找排列組合，又要評估好壞，因此只能實作在較小的資料集如 CIFAR-10
上，以 Google 的例子光 CIFAR-10 就要 500 台 GPU 運行 28
天才有結果，如果要應用在大資料集如 ImageNet 上那更不知需要多大的 GPU
了。</p>
<p>而為了能將 NAS 應用在大資料集上 Google 又在 2017 年提出
NASNet，其最大的不同在於把 Search Space
裡中的單位「變大」，也就是稍微變得複雜了點，不像 NAS 中一個基本元件是
kernel 的長、寬…等等，NASNet
的基本元件是已經組裝好的，例如下圖所示：</p>
<p><img src="https://i.imgur.com/4EV40JP.png"
alt="image-20220123161850799" /></p>
<p>而 NASNet 的基本元件又分為兩種：Normal Cell、Reduction
Cell，簡單來說前者不會降維、後者則會，用意是模仿 ResNet
會慢慢降低解析度。如下圖：</p>
<p><img src="https://i.imgur.com/3GXfKxF.png"
alt="image-20220123162428688" /></p>
<p>不管是 Normal Cell、Reduction Cell，一個 Cell
都由五個步驟所組成，分別對應下圖的「灰色 x2」「黃色 x2」「綠色」</p>
<p><img src="https://i.imgur.com/HxpXrU5.png"
alt="image-20220123162908703" /></p>
<p>NASNet 的詳細流程為：</p>
<ol type="1">
<li>「灰一」從之前已經訓練好的第 <span
class="math inline">\(h_i\)</span> 層中，選擇一個 Feature Map 作為
Hidden Layer A 的輸入</li>
<li>「灰二」再從第 <span class="math inline">\(h_{i-1}\)</span>
層中，再選擇一個 Feature Map 作為 Hidden Layer B的輸入</li>
<li>「黃一」從 Search Space 中選擇一個運算給 Hidden Layer A</li>
<li>「黃二」從 Search Space 中選擇一個運算給 Hidden Layer B</li>
<li>「綠」把 Hidden Layer A、B，用 Concat 或 Element-wise
的方法加起來</li>
</ol>
<p>為了能讓 RNN controller 同時找出 Normal Cell 與 Reduction Cell，RNN
會有 <span class="math inline">\(2\times 5\times B\)</span>
個輸出，其中前 <span class="math inline">\(2\times 5\)</span> 作為
Normal Cell，後 <span class="math inline">\(2\times 5\)</span> 作為
Reduction Cell</p>
<p>找出來效果最好的網路架構如下，可以看看參考一下</p>
<p><img src="https://i.imgur.com/hIYg2lO.png"
alt="image-20220123163851151" /></p>
<p>同樣，對於 NASNet 的 Search Strategy
這邊不做多述，有興趣的人可以參考下面知乎文章：</p>
<p><a href="https://zhuanlan.zhihu.com/p/52616166">NASNet
知乎詳解</a></p>
<h2 id="結論">結論</h2>
<p>NASNet 解決了 NAS
無法應用在大資料集上的問題，同時也因把「基本單位」變複雜了，也可以換句話說：「基本單位」由人工設計組合給定死了，雖然網路的彈性下降，但減少了
Search Space 所代來最大的好處就是運算時間變少了。</p>
<p>而 NASNet 這一篇論文也有達到當時的 SOTA，也開起了後續 AutoML
領域的發展，也更是為後面另一個強大的論文 EfficientNet 奠定基礎</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://medium.com/ai-academy-taiwan/%E6%8F%90%E7%85%89%E5%86%8D%E6%8F%90%E7%85%89%E6%BF%83%E7%B8%AE%E5%86%8D%E6%BF%83%E7%B8%AE-neural-architecture-search-%E4%BB%8B%E7%B4%B9-ef366ffdc818">medium
NAS 大全筆記</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/52471966">NAS 知乎詳解</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/52616166">NASNet
知乎詳解</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>AutoML</tag>
      </tags>
  </entry>
  <entry>
    <title>NFNet: High-Performance Large-Scale Image Recognition Without Normalization</title>
    <url>/2022/02/12/NFNet-High-Performance-Large-Scale-Image-Recognition-Without-Normalization/</url>
    <content><![CDATA[<p>DeepMind 在 2021 年 2 月提出一篇以 CNN based 的
NFNet，旨在把深度學習中已經使用已久的 Batch Normalization
去掉，希望能藉此建構出 Normalize-Free 的網路架構 (正是 NFNet
的名稱由來)</p>
<p>並提出代替 BN 的 AGC (自適應梯度修剪 Adaptive Gradient
Clipping)，在調整梯度大小上有著不錯的效果</p>
<p>在手動選用 SE+ResNeXt 網路下，並加上 AGC 的加持，NFNet
成功達到了當前的 SOTA</p>
<p><a
href="https://arxiv.org/pdf/2102.06171.pdf">https://arxiv.org/pdf/2102.06171.pdf</a></p>
<p>keywords: NFNet、AGC <span id="more"></span></p>
<h2 id="batch-normalization-的缺點">Batch Normalization 的缺點</h2>
<p>首先來看看為什麼這篇論文要把 BN 給去掉，BN
做為算是深度學習中的基石元件，倒底發生了什麼事情呢？</p>
<p>本篇論文給出了以下三點缺點：</p>
<ol type="1">
<li><strong>BN 需要額外的計算及記憶體資源</strong>。在計算一個 mini
batch 之間的 <span class="math inline">\(\mu\)</span> 及 <span
class="math inline">\(\sigma\)</span> 會需額外保存它的臨時變量</li>
<li><strong>BN 會使得網路在訓練及測試時會有差異
(discrepency)</strong>。也就是 pytorch 中的 <code>model.train()</code>
和 <code>model.eval()</code> 的差異，資料在進網路參數時會因為 BN
而有不同的行為模式，會需要用一個隱藏參數來調整</li>
<li><strong>BN 破壞了資料樣本之間的獨立性</strong>。BN 與 Batch Size
有絕對的關系，當 Batch Size
越大越能反應真實資料的分佈，效果越好，反之越差。換句話說，網路訓練的好壞會與資料的選擇有關</li>
</ol>
<p>另外還有一點 (我自己多認為的)，在 pytorch DDP (分佈式訓練) 中，BN
的存在會使得不同機器上的資料分佈不同，各個機器最後在整合資訊時，會出現一定程度上資料不合的問題</p>
<h2 id="batch-normalization-的優點">Batch Normalization 的優點</h2>
<p>雖然 BN 有上述種種小問題，但是在先把 BN 去掉之前也要先認識一下 BN
倒底有哪些優點，才會讓它在深度學習獨霸一時</p>
<p><strong>BN 會降低 Residual Branch 的隱參數權重</strong>。所謂
Residual Branch 就是 ResNet 中的「主要網路塊」，如圖最中間網路塊</p>
<p><img src="https://i.imgur.com/LPHRZr4.png"
alt="image-20220214132956188" /></p>
<p>而在 Residual Branch 中加入 BN 可以有效的使 ResNet
中大部份的資料流，流向 Skip
Connection，使得資料得以往網路深層前進，加深網路的層數。也可看成加入 BN
後，會使得主支線的輸出非常小，經 <span
class="math inline">\(\mathcal{F}(x)+x\)</span>
公式後，網路下一層的輸入的初始值會與上一層網路差不多 <span
class="math inline">\(x\)</span></p>
<p><strong>BN
會減少資料分佈</strong>。如果資料分佈非常鬆散，會使得網路非常難收斂，非常難訓練。也可說
BN 可以有效的平滑 landscape。</p>
<p><strong>BN 有正規化的效果、BN 在訓練大 Batch Size
比較有效果</strong>。與上一點相當，可以使網路的 landscape
平滑化，進而在設定 learning rate 時可以調大一點，再進而可以加速網路訓練
(但不會加強太多效果)</p>
<h2 id="nf-resnet">NF-ResNet</h2>
<p>其實早在 2018-2019 年，就有人陸陸續續提出不含 BN
的網路架構了，但基本上都沒辨法達到當前的 SOTA。2021 年同樣也是由
Deepmind 提出 NF-ResNet，而本篇的 NFNet
正是由「自家」的網路加以修改而來。</p>
<p>而 NF-ResNet 最核心的理念如下圖：</p>
<p><img src="https://i.imgur.com/BpN2e3j.png"
alt="image-20220214144159705" /></p>
<p>將 ResNet 的公式，新增了兩個超參數 <span
class="math inline">\(\alpha\beta\)</span> ，修改如下： <span
class="math display">\[
\begin{gather}
x=\mathcal{F}(x)+x \rightarrow\\
h_{i+1} = h_i+\alpha f_i(h_i/\beta_i)
\end{gather}
\]</span> 簡單來說 <span class="math inline">\(\alpha\)</span>
為經網路後的加權值，通常都設得很小 0.2，<span
class="math inline">\(\beta\)</span>
為預測輸入的標準差。加入以上兩個超參數的用意是為了模仿 BN
使大部份資料流向 Skip Connection</p>
<h2 id="adaptive-gradient-clipping-agc">Adaptive Gradient Clipping
(AGC)</h2>
<p>而本篇 NFNet 網路是架構在 NF-ResNet 之下的，並且提出了 Adaptive
Gradient Clipping，將 NFNet 可訓練的 Batch Size 進一步增大</p>
<p>首先什麼是 Gradient Clipping？Gradient Clipping
白話來說就是：為了使網路訓練穩定，當梯度下降的量值過大於一定設定值時，而強迫它改為一固定常數。公式如下：
<span class="math display">\[
G\rightarrow \left\{
\begin{array}{ll}
\lambda\frac{G}{||G||} &amp;\mathrm{if} ||G||&gt;\lambda,\\
G&amp;\mathrm{otherwise.}
\end{array}
\right.
\]</span> <img src="https://i.imgur.com/k0UiJNV.png"
alt="image-20220214152848254" /></p>
<p>但以上公式會有一個問題：<span class="math inline">\(\lambda\)</span>
的值非常敏感，太大太小效果都不好</p>
<p>於是本篇作者提出：可自適應調整 <span
class="math inline">\(\lambda\)</span> 的 Gradient Clipping，公式如下：
<span class="math display">\[
G^l_i\rightarrow\left\{
\begin{array}{ll}
\lambda\frac{||W^l_i||^*_F}{||G^l_i||_F}G^l_i &amp; \mathrm{if}
\frac{||G^l_i||}{||W^l_i||^*_F}&gt;\lambda,\\
G^l_i&amp;\mathrm{otherwise.}
\end{array}
\right.
\]</span> 其函意如下：</p>
<p><span class="math inline">\(G^l\)</span> 為一層中算出來的梯度，<span
class="math inline">\(W^l\)</span> 為一層中目前的權重值</p>
<p>當「算出來的梯度」與「目前權重值」的比值大於 <span
class="math inline">\(\lambda\)</span> 就進入 Clipping</p>
<p>Clipping 多少呢？Clipping 「算出來的梯度」乘上剛剛比值的「倒數」</p>
<p>意義為：我們的上限決定值加入當前權重變量，如果前一時刻權重變化大，梯度計算也大，那比值小代表還算合理；如果前一時刻權重變化小，梯度計算大，比值就會超大，進入
Clipping</p>
<p>利用這個方法就可以自動的來調整 <span
class="math inline">\(\lambda\)</span>，那為什麼加入 AGC 可以改善沒有 BN
的問題呢？前面也有提到了，少了 BN 網路中的 landscape 非常崎嶇
(以下為示意圖，非本例子)，在非常崎嶇下梯度常常算一算就跑掉了，因此才需加入
AGC 穩定訓練</p>
<p><img src="https://i.imgur.com/pRc9S3z.png"
alt="image-20220214163945964" /></p>
<h2 id="效果">效果</h2>
<p>下圖為：使用 BN (藍)、使用 NF-ResNet 沒有 AGC (橘)、使用 NFNet 有 AGC
(綠)，在不同 Batch Size 下的 Top1 準確率</p>
<p><img src="https://i.imgur.com/QvmDKAQ.png"
alt="image-20220214154715148" /></p>
<p>可以發現未加 AGC 以及 未加 BN 的 NF-ResNet 在 Batch Size 超過 2048
後就爆掉了，而加入 AGC 的 NFNet 可以很好的解決不使用 BN Batch Size
不能設太大的問題</p>
<h2 id="網路架構">網路架構</h2>
<p>相較之下本篇的網路架構比較不是重點，作者為了能夠把本架構刷到 SOTA
而使用了 SE-ResNeXt-50。詳細的架構圖如下：</p>
<p><img src="https://i.imgur.com/YrN8TP5.png"
alt="image-20220214191340591" /></p>
<p>左手為 Transition Block，右手為 Non-Transition Block</p>
<h2 id="實驗結果">實驗結果</h2>
<p>首先來看 SOTA 表</p>
<p><img src="https://i.imgur.com/PyChVBf.png"
alt="image-20220214191548689" /></p>
<p>雖然說使用了 SE-ResNeXt-50 所以效果才那麼好，但作者還是有做 Ablation
實驗，看看使用 AGC 是不是真的有比較好？</p>
<p><img src="https://i.imgur.com/komL4Bd.png"
alt="image-20220214191855754" /></p>
<p>實驗證明在相同架構下，使用 AGC 效果有好大約 1%</p>
<h2 id="結論">結論</h2>
<p>本篇成功的提出把 BN 去掉的網路架構 NFNet，並且也刷上了當前的 SOTA
(力抗 Transformer 架構 XD)</p>
<p>但是我覺得是不是有達到 SOTA 到不是其次，那是因為作者使用了
SE-ResNeXt-50 (沒有為什麼) 才有可能的</p>
<p>而是這篇論文提出了另一個不用 BN 效果也不差的方法 AGC，使得 AGC
網路也有 BN 網路的「Batch Size 大」「訓練快」「landscape
平滑」等優點</p>
<p>至於 BN 真的有沒有必要去掉呢？我認為在普通情況下其實差不多，但在 DDP
分佈式訓練上，或許就是 AGC 的天下了</p>
<h2 id="reference">Reference</h2>
<p><a href="https://www.youtube.com/watch?v=rNkHjZtH0RQ">Yannic Kilcher
論文圖解 (英文) (推)</a></p>
<p><a
href="https://blog.csdn.net/zhouchen1998/article/details/113824617">csdn
文章 (我覺得這篇很詳細)</a></p>
<p><a
href="https://medium.com/ching-i/nfnet-normalizer-free-resnets-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80-ce7235d1b123">medium
文章</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/358228383">知乎大神</a></p>
<p><a href="https://arxiv.org/pdf/2101.08692.pdf">NF-ResNets
arxiv</a></p>
<p><a href="https://arxiv.org/pdf/1712.09913.pdf">Visualizing the Loss
Landscape of Neural Nets (landscape 圖)</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Self-supervised Learning 與 Contrastive Learning 速讀</title>
    <url>/2022/02/17/Self-supervised-Learning-%E8%88%87-Contrastive-Learning-%E9%80%9F%E8%AE%80/</url>
    <content><![CDATA[<p>2018 Google 提出 BERT，給 NLP
下了一個定心丸，同時也證明了無監督學習以及預訓練的潛力</p>
<p>但是在坐穩監督式學習的 CV
中，似乎不論如何無監督學習始終超越不了有監督式學習，但是收集資料以及標記資料所花的成本也偷偷在告訴我們無監督的強項</p>
<p>其實早在 2006 年，AI 大佬 LeCun
就曾提過類似的想法了，並且在日後還說出：<code>self-supervised learning is the future of ai</code>
這番很有野心的話，可見大家對於它的期望還是還高的</p>
<p>隨著時間的進展，無監督式學習被 NLP
玩得走火入魔，也慢慢誕生出了新名詞：Self-supervised Learning
自監督學習，這個詞是 LeCun
自己這麼叫的，目的是為了和無監督式學有個區分，但本質上又有哪一點點類似</p>
<p>後來有了 Contrastive Learning、以及 MoCo SimCLR 的提出，應用在 CV
上的自監督學習似乎也在慢慢成長起來…</p>
<p>keywords: Self-supervised Learning、Contrastive Learning
<span id="more"></span></p>
<h2 id="什麼是自監督學習-self-supervised-learning">什麼是自監督學習
(Self-Supervised Learning)</h2>
<p>自監督學習是無監督學習的一種分支，主要是利用輔助任務
(pretext)，先使用一大堆無標記的資料中挖掘自身的資訊，再來把得到的資訊放到下游任務中做進一步的分析
(Pretrain -&gt; Finetune)</p>
<p>與無監督學習最大的差別在於「挖掘自己的資訊」，最後的特徵結果是從自己與自己相互比較得出來的</p>
<p><img src="https://i.imgur.com/9VsiOqR.jpg"
alt="v2-8d077a997287e6fc7f9b5576b3e16f00_720w" /></p>
<p>大致上來說自監督學習可為兩類：生成式以及判別式</p>
<p><strong>生成式</strong>的代表任務有：GAN、VAE、ELMo、BERT、GPT…。期望能利用數據重新生成一張新的數據。目前在
NLP
上非常流行，但是在影像的本質上不像語言，「理解」後就可以「實行」出來，語言理解了後我們都會說，但理解了一張圖片我們不一定能「畫」出來。如下圖：能知道什麼是鈔票但是畫不出來</p>
<p><img src="https://i.imgur.com/zLU4fD7.png"
alt="image-20220218173741384" /></p>
<p><strong>判別式</strong>的代表任務有：MoCo、SimCLR…。利用無監督數據，自行建立學習任務以及樣本，最後得到數據的向量表示。而判別式
SSL 應用在 CV
又可分為三種方法：基於背景的輔助任務、基於時序的輔助任務、基於對比學習</p>
<p>聽不懂上面在講什麼嗎
XD，用兩句話來解釋的話就是。<strong>生成式</strong>，輸入一張圖片，通過
Encoder Decoder
還原輸入圖片資訊。<strong>判別式</strong>，輸入兩張圖片，通過
Encoder，判斷兩張圖是否相似 0 or 1</p>
<p><img src="https://i.imgur.com/3wJlaGs.png"
alt="image-20220221205043036" /></p>
<h2 id="基於背景的輔助任務-pretext">基於背景的輔助任務 (pretext)</h2>
<p>也可說是基於上下文 (context based) 的方法，在 NLP
中已經玩得非常成熟了，像 Word2Vec 就是基於前後文的順序來預測。而 CV
中也有非常多的論文也提出了相關的做法，下面就來簡單的掃過一遍：</p>
<h3 id="拼圖任務">拼圖任務</h3>
<p>目的是預測兩 Patch 之間的順序關系，流程如下圖：給定一藍色 Anchor
周圍的 9 個紅色 Patch，則藍色與紅色的相對位置關系是什麼？<a
href="https://arxiv.org/pdf/1505.05192.pdf">Unsupervised Visual
Representation Learning by Context Prediction</a></p>
<p><img src="https://i.imgur.com/lThZLyw.png"
alt="image-20220218210930235" /></p>
<p>也可以如下下圖：隨機給兩個綠色 Patch 看看彼此的相對位置如何？<a
href="https://arxiv.org/pdf/1603.09246.pdf">Unsupervised Learning of
Visual Representations by Solving Jigsaw Puzzles</a></p>
<p><img src="https://i.imgur.com/l6CMosM.png"
alt="image-20220218211619868" /></p>
<p>第一個方法一共有 8 種可能，第二個方法一共有 64
種可能，而且方法二效果好於方法一，於是得到了一個啟發：<strong>使用更強的監督訊息，或說更難的輔助任務，最後網路學到的東西更多，效果更好</strong></p>
<h3 id="挖空任務">挖空任務</h3>
<p>目的是要預測被挖去的內容是什麼？如下圖。這件事也啟發<strong>自監督學習不僅可以學習到特徵，還同時也得到一些神奇的效果</strong>。<a
href="https://arxiv.org/pdf/1604.07379.pdf">Context Encoders: Feature
Learning by Inpainting</a></p>
<p><img src="https://i.imgur.com/Yqxpyky.png"
alt="image-20220218212025594" /></p>
<h3 id="顏色預測">顏色預測</h3>
<p>也可以輸入灰階圖，要預測圖片的顏色。<a
href="https://arxiv.org/pdf/1603.08511.pdf">Colorful Image
Colorization</a></p>
<p><img src="https://i.imgur.com/reOSCRu.png"
alt="image-20220218214019298" /></p>
<h3 id="圖片旋轉預測">圖片旋轉預測</h3>
<p>也可以把圖片轉成各種角度，並預測出對應的角度。<a
href="https://arxiv.org/pdf/1803.07728.pdf">Unsupervised Representation
Learning by Predicting Image Rotations</a></p>
<p><img src="https://i.imgur.com/yi0dvNc.png"
alt="image-20220218214640977" /></p>
<h3 id="解耦特徵互相學習">解耦特徵互相學習</h3>
<p>把原始的數據分成兩個部份，各做一個圖片的修改，並使它們互相學習，就可以達到自監督學式的目標。<a
href="https://arxiv.org/pdf/1611.09842.pdf">Split-Brain Autoencoders:
Unsupervised Learning by Cross-Channel Prediction</a></p>
<p><img src="https://i.imgur.com/0yD1bpB.png"
alt="image-20220218214941924" /></p>
<h3
id="與任務相關的自監督學習-task-related-self-supervised-learning">與任務相關的自監督學習
(Task Related Self-Supervised Learning)</h3>
<p>在以上種種選擇自監督學習的輔助任務，會希望越接近下游任務的目標越好，如果差太多的話，效果可能會不盡其想。所以開始有了「與下游任務結合」的自監督學習想法。像下圖的做法，也很直覺，把圖片的旋轉也看成是下游分類任務的其中一類，要同時預測物體以及旋轉角度</p>
<p><a href="https://arxiv.org/pdf/1910.05872.pdf">Self-supervised Label
Augmentation via Input Transformations</a></p>
<p><img src="https://i.imgur.com/4V7YB2j.png"
alt="image-20220218215439870" /></p>
<h2 id="基於時序的輔助任務">基於時序的輔助任務</h2>
<p>除了一張一張圖片之外，我們也可以對有「時間相關」的資料做輔助任務，例如影片、音樂、聲音…。</p>
<h3 id="影片上的時序">影片上的時序</h3>
<p>我們可以把一個影片中相近的 frame 看成是有相關的樣本、相遠的 frame
是不相關的樣本，或是放多個攝影機，同個角度拍出來的相關樣本，不同角度拍的是不相關樣本</p>
<p><img src="https://i.imgur.com/oaCxxIg.png"
alt="image-20220218221520253" /></p>
<h2 id="對比學習-contrastive-learning">對比學習 Contrastive
Learning</h2>
<p>通過數據之間的對比來學習特徵，就好像以面這句話一樣：We don't know
something is blue until we see
red，沒有比較我們就永遠不知道類別差在哪裡。而核心理念很簡單：<strong>相似的影像結果也要相似，不相似的影像結果也要不相似</strong>，用數學公式來表達會是：其中
+ 是指正樣本，相似的樣本，- 是指負樣本，不相似的樣本</p>
<p><span class="math display">\[
\mathrm{score}(f(x),f(x^+)) &gt;&gt; \mathrm{score}(f(x), f(x^-))
\]</span> <img
src="https://1.bp.blogspot.com/--vH4PKpE9Yo/Xo4a2BYervI/AAAAAAAAFpM/vaFDwPXOyAokAC8Xh852DzOgEs22NhbXwCLcBGAsYHQ/s1600/image4.gif"
alt="gif" /></p>
<p>以下介紹而常見的損失函數</p>
<h3 id="noise-contrastive-estimation-nce">Noise-contrastive Estimation
(NCE)</h3>
<p><a
href="https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">Noise-contrastive
estimation: A new estimation principle for unnormalized statistical
models</a></p>
<p>把正樣本看成一個類別、負樣本看成一個類別，列出來一個正樣本以及負樣本的
cross entropy，跟二元 cross entropy 其實差不多。其中第一項 <span
class="math inline">\(v^+\)</span> 代表正樣本越大代表越相近，第二項
<span class="math inline">\(v^-\)</span>
代表負樣本越小代表越不相近，加個負號代表越小越好 <span
class="math display">\[
log\,\sigma(u^Tv^+/\tau)+log\,\sigma(-u^Tv^-/\tau)
\]</span></p>
<h3 id="infonce">infoNCE</h3>
<p><a href="https://arxiv.org/pdf/1807.03748.pdf">Representation
Learning with Contrastive Predictive Coding</a></p>
<p>像是在 NCE 中加入 Softmax，使得上式子中的大小分佈差距加大 <span
class="math display">\[
\mathcal{L}_N = -\mathbb{E}_X[\log\frac{f_k(x_{t+k},c_t)}{\sum_{x_j\in
X}f_k(x_j,c_t)}]
\]</span></p>
<p><span class="math display">\[
I(x_{t+k}, c_t) \geq\log(N)-\mathcal{L}_N
\]</span> 而比較有趣的是，這個 infoNCE 是 MI (Mutual Information) 的
lower bound。MI 其實表示的是「期望值」，是 Pointwise Mutual Information
(PMI) 的期望值，PMI 是個像件機率，公式可定義為：設隨機變數 <span
class="math inline">\((X,Y)\)</span> 是空間 <span
class="math inline">\(X\times Y\)</span>
中的一對隨機變數。他們的聯合分布是 <span
class="math inline">\(p(x,y)\)</span>，邊緣分布分別是 <span
class="math inline">\(p(x)\)</span> <span
class="math inline">\(p(y)\)</span></p>
<p><span class="math display">\[
PMI = \log(\frac{p(x,y)}{p(x)p(y)})
\]</span></p>
<p>而再把機率乘上自己就可以得到期望值 MI</p>
<p><span class="math display">\[
MI=I(X;Y)=\int_Y\int_Xp(x,y)\log(\frac{p(x,y)}{p(x)p(y)})
\]</span></p>
<p>而 MI 也可以用 KL 來列式，兩者的想法正好相同 -&gt;
兩機率分佈之間的關系。對上兩機率的乘積的 KL
差距。當兩集合獨立時，因聯集為 0、乘積也為 0，所以 KL 差距也為 0，<a
href="https://zh.wikipedia.org/wiki/%E4%BA%92%E4%BF%A1%E6%81%AF">相互資訊
wiki</a> <span class="math display">\[
I(X;Y)=D_{KL}(p(x,y)\,||\,p(x)\otimes p(y))
\]</span></p>
<p><img src="https://i.imgur.com/LU0Scoo.png"
alt="image-20220220161230417" /></p>
<p>同時這個 MI 也可以與條件機率有一些公式，如同貝氏圖那樣</p>
<p><span class="math display">\[
\begin{aligned}
I(X;Y) &amp;=\\
&amp;= H(X) - H(X|Y)\\
&amp;= H(Y) - H(Y|X)\\
&amp;= H(X) + H(Y) - H(X,Y) ...
\end{aligned}
\]</span></p>
<p>那為什麼 infoNCE 是 MI 的一個下界呢？因為在 infoNCE 公式中的 <span
class="math inline">\(f\)</span>，會正比於剛剛提到的 MI</p>
<p><span class="math display">\[
\mathcal{L}_N = -\mathbb{E}_X[\log\frac{f_k(x_{t+k},c_t)}{\sum_{x_j\in
X}f_k(x_j,c_t)}]
\]</span></p>
<p>而 <span class="math inline">\(f\)</span> 展開會得到</p>
<p><span class="math display">\[
\begin{aligned}
f_k(x_{t+k},c_t) &amp;=\\
&amp;= p(d=i|X,c_t)\\
&amp;= \frac{p(x_i|c_t)\prod_{l\neq
i}p(x_l)}{\sum^N_{j=1}p(x_j|c_t)\prod_{i\neq j}p(x_l)}\\
&amp;=
\frac{\frac{p(x_i|c_t)}{p(x_i)}}{\sum^N_{j=1}\frac{p(x_j|c_t)}{p(x_j)}}
\end{aligned}
\]</span></p>
<p><span class="math inline">\(c\)</span> 代表 context 正確的目標
(原論文是應用在 NLP 上，所以名稱這樣取)</p>
<p><span class="math inline">\(p(x_i|c_t)\)</span>
代表從正確目標出選出正樣本的機率分佈、<span
class="math inline">\(p(x_l)\)</span> 代表從其它與 c
無關的地方「亂」取的負樣本</p>
<p>給定大 <span class="math inline">\(X=\{x_1,...,x_N\}\)</span>
其中包含 <span class="math inline">\(1\)</span> 個從 <span
class="math inline">\(p(x_i|c_t)\)</span> 選出來的正樣本，與 <span
class="math inline">\(N-1\)</span> 個從 <span
class="math inline">\(p(x_l)\)</span> 選出來的負樣本</p>
<p><span class="math inline">\(p(d=i|X,c_t)\)</span>
的意思是：給定一目標正確 context，與從 <span
class="math inline">\(X\)</span> 中選一個 <span
class="math inline">\(x\)</span> 分佈，是正樣本 <span
class="math inline">\(i\)</span> 的機率為何</p>
<p>所以 <span class="math inline">\(p(x_i|c_t)\prod_{l\neq
i}p(x_l)\)</span> 的意思是：從 c
中選了一個正樣本，其餘選了負樣本的意思</p>
<p>分母的部就是全部的正樣本跟全部的負樣本</p>
<p>最後可以發現我最後推導出來的式子，分子的部分與 PIM
相似，是一個正比的關系，也就是說我們只要去優化這個 infoNCE
就可以順便也優化了 MI</p>
<p>下式 <span class="math inline">\(N\)</span>
指的是負樣本的大小，所以下式可理解為，增加負樣本效果越好，而 MI
也是越大越好 (代表兩者越近)，同時要最小化 infoNCE 的 loss 才能使 MI
最大化 <span class="math display">\[
I(x, c)\ge log(N)-\mathcal{L}_N
\]</span></p>
<h2 id="應用在-cv-的對比學習">應用在 CV 的對比學習</h2>
<p>從上面的結論可知：<strong>負樣本的數量越多，效果越好</strong>，我們一共有兩種方法可以增加負樣本：</p>
<ul>
<li>把之前訓練過的負樣本存起來下次再用 -&gt; Memory Bank、MoCo</li>
<li>直接增大 Batch Size -&gt; SimCLR</li>
</ul>
<p>看開始介紹之前，先來看看最基本的訓練過程。我們要把網路分成兩個部份 q
k、其中 q 是 anchor 錨點、k 是正負樣本，兩個樣本獨立訓練，最後再用一個
loss 函數統一在一起，query 會不斷的去與 key
相比，看看是不是正樣本或負樣本</p>
<p><img src="https://i.imgur.com/75CHVCI.png"
alt="image-20220220164453375" /></p>
<h3 id="memory-bank">Memory Bank</h3>
<p><a href="https://arxiv.org/pdf/1805.01978.pdf">Unsupervised Feature
Learning via Non-Parametric Instance Discrimination</a></p>
<p>在一個 Batch 下，負樣本的數量是一定的 (除非啦…你的 Batch
可以設超大)，在錢不夠的條件下，要如何增大負樣本數呢？</p>
<p>一個做法是把上次訓練的負樣本「向量表示法」存起來，下一階段訓練時再隨機從
memory bank 中拿取一定數量的負樣本，而因為 memory bank
是一個記憶體的概念，所以沒有 Backpropagation 去更新參數</p>
<p><img src="https://i.imgur.com/WyTTGI4.png"
alt="image-20220220165121350" /></p>
<h2 id="結論">結論</h2>
<p>以上快速的帶過自監督學習的歷史，一路從自監督學習 -&gt; 應用在 NLP 上
-&gt; 提出對比學習 -&gt; 應用在 CV
上演進，而其效果也不斷的往監督式學習逼近。我認為對比學習還有很大的進步空間，尤其是看到了
NLP 的成功，大家也不免俗的想要在 CV 上複製一份嘛 XD</p>
<p>下一篇繼續來看看 2020 年由 FaceBook、Google
兩大巨頭所提出對比學習的方法，兩篇都把對比學習往前推了一大步</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://www.bilibili.com/video/BV1v5411x7rD?share_source=copy_web">bilibili
講得很好的對比學習影片</a></p>
<p><a
href="https://www.bilibili.com/video/BV1Sa4y1x7Am?share_source=copy_web">bilibili
自監督式學習 Loss 公式講解 (前半段)</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/108906502">知乎大神自監督學習文章
(本心得大部份都是參考它的，大推)</a></p>
<p><a
href="https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html">自監督學習文章
(英文)</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Self-supervised Learning</tag>
        <tag>Contrastive Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning 對比學習: MoCo 與 SimCLR</title>
    <url>/2022/02/21/Contrastive-Learning-%E5%B0%8D%E6%AF%94%E5%AD%B8%E7%BF%92-MoCo-%E8%88%87-SimCLR/</url>
    <content><![CDATA[<p>本篇接續上篇文章，依照時間順序介紹有關對比學習的論文：MoCo -&gt;
SimCLR -&gt; MoCo v2</p>
<p>keywords: MoCo、SimCLR <span id="more"></span></p>
<h2 id="moco">MoCo</h2>
<p>由 FaceBook 何凱明大神團隊提出 <a
href="https://arxiv.org/pdf/1911.05722.pdf">Momentum Contrast for
Unsupervised Visual Representation Learning</a></p>
<p>上個 Memory Bank 中有個問題，就是 memory bank 中的資料不同步，q
中會隨著每一次 Batch 而更新，隨後放進 bank 中，而 k
因為是去記憶體中直接拿資料所以不參與更新，如果 q
訓練速度快的話，久而久之 memory bank
資料的表示就會出現訓練差異。一個比較直觀的想法是在 k 中也加入一個
encoder 去學習，但是 memory bank 會隨著時候而增加，要做 BackPropagation
的話計算量會越來越大</p>
<p><strong>momentum encoder的輸出會被一個queue儲存起來，取代原本的memory
bank</strong></p>
<p>改進方法是使用了兩個不同的 encoder，q 的 encoder
是從自監督學習學來的特徵，而 k 的 encoder
是基於動量來更新的，會一點一點的更新 k，確保放進 memory bank
中的資料之間不會相差太多 <span class="math display">\[
\theta_k = m\theta_k + (1-m)\theta_q,\,\mathrm{where}\,m=0.999
\]</span></p>
<p><img src="https://i.imgur.com/1pVnv8m.png"
alt="image-20220220172014505" /></p>
<h2 id="simclr">SimCLR</h2>
<p>由 Google Hinton 團隊提出 <a
href="https://arxiv.org/pdf/2002.05709.pdf">A Simple Framework for
Contrastive Learning of Visual Representations</a>，提出了 CL
訓練的一個大框架</p>
<p>與 MoCo 最大的不同在於，SimCLR 不使用計算過程複雜的 memory
bank，memory bank 在實作上最大的困難在於需要使用兩個不同的 encoder
訓練，實作成本相對複雜，而是改使用增大 Batch size
來達到更多負樣本的效果</p>
<p>並且得到了以下三個結果：</p>
<ul>
<li>資料擴增，以及方法的選擇，在自監督式學習中有著相當重要的角色</li>
<li>在網路中新增非線性層，使效果變超好</li>
<li>Batch Size 的增大，在自監督式中提升的效果比監督式提昇更多</li>
</ul>
<p>單一流程是：x 是輸入圖片，會做兩個不同的資料擴增 <span
class="math inline">\(t\)</span> <span
class="math inline">\(t&#39;\)</span>
(下面狗狗為例子，選兩種)，經過一個特徵提取層 (ResNet) 得到 <span
class="math inline">\(h\)</span> 結果，接著在 <span
class="math inline">\(h\)</span> 後面加一個 non-linear 層 (2 層 MLP)
得到 <span class="math inline">\(z\)</span>
。<strong>大個embedding網路執行特徵抽取得到</strong><code>h</code>，接下來使用一個<strong>小的網路投影到某個固定為度的空間得到</strong><code>z</code>。本篇論文發現這個
non-linear 層會有顯著的增加效果</p>
<p><img src="https://i.imgur.com/QqFzOn3.png"
alt="image-20220220174550589" /></p>
<p><img src="https://i.imgur.com/JRjgGk4.png"
alt="image-20220220175013533" /></p>
<p>實際流程是：取一個 Batch 大小為
N，每一個圖片都做兩個不一樣的資料擴增，總數量變為 2N，而 2N 中其中 2
個目標資料為正樣本，其餘 2(N-1) 為負樣本，我們定義正資料的相似度為 cos
相似度 <span class="math display">\[
  \mathrm{sim}(u,v)=\frac{u^Tv}{||u||||v||}
\]</span> 而目標正樣本的 loss 函數定義為，本篇論文稱之 NT-Xent (the
normalized temperature-scaled cross entropy
loss)，分子為正樣本相似度、分母為正樣本與負樣本相似度之合： <span
class="math display">\[
l_{i,j}=-\mathrm{log}\frac{\mathrm{exp}(\mathrm{sim(z_i,z_j)/\tau})}{\sum^{2N}_{k=1}\mathbb{1}_{[k\neq
i]}\mathrm{exp}(\mathrm{sim(z_i,z_k)/\tau})}
\]</span> 而以上的式子，其實與 infoNCE 非常相似，後面也很像是一個
softmax 表示，不同的地方在 infoNCE 相似度是用交差熵，NT-Xent 是使用 cos
相似度。希望後式越大越好，前面加個負號符合 loss 的定義。</p>
<p>最後把 2N 中每兩兩一對做上面的計算後，除以總數量得出總平均 <span
class="math display">\[
\mathcal{L}=\frac{1}{2N}\sum^N_{k=1}[(l(2k-1, 2k), l(2k, 2k-1))]
\]</span></p>
<p>SimCLR 的結果表，其中 2x 4x 代表最後一層 linear 的倍數，可發現 SimCLR
(4x) 已經與監督式學式媲美</p>
<p><img src="https://i.imgur.com/wgBlabt.png"
alt="image-20220220175852964" /></p>
<p>SimCLR 中做了非常非常多的實驗，大概簡單的說一下：</p>
<h4 id="資料擴增結論一">資料擴增結論一</h4>
<p>比較各資料擴增的好壞，作者兩兩對比尋找效果最好的前兩個資料擴增方法，結論是
crop + color distribution 效果最好，作者還發現如果只做 crop
機器可能只關注顏色的大概分佈就好了，被挖掉的內容不重要，這時如果加入顏色改變可以很有效的解決這個問題</p>
<p><img src="https://i.imgur.com/horypbk.png"
alt="image-20220220180327444" /></p>
<p>作者同時也提到了 crop 的妙用，crop 可以同時達到一般 crop 以及鄰近
crop 的應用，如同前面的論文「拼圖任務」中，作者覺得這個 crop
的方法可以同時包含這兩個擴增</p>
<p><img src="https://i.imgur.com/sQ6GHKH.png"
alt="image-20220220180744507" /></p>
<p>最後作者選擇使用隨機
crop、隨機改變顏色、隨機高斯模糊化，來把圖片做擴增</p>
<p>前面也有提到要選 <span class="math inline">\(h\)</span> 而不是 <span
class="math inline">\(z\)</span>
這樣放在下游任務的效果好。作者給的解釋是：雖然經過 loss 的是
g()，可以最小化
loss，但真正要應用的資料是一些特徵，可能資料經過兩個非線性的 g()
後已經沒有資料擴增的特徵在裡面了。</p>
<p><img src="https://i.imgur.com/WUtUUAy.png"
alt="image-20220220181123989" /></p>
<h2 id="moco-v2">MoCo v2</h2>
<p>同樣由 Facebook 提出，有趣的是本篇改版論文緊接著 SimCLR
提出，而改進的部份也大都來自 SimCLR 的 non-linear
概念，論文內容也只有短短的兩頁，火藥味意外的濃厚。<a
href="https://arxiv.org/pdf/2003.04297.pdf">Improved Baselines with
Momentum Contrastive Learning</a></p>
<p>MoCo v2 參考了 SimCLR 的三個優點</p>
<ul>
<li>超大 Batch</li>
<li>多了 non-linear 層</li>
<li>有效的資料擴增方法</li>
</ul>
<p>超大 Batch 的部份 MoCo 已經有 Memory Bank 了所以不用，所以作者把
non-linear 層應用在 MoCo 上面，看看這個框架對於自監督式學習的可行性</p>
<h3 id="增加-non-linear-mlp-層">增加 non-linear MLP 層</h3>
<p>MoCo v2 模仿 SimCLR 在最後面加上一層 MLP 層，可以發現在任何 <span
class="math inline">\(\tau\)</span> (溫度) 下，效果皆好出超多的</p>
<p><img src="https://i.imgur.com/gP7PlXP.png"
alt="image-20220222195315386" /></p>
<h3 id="模仿-simclr-的資料擴增">模仿 SimCLR 的資料擴增</h3>
<p>MoCo v2 還模仿 SimCLR 在顏色變化中加入高斯雜訊，並且也模仿 SimCLR 的
cosine (half-period) learning rate
schedule。不管加入哪一項皆有明顯的提升，而且不管是 batch 或是 epochs
上都有比 SimCLR 優秀</p>
<p><img src="https://i.imgur.com/dmhbTYb.png"
alt="image-20220222200530916" /></p>
<h2 id="reference">Reference</h2>
<p><a
href="https://www.bilibili.com/video/BV1v5411x7rD?share_source=copy_web">bilibili
講得很好的對比學習影片</a></p>
<p><a
href="https://www.bilibili.com/video/BV1Sa4y1x7Am?share_source=copy_web">bilibili
自監督式學習 Loss 公式講解 (前半段)</a></p>
<p><a
href="https://ankeshanand.com/blog/2020/01/26/contrative-self-supervised-learning.html">自監督學習文章
(英文)</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/378953015">科技猛獸大神文章
(知乎)</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Contrastive Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>ConvNeXt: A ConvNet for the 2020s - 模仿 Swin 的 CNN 網路</title>
    <url>/2022/03/15/ConvNeXt-A-ConvNet-for-the-2020s-%E6%A8%A1%E4%BB%BF-Swin-%E7%9A%84-CNN-%E7%B6%B2%E8%B7%AF/</url>
    <content><![CDATA[<p>2021 年是 Transformer
發揚光大的一年，短短的一年間推出了許多新的架構，其中尤其又以 Swin
Transformer 效果最為突出，其效果甚至超越了當前 CNN 的 SOTA。</p>
<p>2022 年 FAIR 重新探討了 CNN 與 Transformer
之間的關系，試著建立出一個「很像 Transformer 的 CNN 網路」，提出了基於
ResNet 魔改的 ConvNeXt。經實驗得知只使用 CNN 架構的效果就超越了 Swin
Transformer。</p>
<p>keywords: ConvNeXt <span id="more"></span></p>
<h2 id="前言">前言</h2>
<p><img src="https://i.imgur.com/8jMGxop.png"
alt="image-20220316194754349" /></p>
<p>自從 2020 10 月 ViT 提出後，Vision Transformer based 的方法開始屠榜
SOTA，CNN 好像被遺棄了一般，大家一窩峰的去研究 Transformer。而後 2021 3
月 Swin 被提出，Swin 基於「計算量過大」「缺少多重解析度」以上兩個理由把
CNN 的一些想法引進到 Transformer 中。從這之後，更多的論文試著把 CNN
的獨有的想法融入到 Transformer 中。有趣的是到了 2022 的現在，FaceBook
這篇論文反過來思考：<strong>如果我們是把 Transformer 的特色融入到 CNN
中呢？</strong>於是誕生了這篇論文</p>
<p>這篇論文的核心理念是：現在 Transformer 架構之所以好的原因可能不只是在
Self-attention 上而已，<strong>Transformer
特有的訓練技巧也是效果好的原因之一</strong>，我們能不能不斷的優化 CNN
的訓練來取得更好的效果呢？</p>
<p>其實早在 2021 10 月著名的 <a
href="https://github.com/rwightman/pytorch-image-models">pytorch image
model - timm</a> 套件作者 Ross Wightman 就提出了篇論文：<a
href="https://arxiv.org/abs/2110.00476">ResNet strikes back: An improved
training procedure in timm</a>，它的核心想法是把經典的 ResNet-50
用新的訓練想法來練 (Mixcut 資料擴增、LARS
optimizer)，在相同網路架構下成功的把 ImageNet 分類問題準確率提升到
80.4%</p>
<p>可以參考以下的文章有更詳細的說明：</p>
<p><a
href="https://www.zhihu.com/question/492966803/answer/2176330600">如何看待timm作者发布ResNet新基准：ResNet50提至80.4，这对后续研究会带来哪些影响？</a></p>
<p><a
href="https://www.youtube.com/watch?v=Gl0s0GDqN3c&amp;ab_channel=TheAIEpiphany">ResNet
Strikes Back! | Patches Are All You Need? | Papers Explained</a></p>
<p>以上實驗也間接說明了一件事情：其實 CNN
架構還有很好的優化空間，在適當的優化下是可以提升不少點的</p>
<h2 id="introduction">Introduction</h2>
<p>如同前面提到的，這篇論文試著參考 Transformer 的訓練流程來套用到 CNN
上，優化 CNN 使其能得到更好的效果。並把結果網路取名叫 ConvNeXt，且是一個
pure-CNN 架構。</p>
<h2 id="魔改-resnet">魔改 ResNet</h2>
<p>作者使用 ResNet-50 以及 ResNet-200 作為 baseline
當作網路魔改的起始點。</p>
<p>ResNet 一共經過四次修改最後成為了 ConvNeXt，不管是最後效果或是 FLOPs
都對標了當前最強的 Swin，在相同運算量下取得更好的效果</p>
<p>下面是修改流程圖：以下一步一步來解說</p>
<p><img src="https://i.imgur.com/Jdm64HD.png"
alt="image-20220316193217265" /></p>
<h2 id="訓練技巧-traning-techniques">訓練技巧 Traning Techniques</h2>
<p>與 CNN 不同的是 Transformer 使用的一些訓練方法都比較新穎，ResNet
畢竟也是 2015 年的產物了，這個實驗想試試看，如果 ResNet 使用了 DeiT 與
Swin 的方法是不是效果會有所改變？</p>
<p>詳細的改變有：epochs 變 300、使用 AdamW optimizer、使用 Mixup Cutmix
RandAugment 等資料擴增、Stochastic Depth、Label Smoothing</p>
<p>最後結果把 ResNet-50 的效果從 76.1% 提升到 78.8% (+2.7%)</p>
<p>詳細的訓練參數：</p>
<p><img src="https://i.imgur.com/ljMhHe2.png"
alt="image-20220316201155238" /></p>
<h2 id="大架構修改-macro-design">大架構修改 Macro Design</h2>
<h3 id="改變各-stage-的層數比例">改變各 Stage 的層數比例</h3>
<p>在 Swin-T 中一共有 4 個 Stage，分別做 Self-attention 的比例是
1:1:3:1；更大一點的架構則是 1:1:9:1。而原始 ResNet-50 層數比例也從 3: 4:
6: 3 修改成 3: 3: 9: 3</p>
<p>最後結果把 ResNet-50 的效果從 78.8% 提升到 79.4%</p>
<p>(不過這個性能提升也可能是來自於 FLOPs 的增加…)</p>
<h3 id="修改網路最初架構-stem-的運算">修改網路最初架構 (stem)
的運算</h3>
<p>Swin-T 在網路最一開始做 Patch
Embedding，把三維影像轉換為二維序列，而其核心的運算其實是用到了一個 4x4
的大 kernel 來實現的</p>
<p>而 ResNet 的最初的運算稱做 stem 它較為複雜一些，是用一個 7x7 kernel
with stride 2，再一個 max pool 來達成</p>
<p>作者直接把 Swin 的做法放到 ResNet 上面，也就是 4x4 kernel with stride
4 (也可看成不重疊的 kernel)。把 Patch Embedding 的想法套用到 ResNet
上面。</p>
<p>最後結果把 ResNet-50 的效果從 79.4% 提升到 79.5%
(提升了一點點點而已)</p>
<h2 id="resnext-化">ResNeXt 化</h2>
<p>ResNeXt 引入了 Grouped
Convolution，利用<strong>增加網路寬度</strong>的方法來提升效果，而
Grouped Convolution 的極端就是一個 channel 一個 Grouped，而這就是
Depthwise Convolution 的想法。</p>
<p>作者把 ResNet 的卷積層全部換為 Depthwise
Convolution，理所當然的因為計算量的下降，最後的效果也下降了，但同時也把經
stem 後的 channel 數量從 64 提升至 96，與 Swin-T 一模一樣</p>
<p>這一加一減的操作下，最後結果把 ResNet-50 的效果從 79.5% 提升到
80.5%</p>
<p>作者在論文中提到：Depthwise Convolution 與 Self-attention
的比較。與之前我有寫過的 <a href="">MobileViT</a>
有相同的想法，其實這兩個東西是相似的。Depthwise Convolution 是對 kernel
裡面的特徵算加權和，可看成是 local attention，而 Self-attention 則沒有
kernel 的限制，是 global attention。這兩個最的區別在於：Depthwise
Convolution 就是固定學習 kernel 中權重，而 Self-attention
因一次看整張圖片，因此權重是動態的。可以參考 Microsoft 的論文 <a
href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2106.04263">Demystifying
Local Vision Transformer: Sparse Connectivity, Weight Sharing, and
Dynamic Weight</a> 有深入的分析。</p>
<h2 id="inverted-bottleneck">Inverted Bottleneck</h2>
<p>作者把 Depthwise Convolution 當成是 Self-attention Layer 看，並模仿
ViT 的整體架構。首先採用 inverted bottleneck 用兩個 1x1 conv 放大 4
倍再縮小 4 倍 (下圖 b)，接著把 Depthwise Convolution 層移到第一層輸入處
(下圖 c)，模仿下下圖 ViT 的 Self-attention -&gt; MLP 形式</p>
<p><img src="https://i.imgur.com/X2jp898.png"
alt="image-20220317011954779" /></p>
<p><img src="https://i.imgur.com/Wgpj9Ii.png"
alt="image-20220317012304797" /></p>
<h2 id="增大-kernel-size">增大 kernel size</h2>
<p>因為 Swin-T 的 window size 是 7x7，但是自 VGG 提出以來都是使用
3x3，因為有著更低的運算量，及更多非線性轉換。繼然要模仿那就要模仿到底阿，於是作者設計了不同的
kernel 大小 3x3 5x5 7x7 11x11</p>
<p>經實驗發現效果為 79.9% (3×3) -&gt; 80.6% (7×7)，使用 7x7
會使效果變更好 (這是當然在相同層數下使用 7x7
運算複雜度較高)。但是這高計算量被上一步的 inverted bottleneck 圖c
設計兩兩相互抵消</p>
<h2 id="其它小改動">其它小改動</h2>
<h3 id="把-relu-換成是-gelu">把 ReLU 換成是 GELU</h3>
<p>也把 activation function 換成 NLP 常用的 GELU，作者經實驗發現，在
ConvNeXt 架構下效果差不了多少</p>
<h3 id="更少-activation-functionnormalization-層">更少 activation
function、normalization 層</h3>
<p>以前 CNN 每一個 conv 後都會接一層 BN、ReLU 層，而現在只會在 Depthwise
Convolution 後加 LN，在 inverted bottleneck 中加入 GELU。如下圖：</p>
<p>這個操作把效果提高到 81.4% 已經超越了 Swin-T 的效果了</p>
<p><img src="https://i.imgur.com/XuEdpUH.png"
alt="image-20220317014901729" /></p>
<h3 id="把-bn-換成-ln">把 BN 換成 LN</h3>
<p>BN 的種種缺點我在 NFNet 這篇論文中有提過了，但是在影像上 BN
仍然有它優勢在。把 BN 替換成專為 NLP 設計的
LN，在這篇論文實驗下效果差不多，從 81.4% -&gt; 81.5%</p>
<h3 id="修改-downsampling-下採樣的策略">修改 downsampling
下採樣的策略</h3>
<p>ResNet 中是使用 3x3 with stride 2 來達成減少特徵圖維度，而在 Swin
中是 2x2 conv with stride 2。於是 ConvNeXt 完全模仿 Swin 使用 2x2 conv
with stride 2。經實驗證明效果從 81.5% 提升至 82.5%，是個大提升呢</p>
<p>而這個就是最後魔改 ResNet 後的架構 ConvNeXt
了，最後再來一張總表整理一下所有 trick
對應的分類效果與計算量的改動：</p>
<p><img src="https://i.imgur.com/d8oxmVq.png"
alt="image-20220317015934289" /></p>
<h2 id="experiment">Experiment</h2>
<p>設計了 5 種不同大小的架構，彼此差別僅在於 channal
數的不同及層數重覆的不同。其中 ConvNeXt-T ConvNeXt-B 與 Swin-T Swin-B
計算量是對標的。</p>
<p><img src="https://i.imgur.com/9kEf89T.png"
alt="image-20220317020218628" /></p>
<p>ImageNet-1K 分類的 SOTA 表</p>
<p><img src="https://i.imgur.com/ONCJwTA.png"
alt="image-20220317020417267" /></p>
<p>ImageNet-22K 分類的 SOTA 表</p>
<p><img src="https://i.imgur.com/9fXZ457.png"
alt="image-20220317020434857" /></p>
<p>可發現 ConvNeXt Swin
不管在參數使用量及運算量上都差不大多，但是效果就是好了一些些</p>
<h2 id="結論">結論</h2>
<p>可以發現 ResNet 經魔改後竟然能與流行的 Transformer 相提並論了，
可謂捲土重來，也可觀察到 CNN 網路還有優化的可能，會不會其實這還不是 CNN
的完全體呢？</p>
<p>另外雖然 ConvNeXt Swin
不管在參數使用量及運算量上都差不大多，兩方面都算不上少了很多，但是在應用工業部署上，大家對於
CNN 的優化及接收度仍效高，已經是很成熟的技術了，相對於 Transformer
大家還沒有一定的優化部署方案，我想在應用上應該還是 CNN
占了不少優勢在</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://zhuanlan.zhihu.com/p/458016349">ConvNeXt：全面超越Swin
Transformer的CNN (知乎 大推)</a></p>
<p><a
href="https://www.youtube.com/watch?v=idiIllIQOfU&amp;t=1783s&amp;ab_channel=TheAIEpiphany">The
AI Epiphany youtube 解說影片</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>LightGCN pytorch 原始碼筆記</title>
    <url>/2022/04/28/LightGCN-pytorch-%E5%8E%9F%E5%A7%8B%E7%A2%BC%E7%AD%86%E8%A8%98/</url>
    <content><![CDATA[<p>研究所社群媒體探勘的期末作業，分析 PTT 的推薦系統，使用 LightGCN
作為主網路</p>
<p>keywords: LightGCN <span id="more"></span></p>
<h3 id="程式簡介">程式簡介</h3>
<p>使用 MovieLens (small) 資料集。由 9,000 個電影及 600 個使用者建立出
100,000 個評價 (edge)</p>
<h3 id="import-data">import data</h3>
<p>這一步在把所有會用到的套件 import
進來，一共四個部份。雜七雜八套件、sklearn 分資料集、Pytorch、Pytorch
Geometric <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># import required modules</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># import one line split data set</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim, Tensor</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch_sparse <span class="keyword">import</span> SparseTensor, matmul</span><br><span class="line"></span><br><span class="line"><span class="comment"># import Pytorch Geometric</span></span><br><span class="line"><span class="keyword">from</span> torch_geometric.utils <span class="keyword">import</span> structured_negative_sampling</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> download_url, extract_zip</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn.conv.gcn_conv <span class="keyword">import</span> gcn_norm</span><br><span class="line"><span class="keyword">from</span> torch_geometric.nn.conv <span class="keyword">import</span> MessagePassing</span><br><span class="line"><span class="keyword">from</span> torch_geometric.typing <span class="keyword">import</span> Adj</span><br></pre></td></tr></table></figure></p>
<h3 id="資料集前處理">資料集前處理</h3>
<p>一共有四個處理：下載資料、讀取 node 資料、讀取 edge 資料、分
Training、Testing、Validation 資料、轉換為 SparseTensor</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># download the dataset</span></span><br><span class="line">url = <span class="string">&#x27;https://files.grouplens.org/datasets/movielens/ml-latest-small.zip&#x27;</span></span><br><span class="line">extract_zip(download_url(url, <span class="string">&#x27;.&#x27;</span>), <span class="string">&#x27;.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義資料位置</span></span><br><span class="line">movie_path = <span class="string">&#x27;./ml-latest-small/movies.csv&#x27;</span></span><br><span class="line">rating_path = <span class="string">&#x27;./ml-latest-small/ratings.csv&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># load user and movie nodes</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_node_csv</span>(<span class="params">path, index_col</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Loads csv containing node information</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        path (str): path to csv file</span></span><br><span class="line"><span class="string">        index_col (str): column name of index column</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        dict: mapping of csv row to node id</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 把 user item 的 node id 給讀出來</span></span><br><span class="line">    <span class="comment"># (1) 先用 panda 讀取 csv 檔，且把 index_col 設為目標欄位 (指定為 index 的用意)</span></span><br><span class="line">    <span class="comment"># (2) 用 df.index 讀取 index 資料，再用 .unique() 使得 index 唯一 (不重覆)</span></span><br><span class="line">    <span class="comment"># (3) 建立一個 mapping 為 &#123;原 user item id: 從 0 開始的編碼&#125;</span></span><br><span class="line">    df = pd.read_csv(path, index_col=index_col)</span><br><span class="line">    mapping = &#123;index: i <span class="keyword">for</span> i, index <span class="keyword">in</span> <span class="built_in">enumerate</span>(df.index.unique())&#125;</span><br><span class="line">    <span class="keyword">return</span> mapping</span><br><span class="line"></span><br><span class="line"><span class="comment"># index_col 為欄位名稱</span></span><br><span class="line">user_mapping = load_node_csv(rating_path, index_col=<span class="string">&#x27;userId&#x27;</span>)</span><br><span class="line">movie_mapping = load_node_csv(movie_path, index_col=<span class="string">&#x27;movieId&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># load edges between users and movies</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_edge_csv</span>(<span class="params">path, src_index_col, src_mapping, dst_index_col, dst_mapping, link_index_col, rating_threshold=<span class="number">4</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Loads csv containing edges between users and items</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        path (str): path to csv file</span></span><br><span class="line"><span class="string">        src_index_col (str): column name of users</span></span><br><span class="line"><span class="string">        src_mapping (dict): mapping between row number and user id</span></span><br><span class="line"><span class="string">        dst_index_col (str): column name of items</span></span><br><span class="line"><span class="string">        dst_mapping (dict): mapping between row number and item id</span></span><br><span class="line"><span class="string">        link_index_col (str): column name of user item interaction</span></span><br><span class="line"><span class="string">        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        torch.Tensor: 2 by N matrix containing the node ids of N user-item edges</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    df = pd.read_csv(path)</span><br><span class="line">    edge_index = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 把每一個 user item id 做一對一對應</span></span><br><span class="line">    <span class="comment"># 轉換為從 0 開始的編碼，edge 的起點：src、終點：dst</span></span><br><span class="line">    src = [src_mapping[index] <span class="keyword">for</span> index <span class="keyword">in</span> df[src_index_col]]</span><br><span class="line">    dst = [dst_mapping[index] <span class="keyword">for</span> index <span class="keyword">in</span> df[dst_index_col]]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># (1) 利用 rating 的高低作為 edge_attr (0.5 ~ 5)</span></span><br><span class="line">    <span class="comment"># (2) 把 numpy 格式轉為 pytorch tensor</span></span><br><span class="line">    <span class="comment"># (3) 把 list 的維度從 (1, n) 變為 (n, 1) </span></span><br><span class="line">    <span class="comment"># (4) 且轉換為 long 整數型態</span></span><br><span class="line">    <span class="comment"># (4) 如果 rating 超過 4 才會計算</span></span><br><span class="line">    edge_attr = torch.from_numpy(df[link_index_col].values).view(-<span class="number">1</span>, <span class="number">1</span>).to(torch.long) &gt;= rating_threshold</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 合併 src dst 為 COO 格式</span></span><br><span class="line">    edge_index = [[], []]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(edge_attr.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> edge_attr[i]:</span><br><span class="line">            edge_index[<span class="number">0</span>].append(src[i])</span><br><span class="line">            edge_index[<span class="number">1</span>].append(dst[i])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 從 python list 轉為 torch tensor 格式</span></span><br><span class="line">    <span class="keyword">return</span> torch.tensor(edge_index)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">edge_index = load_edge_csv(</span><br><span class="line">    rating_path,</span><br><span class="line">    src_index_col=<span class="string">&#x27;userId&#x27;</span>,</span><br><span class="line">    src_mapping=user_mapping,</span><br><span class="line">    dst_index_col=<span class="string">&#x27;movieId&#x27;</span>,</span><br><span class="line">    dst_mapping=movie_mapping,</span><br><span class="line">    link_index_col=<span class="string">&#x27;rating&#x27;</span>,</span><br><span class="line">    rating_threshold=<span class="number">4</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># split the edges of the graph using a 80/10/10 train/validation/test split</span></span><br><span class="line">num_users, num_movies = <span class="built_in">len</span>(user_mapping), <span class="built_in">len</span>(movie_mapping)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 做一個 edge 的「編碼」表，用來做 Traning Testing Validation 資料集打亂對應的</span></span><br><span class="line">num_interactions = edge_index.shape[<span class="number">1</span>]</span><br><span class="line">all_indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_interactions)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 sklearn 中一個很好用的套件，可以只使用「一行」程式完為分資料集的工作</span></span><br><span class="line"><span class="comment"># https://clay-atlas.com/blog/2019/12/13/machine-learning-scikit-learn-train-test-split-function/</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 隨機取樣 edge 的「id」，再依 80/10/10 的比例劃分</span></span><br><span class="line"><span class="comment"># 先分 80/20，再分 20 中的 50/50</span></span><br><span class="line">train_indices, test_indices = train_test_split(</span><br><span class="line">    all_indices, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br><span class="line">val_indices, test_indices = train_test_split(</span><br><span class="line">    test_indices, test_size=<span class="number">0.5</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用 [:, [id]] 可以保留仍意位置上的 edge，且回傳成一個 list (神奇)</span></span><br><span class="line">train_edge_index = edge_index[:, train_indices]</span><br><span class="line">val_edge_index = edge_index[:, val_indices]</span><br><span class="line">test_edge_index = edge_index[:, test_indices]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># convert edge indices into Sparse Tensors: https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因記憶體效能原因，把 COO 換成稀疏矩陣會好多</span></span><br><span class="line"><span class="comment"># 使用 SparseTensor 這個包把 COO 的 [2, n] ，轉為 [len(u) + len(i), len(u) + len(i)] 的方型矩陣</span></span><br><span class="line">train_sparse_edge_index = SparseTensor(row=train_edge_index[<span class="number">0</span>], col=train_edge_index[<span class="number">1</span>], sparse_sizes=(</span><br><span class="line">    num_users + num_movies, num_users + num_movies))</span><br><span class="line">val_sparse_edge_index = SparseTensor(row=val_edge_index[<span class="number">0</span>], col=val_edge_index[<span class="number">1</span>], sparse_sizes=(</span><br><span class="line">    num_users + num_movies, num_users + num_movies))</span><br><span class="line">test_sparse_edge_index = SparseTensor(row=test_edge_index[<span class="number">0</span>], col=test_edge_index[<span class="number">1</span>], sparse_sizes=(</span><br><span class="line">    num_users + num_movies, num_users + num_movies))</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/7eaavGC.png" alt="Image" />
轉換為最右的表示法</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># function which random samples a mini-batch of positive and negative samples</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 因為推薦系統的任務是：edge prediction</span></span><br><span class="line"><span class="comment"># 所以我們要用 transductive 的方式來切分資料集</span></span><br><span class="line"><span class="comment"># 也就是說 Traning Testing Validation 三個資料集，彼此的邊都不一樣，但是三個加在一起等於原 Graph 的邊</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_mini_batch</span>(<span class="params">batch_size, edge_index</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Randomly samples indices of a minibatch given an adjacency matrix</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        batch_size (int): minibatch size</span></span><br><span class="line"><span class="string">        edge_index (torch.Tensor): 2 by N list of edges</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        tuple: user indices, positive item indices, negative item indices</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 為了使後續計算 BPR loss (有相關的邊分數高、沒相關的邊分數低)</span></span><br><span class="line">    <span class="comment"># 所以我們這邊要先手動生出一些「沒相關」的邊來</span></span><br><span class="line">    <span class="comment"># 使用 Pytorch Geometric 中的 structured_negative_sampling 函式</span></span><br><span class="line">    edges = structured_negative_sampling(edge_index)</span><br><span class="line">    edges = torch.stack(edges, dim=<span class="number">0</span>)</span><br><span class="line">    indices = random.choices(</span><br><span class="line">        [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(edges[<span class="number">0</span>].shape[<span class="number">0</span>])], k=batch_size)</span><br><span class="line">    batch = edges[:, indices]</span><br><span class="line">    user_indices, pos_item_indices, neg_item_indices = batch[<span class="number">0</span>], batch[<span class="number">1</span>], batch[<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> user_indices, pos_item_indices, neg_item_indices</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/CMQIQUJ.png" alt="Image" /></p>
<h3 id="lightgcn-model">LightGCN Model</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># defines LightGCN model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LightGCN</span>(<span class="params">MessagePassing</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;LightGCN Model as proposed in https://arxiv.org/abs/2002.02126</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, num_users, num_items, embedding_dim=<span class="number">64</span>, K=<span class="number">3</span>, add_self_loops=<span class="literal">False</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Initializes LightGCN Model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            num_users (int): Number of users</span></span><br><span class="line"><span class="string">            num_items (int): Number of items</span></span><br><span class="line"><span class="string">            embedding_dim (int, optional): Dimensionality of embeddings. Defaults to 8.</span></span><br><span class="line"><span class="string">            K (int, optional): Number of message passing layers. Defaults to 3.</span></span><br><span class="line"><span class="string">            add_self_loops (bool, optional): Whether to add self loops for message passing. Defaults to False.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_users, self.num_items = num_users, num_items</span><br><span class="line">        self.embedding_dim, self.K = embedding_dim, K</span><br><span class="line">        self.add_self_loops = add_self_loops</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 加入 l = 0，這是整個網路唯一個可學習參數</span></span><br><span class="line">        self.users_emb = nn.Embedding(</span><br><span class="line">            num_embeddings=self.num_users, embedding_dim=self.embedding_dim) <span class="comment"># e_u^0</span></span><br><span class="line">        self.items_emb = nn.Embedding(</span><br><span class="line">            num_embeddings=self.num_items, embedding_dim=self.embedding_dim) <span class="comment"># e_i^0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化 Embedding</span></span><br><span class="line">        nn.init.normal_(self.users_emb.weight, std=<span class="number">0.1</span>)</span><br><span class="line">        nn.init.normal_(self.items_emb.weight, std=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, edge_index: SparseTensor</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Forward propagation of LightGCN Model.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            edge_index (SparseTensor): adjacency matrix</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            tuple (Tensor): e_u_k, e_u_0, e_i_k, e_i_0</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># compute \tilde&#123;A&#125;: symmetrically normalized adjacency matrix</span></span><br><span class="line">        <span class="comment"># 使用 gcn_norm 來簡化 A=DAD 的運算過程</span></span><br><span class="line">        <span class="comment"># 且加入了 self_loops</span></span><br><span class="line">        edge_index_norm = gcn_norm(</span><br><span class="line">            edge_index, add_self_loops=self.add_self_loops)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 這一步是在實作 E_0</span></span><br><span class="line">        emb_0 = torch.cat([self.users_emb.weight, self.items_emb.weight]) <span class="comment"># E^0</span></span><br><span class="line">        embs = [emb_0]</span><br><span class="line">        emb_k = emb_0</span><br><span class="line"></span><br><span class="line">        <span class="comment"># multi-scale diffusion</span></span><br><span class="line">        <span class="comment"># 從中間 node 往外 hop K 個步，並計算 propagation (Wx+b)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.K):</span><br><span class="line">            emb_k = self.propagate(edge_index_norm, x=emb_k)</span><br><span class="line">            embs.append(emb_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把最後不同 K 層的結果，用 mean Aggregation0</span></span><br><span class="line">        embs = torch.stack(embs, dim=<span class="number">1</span>)</span><br><span class="line">        emb_final = torch.mean(embs, dim=<span class="number">1</span>) <span class="comment"># E^K</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把 user item 的特徵向量分開</span></span><br><span class="line">        users_emb_final, items_emb_final = torch.split(</span><br><span class="line">            emb_final, [self.num_users, self.num_items]) <span class="comment"># splits into e_u^K and e_i^K</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># returns e_u^K, e_u^0, e_i^K, e_i^0</span></span><br><span class="line">        <span class="keyword">return</span> users_emb_final, self.users_emb.weight, items_emb_final, self.items_emb.weight</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">message</span>(<span class="params">self, x_j: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="keyword">return</span> x_j</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">message_and_aggregate</span>(<span class="params">self, adj_t: SparseTensor, x: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        <span class="comment"># computes \tilde&#123;A&#125; @ x</span></span><br><span class="line">        <span class="keyword">return</span> matmul(adj_t, x)</span><br><span class="line"></span><br><span class="line">model = LightGCN(num_users, num_movies)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/BiT5LYW.png" alt="Image" /> <img
src="https://i.imgur.com/UGp6lJU.png" alt="Image" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bpr_loss</span>(<span class="params">users_emb_final, users_emb_0, pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0, lambda_val</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        users_emb_final (torch.Tensor): e_u_k</span></span><br><span class="line"><span class="string">        users_emb_0 (torch.Tensor): e_u_0</span></span><br><span class="line"><span class="string">        pos_items_emb_final (torch.Tensor): positive e_i_k</span></span><br><span class="line"><span class="string">        pos_items_emb_0 (torch.Tensor): positive e_i_0</span></span><br><span class="line"><span class="string">        neg_items_emb_final (torch.Tensor): negative e_i_k</span></span><br><span class="line"><span class="string">        neg_items_emb_0 (torch.Tensor): negative e_i_0</span></span><br><span class="line"><span class="string">        lambda_val (float): lambda value for regularization loss term</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        torch.Tensor: scalar bpr loss value</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    reg_loss = lambda_val * (users_emb_0.norm(<span class="number">2</span>).<span class="built_in">pow</span>(<span class="number">2</span>) +</span><br><span class="line">                             pos_items_emb_0.norm(<span class="number">2</span>).<span class="built_in">pow</span>(<span class="number">2</span>) +</span><br><span class="line">                             neg_items_emb_0.norm(<span class="number">2</span>).<span class="built_in">pow</span>(<span class="number">2</span>)) <span class="comment"># L2 loss</span></span><br><span class="line"></span><br><span class="line">    pos_scores = torch.mul(users_emb_final, pos_items_emb_final)</span><br><span class="line">    pos_scores = torch.<span class="built_in">sum</span>(pos_scores, dim=-<span class="number">1</span>) <span class="comment"># predicted scores of positive samples</span></span><br><span class="line">    neg_scores = torch.mul(users_emb_final, neg_items_emb_final)</span><br><span class="line">    neg_scores = torch.<span class="built_in">sum</span>(neg_scores, dim=-<span class="number">1</span>) <span class="comment"># predicted scores of negative samples</span></span><br><span class="line"></span><br><span class="line">    loss = -torch.mean(torch.nn.functional.softplus(pos_scores - neg_scores)) + reg_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p><span class="math display">\[
\begin{equation}
L_{BPR} = -\sum_{u = 1}^M \sum_{i \in N_u} \sum_{j \notin N_u}
\ln{\sigma(\hat{y}_{ui} - \hat{y}_{uj})} + \lambda ||E^{(0)}||^2
\end{equation}
\]</span> ### Reference</p>
<p><a
href="https://clay-atlas.com/blog/2019/12/13/machine-learning-scikit-learn-train-test-split-function/">sklearn
train_test_split 使用方法</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
        <tag>Source Code</tag>
      </tags>
  </entry>
  <entry>
    <title>DINO: Emerging Properties in Self-Supervised Vision Transformers - 把 Vision Transformer 用在自監督學習上</title>
    <url>/2022/02/25/DINO-Emerging-Properties-in-Self-Supervised-Vision-Transformers-%E6%8A%8A-Vision-Transformer-%E7%94%A8%E5%9C%A8%E8%87%AA%E7%9B%A3%E7%9D%A3%E5%AD%B8%E7%BF%92%E4%B8%8A/</url>
    <content><![CDATA[<p>2021 年 4 月，正是 Transformer 熱潮發揚光大的時候，而 Facebook
這時也趁熱出了一篇把 Transformer 應用在自監督式學習上面，並藉著
distillation 的概念，把網路架構稱作 DINO。得益於 Transformer
的強大，基於 ViT based 的架構成功刷到了當前的 SOTA。</p>
<p><a
href="https://arxiv.org/pdf/2104.14294.pdf">https://arxiv.org/pdf/2104.14294.pdf</a></p>
<p>keywords: DINO <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p>DINO 全名為 self-<strong>di</strong>stillation with
<strong>no</strong> labels (嗯…就是這麼硬湊
XD)。翻成中文是：沒有標記的「自知識蒸餾學習」，這篇論文把自監督學習架構看成是一種
student 與 teacher 的 knowledge distillation 方法，所以才會這麼叫它
(就像 SimSiam 中把網路架構看成是一個 Siamese network 一樣)。</p>
<p>DINO 與其它自監督學習的架構相同的是：沒有使用任何負樣本，保留了來自
MoCo 的 momentum</p>
<p>DINO 與其它自監督學習的架構不同的是：它沒有用任何 predictor
(用來預測的 MLP 層)、normalization (online-target 網路結果經過一個 L2
Loss)、contrastive loss (像是 infoNCE)，把 Loss function 改為 cross
entropy，除之還加入了新的 centering 與 sharpening 架構來避免
collapse</p>
<p>本篇論文發現如果把 Transformer 應用在 DINO
上，最後的輸出特徵空間有著非常強的「邊界」資訊，相較於傳統卷積網路的效果好上非常多，對於應用在分割任務上有很大的前途。</p>
<h2 id="網路架構">網路架構</h2>
<p>在詳細介紹前，特別注意這篇文同樣是一篇沒有負樣本訓練的網路架構。以下是網路架構圖：</p>
<p><img src="https://i.imgur.com/C58wnFx.png"
alt="image-20220228005007483" /></p>
<p>流程為：</p>
<ul>
<li>輸入影像 x ，會做兩個不同的資料擴增 (使用從 SwAV 來的 multi-crop
stategy，後面會細說)</li>
<li>分別輸入到 student <span class="math inline">\(g_{\theta_s}\)</span>
與 teacher <span class="math inline">\(g_{\theta_t}\)</span>
網路中，兩者可為卷積層或是 Transformer 層</li>
<li>student 經特徵提取後，經一個 sharpening (銳利化) 的 softmax 得到結果
p1</li>
<li>teacher 經特徵提取後，分別經 centering (中心化) 以及 sharpening
(銳利化) 的 softmax 得到結果 p2</li>
<li>p1 與 p2 做 cross entropy 得到網路 loss</li>
<li>loss 只會在 student 網路中做 backpropagation，teacher 則有個 sg
(stop-gradient) 則不會做 backpropagation</li>
<li>student 會經由一個 EMA (exponential moving average)，其實就跟
momentum 的概念一模一樣，一點一點的慢慢更新 teacher 網路</li>
</ul>
<p>網路中所有輸出前都會經過 softmax，而公式如下，特別的是用到了 <span
class="math inline">\(\tau\)</span> temperature
這個參數來控制銳利化的大小 <span class="math display">\[
P_s(x)^{(i)}=\frac{\exp(g_{\theta_s}(x)^{(i)}/\tau_s)}{\sum^K_{k=1}\exp(g_{\theta_s}(x)^{(k)}/\tau_s)}
\]</span> 而最後的 Loss 要表示的是 teacher 與 student
兩網路學出來的特徵表示在空間中的距離<strong>越近越好</strong>，且使用的是二元
cross entropy loss <span class="math display">\[
\begin{gather}
\min_{\theta_s}H(P_t(x), P_s(x))\\
H(a,b)=-a\log b
\end{gather}
\]</span> 本篇論文有給 pesudo code，一目了然 XD</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># gs, gt: student 和 teacher 網路</span></span><br><span class="line"><span class="comment"># C: centering 中心化</span></span><br><span class="line"><span class="comment"># tps, tpt: student 和 teacher 的溫度參數</span></span><br><span class="line"><span class="comment"># l, m: centering 中心化的比率、momentum 的比率</span></span><br><span class="line">gt.params = gs.params</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> loader: <span class="comment"># 一次讀取一 minibatch 影像</span></span><br><span class="line">    x1, x2 = augment(x), augment(x) <span class="comment"># 把影像擴增成兩個不同的 view</span></span><br><span class="line">    s1, s2 = gs(x1), gs(x2) <span class="comment"># student 的輸出</span></span><br><span class="line">    t1, t2 = gt(x1), gt(x2) <span class="comment"># teacher 的輸出</span></span><br><span class="line">    loss = H(t1, s2)/<span class="number">2</span> + H(t2, s1)/<span class="number">2</span>  <span class="comment"># 理論上 t1 s2、t2 s1 越近越好</span></span><br><span class="line">    loss.backward() <span class="comment"># back-propagate</span></span><br><span class="line">    <span class="comment"># student, teacher and center updates</span></span><br><span class="line">    update(gs) <span class="comment"># SGD</span></span><br><span class="line">    gt.params = l*gt.params + (<span class="number">1</span>-l)*gs.params</span><br><span class="line">    C = m*C + (<span class="number">1</span>-m)*cat([t1, t2]).mean(dim=<span class="number">0</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">H</span>(<span class="params">t, s</span>):</span></span><br><span class="line">    t = t.detach() <span class="comment"># stop gradient</span></span><br><span class="line">    s = softmax(s / tps, dim=<span class="number">1</span>)</span><br><span class="line">    t = softmax((t - C) / tpt, dim=<span class="number">1</span>) <span class="comment"># centering 中心化 + sharpening (softmax) 銳利化</span></span><br><span class="line">    <span class="keyword">return</span> - (t * log(s)).<span class="built_in">sum</span>(dim=<span class="number">1</span>).mean()</span><br></pre></td></tr></table></figure>
<h3
id="自監督學習與知識蒸餾-knowledge-distillation">自監督學習與知識蒸餾
(Knowledge Distillation)</h3>
<p>為何稱知識蒸餾 (Knowledge Distillation)？與 SimSiam
最大不同的點是，SimSiam
是兩個「相同」的網路，所以那篇作者才會歸納為一種「孿生網路 Siamese
network」。</p>
<p>而 DINO 是兩個「不同」的網路，加上中間還有一條 EMA
參數傳導鍊，但其實就是 MoCo 中的 momentum，公式如下也一樣就是了 <span
class="math display">\[
\theta_t \leftarrow\lambda\theta_t+(1-\lambda)\theta_s
\]</span> 所以本篇作者認為是一個 teacher 教導 student
的知識蒸餾網路。(只是不知為何本篇是 student 教 teacher 就是了…)</p>
<h3 id="multi-crop-strategy">multi-crop strategy</h3>
<p>接下來說說論文中提到的 multi-crop strategy，簡單來說就是 crop
的定義升級版。假設影像大小為 224x224，則定義：</p>
<ul>
<li>當 crop 的長寬<strong>大</strong>於影像大小的 50%，稱為 Global
view</li>
<li>當 crop 的長寬<strong>小</strong>於影像大小的 50%，稱為 Local
view</li>
</ul>
<p><img src="https://i.imgur.com/DqvnJ5s.png"
alt="image-20220228023326112" /></p>
<p>作者把 student 放 Global view + Local view 的擴增，而 teacher 只放
Local view 的擴增，作者認為可以達到 local-to-global 的效果。也就是
teacher 學習到的內容遠比 student 來得複雜，或說 teacher 學習的只是
student 的一個子集合而已，藉由知識蒸餾的觀點來解釋：student
會學習複雜的參數，並把整理後的結果放到較簡單的 teacher
中做整理與歸納。等等…這個觀念是不是相反了阿… (正常來說不是要 teacher 教
student 嗎？怎麼反過來了呢？我也不知道反正論文中是這麼起名的就是了…)</p>
<h3 id="centering-中心化與-sharpening-銳利化">centering 中心化與
sharpening 銳利化</h3>
<p>前面也有提到這篇論文沒有使用到負樣本訓練，那要怎麼避免 collapse
的發生呢？本篇作者提出 centering 與 sharpening 的概念。</p>
<p>centering 中心化的目標是：避免特徵維度由單一維獨大控制。做法為在
teacher 的特徵提取層 <span class="math inline">\(g_t(x)\)</span>
後加上一個 bias <span class="math inline">\(c\)</span> <span
class="math display">\[
g_t(x)\leftarrow g_t(x)+c
\]</span> 而這個 <span class="math inline">\(c\)</span> 的更新與 EMA
(momentum) 類似，由一個參數 <span class="math inline">\(m\)</span>
來控制，大部份為上一刻算出來的 <span
class="math inline">\(c\)</span>，小部份為 下一個 Batch 內的結果 <span
class="math display">\[
c\leftarrow mc+(1-m)\frac{1}{B}\sum^{B}_{i=1}g_{\theta_t}(x_i)
\]</span> sharpening
銳利化的目標是：加強相近的特徵，減弱較遠的特徵，簡單說就是 softmax
在做的事</p>
<p><img src="https://i.imgur.com/TaACCBY.png"
alt="image-20220228025442182" /></p>
<p>作者經實驗發現加入這個兩東西可以一定的避免 collapse 的發生，且發現
centering 容易 collapse 而 sharpengin 則相反，兩者正好互相抵消</p>
<h3 id="卷積與-transformer">卷積與 Transformer</h3>
<p>DINO 的 backbone 是可以替換的，作者發現使用 Transformer
的效果非常好，對於找出物體的邊界有著顯著的效果。特別提一下，原卷積 based
的網路 MLP 層中有 BN 層，而改為 Transformer 因架構關系不能用
BN，所以作者特別提了一下 Transformer 版本的架構是
<strong>BN-free</strong> 架構。</p>
<h2 id="實驗">實驗</h2>
<h3 id="sota-表">SOTA 表</h3>
<p>使用 ViT 作為 backbone 的效果明顯於使用 ResNet-50 的效果</p>
<p><img src="https://i.imgur.com/5i9dz5c.png"
alt="image-20220228025928799" /></p>
<h3 id="應用在分割上">應用在分割上</h3>
<p>作者發現相較於監督式學習，自監督學習更能找到目標<strong>真正想要關注的位置</strong>，更集中更接近人類對於物體的定義</p>
<p><img src="https://i.imgur.com/HBJFLEJ.png"
alt="image-20220228030120395" /></p>
<p><img src="https://i.imgur.com/Epprwlk.png"
alt="image-20220228030200153" /></p>
<h3 id="一些架構的-ablation-實驗">一些架構的 ablation 實驗</h3>
<p>發現 momentum 的重要性，以及經實驗發現 DINO 架構下 CE 比 MSE 好</p>
<p><img src="https://i.imgur.com/jmDdBXU.png"
alt="image-20220228030504626" /></p>
<h3 id="centering-與-sharpening-實驗">centering 與 sharpening 實驗</h3>
<p>作者發現有兩種 collapse 的發生，一是網路不管 input
只往一大參數做為輸出，而 centering 就是避免這個情況發生的解法，但是
centering 同時也會讓特徵向量過於平均，明顯的特徵被平滑化了，而
sharpening 就是在避免這個情況的解法。兩者互補，缺一不可</p>
<p><img src="https://i.imgur.com/OG431IX.png"
alt="image-20220228030700498" /></p>
<h2 id="結論">結論</h2>
<p>DINO 是一篇把 Transformer
應用到自監督學習的論文，並且也不是單單搬過來而已，同時也修改了一些地方，像是加入了
centering 與 sharpening 來避免 collapse。</p>
<p>雖然說實驗結果的資料告訴我們：自監督的強項是找到「符合人類認為的物體邊界」，但這也同時告訴我們：<strong>選擇資料的重要性</strong>，網路會慢慢的頃向我們所認為的資料收斂。如果資料今天不是想
ImageNet
分佈的那麼集中呢？它會關注到哪些部份呢？我想這也是一個有趣的議題來討論</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://www.youtube.com/watch?v=h3ij3F3cPIk&amp;t=1010s&amp;ab_channel=YannicKilcher">(Youtube)
Yannic Kilcher 大神講解影片</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
        <tag>Contrastive Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Contrastive Learning 對比學習: BYOL 與 SimSiam</title>
    <url>/2022/02/22/Contrastive-Learning-%E5%B0%8D%E6%AF%94%E5%AD%B8%E7%BF%92-BYOL-%E8%88%87-SimSiam/</url>
    <content><![CDATA[<p>本篇接續上篇文章，依照時間順序介紹有關對比學習的論文：BYOL -&gt;
SimSiam</p>
<p>keywords: BYOL、SimSiam <span id="more"></span></p>
<p>前篇文章介紹了 MoCo、SimCLR
兩篇優秀的自監督式學習，它們有個共通點：<strong>都在負樣本的尋找上動手腳</strong>。MoCo
用一個 Queue 來儲存之前的負樣本、SimCLR 直接把 Batch 設超大來解決</p>
<h2 id="我們一定要負樣本嗎">我們一定要負樣本嗎？</h2>
<p>在回答這個問題前，先來了解為什麼 MoCo SimCLR 視負樣本為重。以 SimCLR
的想法為例：一張圖片做兩種不同的資料擴增，經過網路找出特徵後，在最後結果的向量空間內，兩向量距離應該會非常接近。但如果我們只使用正樣本來這樣訓練的話，那網路是不是每次只要輸出<strong>一個等於自己的常數</strong>就會永遠得到最大的相似度？，大家稱這種現象叫
collapsing output。</p>
<p><img src="https://i.imgur.com/fQ3PXrc.png"
alt="image-20220224165354961" /></p>
<p>其中一種解決 collapsing output
的方法就是引入負樣本，使得樣本間存在一定的負雜度，不會讓網路往奇怪的地方收斂</p>
<p>但 MoCo、SimCLR
也同時證明了，不管使用哪種增加負樣本的方法，訓練起來都非常的麻煩，或是對硬體要求非常高。而後來的
BYOL、SimSiam
就在這個出發點上以提出一個「簡單、直覺」的自監督學習方法，來嘗試去掉負樣本。</p>
<h2 id="byol">BYOL</h2>
<p><a href="https://arxiv.org/pdf/2006.07733.pdf">Bootstrap Your Own
Latent A New Approach to Self-Supervised Learning</a></p>
<p>以下是 BYOL 的架構圖，網路的流程為：</p>
<p>一張影像 x 經過兩個不同的資料擴增得到 <span
class="math inline">\(t\)</span> <span
class="math inline">\(t&#39;\)</span> ，其中上面的分支稱為
<strong>online</strong>，下面的分支稱為 <strong>target</strong>。online
會依序經過三次線性轉換 (view -&gt; representation -&gt; projection -&gt;
prediction)，而 target 只會做兩次線性轉換 (view -&gt; representation
-&gt; projection)。最後 online 的 preduction <span
class="math inline">\(q_\theta(z_\theta)\)</span> 與 target 的
projection <span class="math inline">\(sg(z_\xi&#39;)\)</span>
會做相似度的 loss。</p>
<p><img src="https://i.imgur.com/ep8f3g8.png"
alt="image-20220225143219347" /></p>
<p>而相似度的公式的流程：把最後兩個結果 <span
class="math inline">\(q_\theta(z_\theta)\)</span> <span
class="math inline">\(sg(z_\xi&#39;)\)</span> 做 L2 Loss <span
class="math display">\[
\mathcal{L}_{\theta,\xi}\triangleq\,\mid\mid\bar{q_\theta}(z_\theta)-\bar{z&#39;_\xi}\mid\mid^2_2\quad=2-2*\frac{\langle
q_\theta(z_\theta),z&#39;_\xi\rangle}{\mid\mid
q_\theta(z_\theta)\mid\mid_2\cdot\mid\mid z&#39;_\xi\mid\mid_2}
\]</span> 計算完 Loss 後 online 會照 Loss 做 Backpropagation，而 target
則是透過 momentum 來更新。(上圖中的 sg 代表為 stop-gradient 的意思)
<span class="math display">\[
\begin{gather}
\theta \leftarrow \mathrm{optimizer}(\theta,
\triangledown\theta\mathcal{L}^{BOYL}_{\theta, \xi}, \eta)\\
\xi \leftarrow\tau\xi+(1-\tau)\theta
\end{gather}
\]</span> 整體網路架構與 MoCo 不同的點在於去掉了 memory bank
的設計，整個網路只會使用正樣本來訓練。而與 SimCLR
最大的不同在加上了一個新 prediction
層，換句話說又多加了一層線性轉換層</p>
<p>以上就是 BYOL
整體架構，可以看到網路只使用正樣本來訓練，但是網路並沒有提出任何顯著的方法來避免
collapsing output 的發生，而且我們從 Loss function
就可以發現，當存在一個特殊解：online 與 target 皆輸出一恆定常數時，Loss
為零。可以說在 Loss function 中可以發現 collapsing output
的存在，但是這篇論文是解釋是說，經實驗證明，加入 prediction
層可以把發生的機率降到最低，從而使網路穩定。</p>
<h2 id="simsiam">SimSiam</h2>
<p><a href="https://arxiv.org/pdf/2011.10566v1.pdf">Exploring Simple
Siamese Representation Learning</a></p>
<p>SimSiam 為 Simple Siamese 的縮寫，先開始介紹什麼是 Siamese
網路。Siamese
的原意是孿生的意思，應用在神經網路的的意思為：<strong>有兩個網路，它們有各自的輸入，但是擁有相同的參數權重</strong>。</p>
<p><img src="https://i.imgur.com/WYaWLL9.png"
alt="image-20220225160653482" /></p>
<p>而這一篇 SimSiam 論文中，作者把自監督學習的這種架構看作 Siamese
網路，把輸入圖片做兩種不同的擴增後放進「參數共享」的網路中，最後再比較兩網路輸出的相似度。網路架構如下圖：</p>
<p><img src="https://i.imgur.com/q31nKpw.png"
alt="image-20220225155643647" /></p>
<p>等等…是不是有一個地方怪怪的…，「參數共享」的網路？不就是同一個網路嗎？沒錯在原論文中作者說
<code>In a nutshell, our method can be thought of as "BYOL without the momentum encoder"</code>
也就是在說： SimSiam 與 BYOL 的最大差別在有沒有做 momentum 更新。</p>
<p>作者提出的 SimSiam
主要的核心概念是：提出一個超極直白的自監督式學習的架構，沒有負樣本、沒有超大
Batch Size、沒有 momentum。除了效果很簡單外，也很神奇的避免了 collapsing
output 的發生。</p>
<p>作者經實驗發現 BYOL 的三項改進 momentum encoder、predictor 和 stop
gradient 中，真正能避免 collapsing output 發生的是 stop gradient</p>
<p>但是在論文原文 4.7 Summary
章節中作者自己也提到了：<code>but we have seen no evidence that they are related to collapse prevention</code>
。簡單來說現在大家還不知道為什麼 prediction 層效果這麼好、為什麼 stop
gradient 可以避免 collapsing output</p>
<h2 id="結論">結論</h2>
<p>BYOL 以及 SimSiam
都是在把自監督式學習往更簡單更直覺的方向前進，去除掉了之前論文較複雜的部份，只是目前還沒有人搞懂為什麼這種架構效果這麼的好…</p>
<p>我自己也認為是如此，論文中大部份都是先有實驗結果才有理論證明，就…看起來不是很能說服人呢…希望後續有更多論文可以提出新架構來解釋這一切</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://blog.csdn.net/dhaiuda/article/details/117897881">BYOL
csdn</a></p>
<p><a href="https://iter01.com/581069.html">孿生網路</a></p>
<p><a
href="https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/">MoCo
SimCLR BYOL 大整理 (英文)</a></p>
<p><a href="https://www.gushiciku.cn/pl/gLs8/zh-tw">MoCo SimCLR BYOL
SimSiam 大整理 (極市平台)</a></p>
<p><a href="https://www.gushiciku.cn/pl/gLs8/zh-tw">MoCo SimCLR BYOL
SimSiam 大整理 (軟體之心)</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Contrastive Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>MAE: Masked Autoencoders Are Scalable Vision Learners - 模仿 BERT 且更簡單的自監督學式</title>
    <url>/2022/05/08/MAE-Masked-Autoencoders-Are-Scalable-Vision-Learners-%E6%A8%A1%E4%BB%BF-BERT-%E4%B8%94%E6%9B%B4%E7%B0%A1%E5%96%AE%E7%9A%84%E8%87%AA%E7%9B%A3%E7%9D%A3%E5%AD%B8%E5%BC%8F/</url>
    <content><![CDATA[<p>繼上一篇 BEiT 後，2021 11月 FAIR 也提出了一個基於 BERT
改造且應用在電腦視覺上的自監督式學習，其最核心的想法，就是建構出一個更「直覺」「簡單」的模型。模型取名叫做
masked autoencoders (MAE)，相較於上一篇 BEiT
效果上差不多，但是整體的訓練流程卻相對簡單許多。</p>
<p><a
href="https://arxiv.org/pdf/2111.06377.pdf">https://arxiv.org/pdf/2111.06377.pdf</a></p>
<p>keywords: Self-supervised Learning、BERT、MAE <span id="more"></span></p>
<h2 id="abstract">Abstract</h2>
<p>作者提出一個 scalable(可自由放大縮小) 的網路架構
MAE，想法為：<strong>利用 mask 遮罩隨機的把 patch
給遮掉，經過網路後再重建回原影像</strong>。其核心的實做辨法有二：</p>
<ol type="1">
<li>提出一個「不對稱」的 encoder-decoder 架構</li>
<li>被 mask 遮蓋掉的 patch 高達 75%</li>
</ol>
<p>以上兩個做法不僅在「效率 efficient」上減少相當多，同時在「效能
accuracy」上也是增加的。作者證明使用了最陽春的 ViT-H 做為
backbone，且只在 ImageNet-1K 上訓練，就可以得到 (87.8% accuracy)
的效果。說明 MAE 提取特徵之厲害的地方。</p>
<p><img src="https://i.imgur.com/plhkvt8.png" alt="Image" /></p>
<h2 id="introduction">Introduction</h2>
<p>作者說：BERT (如果不清楚可以看我上一篇文章的介紹)
的觀念很直覺，藉由移除掉訓練資料中的一部份再把它預測回來來訓練網路，這種方法因為「移除」資料的原因，因此它的<strong>訓練資料集</strong>及<strong>模型參數量</strong>也是異常的大。不過以上兩個缺點卻完全沒有影響到
BERT 成功的亮光，BERT 在 NLP
界大放異彩，對後續自監督式學習起到了重要作用。</p>
<p>也因此在 CV 界的大家同時開始在想：要是…我們把 BERT
移到影像上呢？要是…我們今天 mask 蓋住的不是字詞而是一個 patch
呢？因此這篇論文就是在做這件事情：把 BERT 應用在影像上面</p>
<p>而作者開始研究的第一步不是直接想一個網路出來，而是先問自己：如果我們要設
mask
蓋住東西的話，蓋住一個字詞跟蓋住一個圖片的差別倒底在哪裡呢？<code>what makes masked autoencoding different between vision and language?</code>，作者提出了以下三點回覆：</p>
<ol type="1">
<li>直接把影像放在 BERT
上的第一個困難就是：資料維度的不同，一個是二維影像、一個是一維序列，而且
BERT 中還有 positional enbedding 這些 CV
中都沒有的特色，是要怎麼融合在一起呢？多虧了 ViT
論文的提出，我們已經知道直接把影像丟到 Transformer
中訓練不僅是一個可行的方法，同時效果可望還能突破傳統 CNN
架構，所以這已經不是一個困難的點了</li>
<li>資料複雜度非常的不同。對於一個句字來說，裡面包含了非常非常多的資訊：文法、字詞、上下文關系，如果把其中一個字詞挖掉可能會影響到整句話的意思，對於人類來說因為有著很多的「先備知識」所以可能會覺得很簡單，但對機器來說並非如此；那如果是一張影像呢？因為影像有
heavy spatial redundancy 的特色，多一少一個 pixel 對影像的影響不大 (看看
stride pooling 的影響，其實很小)，所以如果跟原本 BERT 一樣只挖 15%
是不夠的，網路會因為訓練難度不夠而效果不好。因此作者提出挖掉非常高
<strong>75%</strong> 的 patch 來解決這項問題</li>
<li>最後是 Decoder 的複雜度。在原本 BERT 中最後 mask
的部份會做分類任務，因為「詞」這個本身已經有很多函意在裡面了，所以只使用了簡簡單單的一層全連接層就搞定了；在影像上為了要
by pixel 的重建回影像，在以往分割的經驗中，我們會需要多層的卷積及
upsampling 才能提取其中的特徵。所以作者有別於 BERT
的一層全連接，設計了相對複雜的 Decoder</li>
</ol>
<p>綜合以上三點作者提出的 MAE 有著以下兩個特色：利用高達 75% 的 mask
來訓練網路、以及「不對稱」的 Encoder-Decoder 架構</p>
<p>MAE 蓋掉 75% 的 patch
重建回原影像的結果，發現網路對圖片的理解非常可怕，蓋掉一大堆還大概知道原圖長什麼樣…</p>
<p><img src="https://i.imgur.com/A0wboRG.png"
alt="image-20220509110908182" /></p>
<p>什麼叫做「不對稱」呢？在 MAE 中，Encoder 是「短而厚」，輸入的 patch
不長 (75% 被蓋住了)，但是網路較深；而 Decoder 是「長而薄」輸入全部的
patch
但是網路較淺，稍後網路架構會有更深入的說明。但可知道的是作者藉著這種操作大量減少了運量，作者在論文中稱：與正常的
Encoder-Decoder 相比減少了近 3
倍的運算量，且可以把省下來的運算量拿去利用給 Encoder
的編碼，更加強網路的效果。</p>
<p>作者利用 ViT-L/16，ViT-H/16 兩個模型，僅在 ImageNet-1K
上面做預訓練，最後再 fine-fune 就可得到 87.8%
的正確率。在其它模型下要取得這種正確率，網路的參數量可要非常大才行。(ViT
可用了 JFT-300M 才有這個效果)</p>
<h2 id="related-work">Related Work</h2>
<p>其中在 Autoencoder 的地方作者做了一個有趣的比較，作者說：MAE
也算是一種 AutoEncoder，有著三大要素：Encoder、Decoder、以及中間的
latent space。MAE 的 mask 作法尤其更像 2008 年的 <a
href="https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf">Denoising
AutoEncoder</a>，同樣都是在圖片上加入雜訊，同樣都有 Encoder-Decoder
架構，作者認為 MAE 「架構」算是某種程度上是 Denoising Autoencoder
的特例，但整體訓練「思維」及訓練「方法」有著相當大的不同</p>
<p>另外還提到了 Self-supervised
Learning。作者說最近自監督式學習在電腦視覺上越來越流行，目前有兩種支派、一是
Contrastive Learning 代表做有 MoCo、SimCLR 等等，而 MAE 與 BEiT
是屬於修改 BERT 類，作者認為最大的不同在於資料預處理上，Contrastive
Learning 非常依賴資料擴增。</p>
<h2 id="approach">Approach</h2>
<p>網路流程如同 Autoencoder 一樣，會經過 Encoder -&gt; latent space
-&gt; Decoder，MAE 特別的地方是：一、進 Encoder 前會用 mask
隨機的把影像上一部分 patch 給遮掉；二、也因為被遮掉後輸入影像 patch
數量改變，所以是一個「非對稱」的 Autoencoder</p>
<h3 id="masking">Masking</h3>
<p>以下架構與 ViT 一樣，首先輸入影像做 Patch embedding 分成許多 16x16 的
patches，接著用一個隨機分佈亂數，隨機的取其中 25% 的
patches，也可反過來理解為把 75% 的 patch
<strong>移除</strong>掉。等等<strong>移除</strong>…那 mask 呢？在 BERT
中 mask 會是一個可學習的變量乀，在 MAE 中又代表什麼呢？其實 MAE
中的做法與 BERT 不一樣，<strong>MAE 不會把帶有 mask 的 patch 放進
transformer 中訓練</strong>，也可換句話說 MAE 中並沒有 mask
這個概念更像是一個被移除 patch 的標籤而已，在移除掉「大量」的 patch
下，進 transformer 訓練的向量數少非常多。而下面 Encoder 會講更清楚</p>
<p><img src="https://i.imgur.com/bnfzznr.png"
alt="image-20220509141211862" /></p>
<h3 id="mae-encoder">MAE Encoder</h3>
<p>Encoder 的部份也與 ViT 一模一樣，是原汁原味的 transformer
架構。上面有提到<strong>只有不帶 mask 的 patch 才會進
Encoder</strong>，且在「大量 (75%)」的 patch
移除下，計算量也更是直接少了 75%</p>
<p>那為什麼要移除掉這麼多 patch 呢？理由是影像資訊非常的 redundancy
「冗」XD，加一少一個 pixel 基本對整體理解並未差太多，如果我們今天照著
BERT 一樣只移除掉 15%
的輸入，那網路可能強度不強學不太到什麼有用的資訊，甚至網路可能只是在學內差而已
(單純用內差也可以解決這個問題)，所以才會提出移掉 75% 的想法。那為什麼是
75%
呢，後面作者有做詳細的實驗，不過我們可以先來看上面的圖，可以發現當移除達
95% 時，好像資料少太多了，重建的圖開始與輸入相差甚遠，而 75%
正剛剛好，不多也不少，重建的影像品質也是裡面最好的一組</p>
<p>再來因為在輸入序列長度上變「短」了，多出來的計算量正好可以補在「厚度」的地方，我們可以把使用較大的
Transformer 架構 (ViT-L, ViT-H) 來訓練，但參數量不會上升太多，因此說 MAE
的 Encoder 「短而厚」。這個特性後續對於網路的 scaling up
放大實現起來非常容易</p>
<h3 id="mae-decoder">MAE Decoder</h3>
<p>Decoder 就回歸正常操作了，輸入是「全部」的 patch (mask + 進 Encoder
的部份)，網路也是做 transformer 運算。MAE Decoder 的 mask
同樣是一個可學習向量，透過在 Decoder 與其它 patch
計算相關性，最後得出一個特徵向量表示</p>
<p>在 Decoder 的地方也對每個 mask 加上 position embedding
不然重建網路時會不知道彼此的絕對關系</p>
<p>如同 BERT 一樣，MAE 的 Decoder 只在訓練 (pre-train) 時存在，在測試
(fine-tune) 的時候只會使用到 Encoder 的特徵層而已，所以 Decoder
的層數就可以不用設計像 Encoder 那麼深，論文中提到計算量大約是 Encoder 的
&lt;10%。因此說 MAE 的 Decoder「長而淺」</p>
<h3 id="reconstruction-target">Reconstruction target</h3>
<p>MAE 重建影像的評估是建立在：每個有 mask 的 patch 與原影像之間 pixel
級別上的關系。Decoder 最後輸出的特徵向量，會經過一個全連接層維度轉換成
256 = 16x16，再把這 256 維透過位置訊息重建回 16x16 的影像，這個 16x16
的向量也不再做什麼分類任務了，它最後直接就是表示成一張影像。最後與原圖
pixel 做 mean squared error (MSE) 得到這個 patch loss，加總所有 masked
patches 得到網路整體的 loss</p>
<h2 id="experiments">Experiments</h2>
<p>作者全部實驗都是 fine-tune 過後的 (先用一大堆無標記
per-train，再用少量有標記 fine-tune)
作為實驗依據，又分別有兩種做法，分別是 end-to-end fine-tuning (全部
encoder 參數都可以修改) 以及 linear probing (固定前 N - 1
層參數，只修改最後第 N 層的參數)，代號分別是 ft、lin。理所當然 fine-tune
因為動到的參數多計算量大所以效果一定比 linear probing 好</p>
<h3 id="imagenet-橫向比較">ImageNet 橫向比較</h3>
<p>同樣都是使用 ViT-L、ImageNet-1K，左邊是 ViT 原始效果，中間是作者在原
ViT 超參數中加了一些 regularization 規則項，右邊是 MAE</p>
<p><img src="https://i.imgur.com/OkYUMZF.png"
alt="image-20220509152108923" /></p>
<p>發現：一、ViT-L 經調教過後還是可以有比較好的表現的，二、僅管如此 MAE
效果還是比較好。且在 fine-tune 上所需要的計算成本非常小 (50 epochs vs
200 epochs)</p>
<h3 id="masking-ratio-比例">Masking ratio 比例</h3>
<p>作者同時比較了 ft 與 lin 在不同 masking ratio
下的表現，發現不管是哪一個 fine-tune
做法，橫軸比例縱軸正確率的圖表下都成一個倒 V
型，太多太少比較效果都不好，中間值落到 75% 時效果最好</p>
<p><img src="https://i.imgur.com/Nu566Mv.png"
alt="image-20220509152557788" /></p>
<h3 id="decoder-的一些設計實驗">Decoder 的一些設計實驗</h3>
<p>因為 Encoder 直接抄 ViT 所以沒什麼好說的 XD，以下簡單看一下 Decoder
的 ablation 實驗</p>
<p><img src="https://i.imgur.com/llbiPES.png"
alt="image-20220509153109437" /></p>
<p>圖 a、發現 Decoder 深度不用深，就有不錯的效果了 (在 ft
更明顯、用一層也行)。</p>
<p>圖 b、Decoder 在特徵維上的大小實驗，發現不用維度也不用大，比起
encoder 的 1024 小了不少。</p>
<p>圖 c、encoder 要不要放入 mask。發現：一、放了效果不好 (84.9 -&gt;
84.2, 73.5 -&gt; 59.6)，二、運算量還多了 3.3 倍。那…幹麻放它進去 XD</p>
<p>圖 d、重建的依據。pixel 代表 by pixel 的 MSE (一個一個算)，發現在做
loss 前做一個 patch 內的 normalization 會使效果更穩定 (合理)。同時與
BEiT 使用的 dVAE
做比較，發現效果其實差不多，但是在觀念和算法複雜度上差很多，那既然如此為什麼不選用簡單直覺的做法呢？</p>
<p>圖 e、MAE
對資料擴增的敏感度。當然有做效果一定會比較好，但是提升不多，可理解為 MAE
對資料擴增不敏感 (我覺得作者刻意提這個是為了與 Contrastive Learning
比較)</p>
<p>圖 f、挖 mask 的方法。隨機挖效果最好，一塊一起挖會不平均
(沒辨法保證重要特徵集中在邊上)，固定挖法
(太簡單了，網路跑去偷吃步去了，不知道學到了啥)</p>
<p><img src="https://i.imgur.com/TDLLFxo.png"
alt="image-20220509154309795" /></p>
<h3 id="training-schedule">Training schedule</h3>
<p>發現 MAE 的方法不容易使網路 overfitting，epoch 都已經調到 1600
了，測試效果還在上升 (當然前提是你的 $$ 足夠你這樣做 XD)</p>
<p><img src="https://i.imgur.com/vqmDapZ.png"
alt="image-20220509154618345" /></p>
<h3 id="sota-表">SOTA 表</h3>
<p>分類：</p>
<p>結論：與 BEiT 差不多，但架構簡單很多。與 Contrastive Learning
還有得比，事後才知道誰是大贏家</p>
<p><img src="https://i.imgur.com/vURKilX.png"
alt="image-20220509154835970" /></p>
<p>偵測：</p>
<p><img src="https://i.imgur.com/fj9yAtb.png"
alt="image-20220509155018121" /></p>
<p>分割：</p>
<p>結論同偵測，在更複雜的影像任務上 MAE
開始展示了它的強悍，提升許多百分點</p>
<p><img src="https://i.imgur.com/4oi5W7y.png"
alt="image-20220509155029106" /></p>
<h2 id="結論">結論</h2>
<p>MAE 與 BEiT
相比：效果差不多，但架構更簡單，更直覺。好像大家都比較喜歡直覺簡單的網路架構呢哈哈。MAE
同樣是學 BERT 來實作，又把 CV
往自監督學習推了一步，這篇感覺可以殿定很好的基礎
(愷明大神的加持？！)，希望後續有更多類似論文的提出 (會不會下一個換 GPT
XD)</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://www.youtube.com/watch?v=mYlX2dpdHHM&amp;ab_channel=MuLi">MAE
论文逐段精读【论文精读】(中文超極詳讀，還帶了很多寫論文的私貨，大推
XD)</a></p>
<p><a
href="https://www.youtube.com/watch?v=Dp6iICL2dVI&amp;ab_channel=AICoffeeBreakwithLetitia">(AI
Coffee Break) Masked Autoencoders Are Scalable Vision Learners - Paper
explained and animated!</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
        <tag>Self-supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>BERT 速讀 - Pre-training of Deep Bidirectional Transformers for Language Understanding - 為自監督式學習打下標竿</title>
    <url>/2022/05/09/BERT-%E9%80%9F%E8%AE%80-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding-%E7%82%BA%E8%87%AA%E7%9B%A3%E7%9D%A3%E5%BC%8F%E5%AD%B8%E7%BF%92%E6%89%93%E4%B8%8B%E6%A8%99%E7%AB%BF/</url>
    <content><![CDATA[<p>最近 CV 流行自監式學習，目前一共分成兩個支派，一是之前介紹的
Contrastive Learning，一是以 BERT
為首魔改的系列，接下來幾個文章會來談談最近兩篇比較熱門的「類
BERT」論文</p>
<p>既然這麼說了，當然就要先從什麼是 BERT 開始說起啦！(怎麼很像以前看
Transformer 前介紹 Self-Attention 呢…) (CV 界一直在往 NLP
界靠，而我要補的論文也越來越多了…)</p>
<p><a
href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p>
<p>keywords: BERT <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p>先來介紹一下 BERT 囉，這邊先直接附上李宏毅的連結：</p>
<p><a
href="https://youtu.be/gh0hewYkjgo">https://youtu.be/gh0hewYkjgo</a></p>
<p>BERT 是一個自監督式學習的架構，其利用兩種不同的任務來 pre-train
模型，最後再利用 transfer learning 把下遊任務做 fine-tune 微調結果。其
backbone 為一大堆 Transformer 的 Encoder 堆疊而成的。</p>
<p><img src="https://i.imgur.com/6OymkLb.png"
alt="image-20220509005826295" /></p>
<p>BERT 模型的 pre-train
過程有點像在模仿我們學語言的過程，一是利用「克漏字」挖空任務，二是利用「段落重組」來達成訓練。</p>
<p>第一個「克漏字」任務，英文稱 Masked LM。給定一個句字，接著把它其中
15% 的「方格字」給去掉替換特別的符號，其中 80% 會替換成 mask
代表什麼都沒有，10% 換成隨機完全跟上下文無關的字，10% 不進行改動
(照原本的字當輸入)。</p>
<p><img src="https://i.imgur.com/KdbjTcz.png" alt="Image" /></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ex:</span><br><span class="line">我今天心情不錯</span><br><span class="line"></span><br><span class="line">我「今」天「心」情不「錯」</span><br><span class="line"></span><br><span class="line">我 mask 天小情不錯</span><br></pre></td></tr></table></figure>
<p>替換後的完整句字會全部送進 Transformer 的 Encoder
中編號學習，最後只有「被修改過」的字把它的向量拿出來，做一層簡單的全連接層後做一個分類任務，對應預測分類的字是不是等於標籤中的字。</p>
<p>這樣子做的原因是，利用在句子中挖
mask，網路會更依賴上下文的關系去推論出字詞。且除了放 mask
之外額外增加的兩種改動
(錯字、正確字)，使得網路除了有從未知詞推論出詞的能力外 (mask -&gt;
詞)，還多了糾錯能力 (錯 -&gt; 詞)，使得 BERT 更
Robust，放在各種下游任務中也更靈活。</p>
<p>第二個「段落重組」任務，英文稱 Next Sentence
Prediction：給定一篇文章中的兩句話，判斷第二句話在文章中是否緊跟在第一句話之後</p>
<p><img src="https://i.imgur.com/UdGRQfw.png" alt="Image" /></p>
<p>在 BERT 最前面加入 CLS token，這個 CLS token 初始值為亂數，會隨著
BERT 中 Encoder 的進行而學習到<strong>其它 token
的綜合特徵表示</strong>，也可理解成其它 token
的精華重點整理，最後進入一個全連接層做一個 Yes/No 的二分類任務 -&gt;
是不是第二句在第一句的後面</p>
<p>這種通過把原文打亂順序再還原出正確順序的做法，需要對文章的大意有充分、準確的理解才做得出來，因此加入了「段落重組」任務後
BERT 加強了在文意理解上的能力</p>
<p>BERT 模型通過對 「克漏字」 Masked LM 和 「段落重組」 Next Sentence
Prediction
兩項任務，使模型對每個字詞的向量特徵表示都能盡可能的全面、準確的描述輸入文章的整體訊息，為後面下游任務的
fine-tune 微調給了很好的參數初始值，打下很好的基礎</p>
<h3 id="reference">Reference</h3>
<p><a
href="https://www.youtube.com/watch?v=xI0HHN5XKDo&amp;t=502s&amp;ab_channel=CodeEmporium">BERT
Neural Network - EXPLAINED!</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/360343071">知乎
关于BERT中的那些为什么 (十萬個為什麼 XD)</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Vision Transformer</tag>
        <tag>Self-supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>RCNN 全家桶速讀：R-CNN、Fast R-CNN、Faster R-CNN</title>
    <url>/2022/06/20/RCNN-%E5%85%A8%E5%AE%B6%E6%A1%B6%E9%80%9F%E8%AE%80%EF%BC%9AR-CNN%E3%80%81Fast-R-CNN%E3%80%81Faster-R-CNN/</url>
    <content><![CDATA[<p>碩論題目下來了，是有關於 3D
的瑕疵辨識，根據前面學長姐的題目來看，看起來這個題目偏向分類任務，可能是瑕疵的二分類吧…，不管題目是什麼，還是多看看一些論文為未來鋪路吧
XD，搞不好哪一天真的用上了。</p>
<p>接下來就先來看看 Object Detection 目標偵測的元老：RCNN 系列吧</p>
<p>keywords: R-CNN、Fast R-CNN、Faster R-CNN、Object Detection
<span id="more"></span></p>
<h2 id="什麼是-object-detection">什麼是 Object detection</h2>
<p>在介紹之前我們先來看看傳統的深度學習電腦視覺任務，可大概分成下列 4
個任務：分類、定位、偵測、分割 <img
src="https://i.imgur.com/OsA93iB.png" alt="Image" /></p>
<p>所謂分類是指：找出影像中的目標物體類別；定位是指要找出目標物體所在的空間範圍；而偵測是分類
+ 定位的結合，且有時還會加上多類別的任務；最後一個分割同樣也是分類 +
定位，它會描出物體的輪廓，不過它比較像「像素」級別的分類</p>
<p>分類的目標為輸出 k 個不同的物體的類別，通常使用 cross entropy 算出 0
~ 1 之間的機率，來表示某類別的可能性；定位的目標為輸出 4 個值 (x, y, w,
h) 用來表示框框的 (起始點、長寬)，通常使用 IoU 來算出交集的條件機率 0 ~
1</p>
IoU 的定義為：兩框框的 聯集/交集，如下圖。通常我們定義 IoU &gt; 0.5
就是一個不錯的效果 <img src="https://i.imgur.com/uWG7Izw.png"
style="width:50.0%" alt="Image" />
<p align="center">
IoU 定義
</p>
<p>我們已經定義好了 IoU
後，但是我們要怎麼在一張影像上選出框呢？一個最直覺的方法是窮舉暴力，把所有
pixel 的框框排列組合一遍…？聽起來效率就超低，於是就有了後續的 Object
detection 來解決這一系列問題…</p>
<h2 id="r-cnn">R-CNN</h2>
<p>原 paper 連結：<a href="https://arxiv.org/pdf/1311.2524.pdf">Rich
feature hierarchies for accurate object detection and semantic
segmentation</a></p>
<h3 id="網路架構">網路架構</h3>
<p>2013 年提出的 R-CNN 是第一個使用 CNN 來實作 Object detection
的網路架構</p>
<img src="https://i.imgur.com/BPQPRjE.png" alt="Image" />
<p align="center">
R-CNN 完整架構圖
</p>
<h3 id="selective-search">Selective Search</h3>
<p>提出 Selective Search，它是一個改善窮舉的演算法，一張影像經過
Selective Search 後會生成出 2k 個 Region Proposal。這個 Selective Search
因為是傳統演算法的緣故，不能放進 GPU 中加速。</p>
<img src="https://i.imgur.com/ONnVaHs.png" alt="Image" />
<p align="center">
Selective Search 示意圖，在影像上有規律的找出許多框框
</p>
<p>接著再把圖片 warped (大小弄成一樣)，好放進後續 CNN(AlexNet)
做訓練。這個 CNN 相當於特徵提取器 backbone，負責找出 wraped
過後的框框的特徵，再用 SVM 做分類
(跟現在直接全連結不大一樣)，雖然這些步驟現在看起有點過時，但在那個年代效果超好。</p>
<h3 id="problems-of-r-cnn">Problems of R-CNN</h3>
<ol type="1">
<li>每一個 Region Proposal 都要經過一次 CNN 運算 -&gt; 超極慢</li>
<li>可以很明顯發現網路是兩階段：(先用 Selective Search、再用 CNN
找特徵)，不是 end-to-end 架構</li>
<li>分類、BBox 是分開的網路，Loss 也是個別計算</li>
<li>找出來的 Region Proposal 要事先存在本地，浪費硬碟空間</li>
</ol>
<h2 id="fast-r-cnn">Fast R-CNN</h2>
<p>原 paper 連結：<a href="https://arxiv.org/pdf/1504.08083.pdf">Fast
R-CNN</a></p>
<h3 id="網路架構-1">網路架構</h3>
<p>2015 年又提出 Fast R-CNN，目的在改善 R-CNN 太慢的問題，首先把 Region
Proposal 改名為 RoI (Region of Interest 感興趣的範圍)。及提出 RoI
Pooling 使得 Fast R-CNN 在後半提取特徵的部份皆是 CNN 架構。</p>
<p><img src="https://i.imgur.com/1u91AnU.png" alt="Image" /></p>
<h3 id="cnn-權重共享">CNN 權重共享</h3>
<p>網路先把整張影像放進「全」CNN 網路 (VGG) 中找特徵，接著找出 RoI
的範圍 (這裡還是用特別的演算法獨立先算出來的)，由於 CNN feature map
的特性，我們可以先找出 RoI 在原圖的座標，接著直接映射到 feature map
上，如圖左邊的紅框框，這麼做的好處是：CNN 權重是共享的，且只需做一次
CNN，不需要有幾個 RoI 就要做幾次 CNN</p>
<p>再找一次 RoI -&gt; RoI Pooling 作用在於把大小不同的影像變成一樣大小
multi-task loss，把分類、BBox 的 Loss 合併在一起 end-to-end model</p>
<h3 id="roi-pooling">RoI Pooling</h3>
<p>RoI Pooling 的目的與 R-CNN warped
相同，皆是把影像變成相同大小，只是原本的做法是直接
scaling，缺點也很明顯：影像比例嚴重失真。改進的方法是使用 max pooling
來取代 scaling。</p>
<p>詳細做法：pooling 取的範圍不再是一個正方型，(例 2x2 pooling，就是在
2x2 選一個最大值替代)，改成一個長方型，它的長寬是：RoI
除以目標大小，去小數。對，其實 RoI Pooling
簡單說就是在去小數而已，也因去了小數，會讓資訊不準
(多框一點，少框一點)。</p>
<p><img src="https://i.imgur.com/R6L3Y4N.png" alt="Image" /></p>
<p>更多詳細介紹可參考以下這篇文章 <a
href="https://erdem.pl/2020/02/understanding-region-of-interest-ro-i-pooling">Understanding
Region of Interest (RoI Pooling)</a>。也可直接記結論：RoI Pooling
就是利用不規則的 pooling 來達到輸出影像大小皆相同</p>
<h3 id="multi-task-loss">multi-task loss</h3>
<p>把下游的分類、BBox 迴歸任務合併在一起，設計出多任務的
Loss，其實就是把兩個不同任務的 Loss
直接相加，這樣做的好處是速度快，只需跑一次網路兩個一起訓練，缺點就是理論上會比分別算的網路不精確一些</p>
<h3 id="problems-of-fast-rcnn">Problems of Fast RCNN</h3>
<p>還是用 selective search 來決定 RoI，這個還是很花時間…</p>
<h2 id="faster-rcnn">Faster RCNN</h2>
<p>原 paper 連結：<a href="https://arxiv.org/pdf/1506.01497.pdf">Faster
R-CNN: Towards Real-Time Object Detection with Region Proposal
Networks</a></p>
<h3 id="網路架構-2">網路架構</h3>
<p><img src="https://i.imgur.com/cLGmQkc.png" style="width:50.0%"
alt="Image" /></p>
<p>提出 RPN (region proposal network) 專來解決 selective search
無法加速的問題。雖然說 selective search 及 RPN
在運算複雜度上並沒相差太多，但 RPN 因是神經網路形式所以可用 GPU
加速。</p>
<p>網路架構大多改動前半段，新增 RPN
用來找出框框，且這個框有個特別的名字：Anchor
(中文可翻錨點、先驗框)。後半段基本上沒什麼變動了</p>
<h3 id="rpn">RPN</h3>
<p>RPN 的輸入是來自 CNN 去掉全連接層的 Feature map，輸出同樣為 RoI</p>
<p>Anchor 先驗框，可看成預先列出幾個，事先設計好的框
(不同長寬比例、不同大小比例)，再依據 Anchor 微調，找出真正的框</p>
<p><img src="https://i.imgur.com/pK8N6lx.png" style="width:50.0%"
alt="Image" /></p>
<p>RPN 的流程如下：會先用預先設計好的 Anchor 來當 window，依據一定的
stride 在 feature map 上移動 (設 stride 是為了更有效率的灑網)，接著經過
RoI Pooling 把影像變為一樣再放進 CNN 中，最後一樣有兩個任務，一是 2k
的分類任務 (背景、目標物)、一是 4k 的 BBox (x, y, h, w)</p>
<p><img src="https://i.imgur.com/jJpTWSr.png" style="width:50.0%"
alt="Image" /></p>
<p>這還沒結束喔，記得以上步驟都只是 RPN 而已，接著會把 4k 的 RoI
結果先做一遍 IoU，如果 IoU &gt; 0.7 當做正樣本、如果 IoU &lt; 0.3
當做負樣本，其餘區間直接捨棄。</p>
<p>接著會把 RPN 所生出來的 RoI 再用 Fast R-CNN 一模一樣方法做下去 (經
VGG 再有 k 分類任務及 4k BBox 任務)</p>
<h3 id="loss">Loss</h3>
<p><span class="math display">\[
\begin{align}
\mathcal{L}(\{p_i\},\{t_i\})=\frac{1}{N_{cls}}\sum_{i}L_{cls}(p_i,p_i^*)+\lambda
\frac{1}{N_{reg}}\sum_{i}p_i^*\mathcal{L}_{reg}(t_i,t_i^*)
\end{align}
\]</span></p>
<p>式子分為左右兩邊，左邊為分類任務 Loss，簡單做了一個 cross
entropy，而右邊為 BBox 的迴歸 loss，迴歸 loss 中的 <span
class="math inline">\(t\)</span> 可再展開如下：</p>
<p>其中 <span class="math inline">\(x_a\)</span> 為 Anchor、<span
class="math inline">\(x\)</span> 是 predict 預測 Box、<span
class="math inline">\(x^*\)</span> 是 ground truth</p>
<p><span class="math display">\[
\begin{align}
t_x = (x-x_a)/w_a&amp;,\quad t_y = (y-y_a)/h_a \\
t_w = \log(w/w_a)&amp;, \quad t_h = \log(h/h_a) \\
t_x^* = (x^*-x_a)/w_a&amp;,\quad t_{y^*} = (y^*-y_a)/h_a \\
t_w^* = \log(w^*/w_a)&amp;, \quad t_h^* = \log(h^*/h_a)
\end{align}
\]</span></p>
<p>所以 <span class="math inline">\(t\)</span> 的意思就是「預測」的 BBox
與「Anchor」 的 BBox 的誤差，而 <span class="math inline">\(t^*\)</span>
的意思就是「Anchor」 的 BBox 與 「Ground truth」 的誤差，而 <span
class="math inline">\(t\)</span> 與 <span
class="math inline">\(t^*\)</span> 做 Loss
就代表它們兩個越像越好，我理解為有點像在 Anchor 與 Ground truth
中間找個中間框，距離到它們兩個剛好相等</p>
<p>最後還加入了 smooth label 來平滑化標籤，避免網路難以收斂</p>
<h2 id="論語">論語</h2>
<p>以上大概介紹了 R-CNN
家族演變史，可看到網路架構不斷的往加速發展，且最後也實驗了全部是神經網路的
end-to-end 架構，不需要再分兩個不同的網路來訓練了。</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://www.youtube.com/watch?v=M1mN03REGU8&amp;t=356s&amp;ab_channel=AshingTsai">Introduction
of RCNN,Fast RCNN,Faster RCNN
中文，講得很清楚，大部份是參考這個影片</a></p>
<p><a
href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf">某線上課程
ppt</a></p>
<p><a
href="https://erdem.pl/2020/02/understanding-region-of-interest-ro-i-pooling">RoI
Pooling 超圖解</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>RCNN 全家桶速讀：Mask R-CNN</title>
    <url>/2022/06/22/RCNN-%E5%85%A8%E5%AE%B6%E6%A1%B6%E9%80%9F%E8%AE%80%EF%BC%9AMask-R-CNN/</url>
    <content><![CDATA[<p>接下來來看看由 FAIR 何愷明大神在 2017 改進 Faster R-CNN 提出的 Mask
R-CNN，實現了 Instance segmentation
實例分割。除了可以有分割的效果外，也可以知道同類別的不同物體
(例如兩隻不同的狗狗)</p>
<p>原論文：<a href="https://arxiv.org/pdf/1703.06870.pdf">Mask
R-CNN</a></p>
<p>keywords: Mask R-CNN <span id="more"></span></p>
<h2 id="網路架構">網路架構</h2>
<p>Mask R-CNN 大部份是架構都是由 Faster R-CNN 改造而來的，簡單的說 Mask
R-CNN 在後面的流程中多加了一個分支
mask，下圖為原論文的圖，可以看到中間做完 RoIAlign 後分了兩個分支，上面
class box 是原 Faster R-CNN 架構，而下面則是 mask R-CNN 新設計的架構</p>
<p><img src="https://i.imgur.com/NGpTdVC.png" alt="Image" /></p>
<p>而更詳細的架構圖可參考 <a
href="https://www.youtube.com/watch?v=5VLI_gbpocE&amp;ab_channel=WilsonHo">圖解兩階段物件偵測算法_Part09
: Mask R-CNN</a> 影片簡報中的附圖</p>
<p>圖中上半部分支與 Fast R-CNN 一致，會產生 box 及 cls
結果，不同的部份在多了下面的 FCN (Fully Convolutional
Network)，經過一串的 CNN 組合最後得到一個 14x14x80 的 feature map (80
的原因是 COCO Dataset 會分 80 類)</p>
<p><img src="https://i.imgur.com/02ABxWS.png" alt="Image" /></p>
<p>那最後分割的結果是怎麼來的呢？首先會從 cls 中知道物體是第 k
個類別，再從對應的 14x14x80 中選出第 k 個 14x14 feature map，再依據
RolAlign 的變形大小把 box 放回原本的影像大小，最後就是結果了。</p>
<p>而以下是原論文提供的 FCN (ResNet)</p>
<p><img src="https://i.imgur.com/IS4qaxp.png" style="width:50.0%"
alt="Image" /></p>
<p>作者也有做使用 FPN 的實驗，效果比 FCN 好上 4 個百分點</p>
<p><img src="https://i.imgur.com/rJIWyAr.png" style="width:50.0%"
alt="Image" /></p>
<p><img src="https://i.imgur.com/R5Yz7D6.png" style="width:50.0%"
alt="Image" /></p>
<h2 id="roialign">RoIAlign</h2>
<p>另一個改動較大的地方是把 RoI Pooling 更換成更複雜的
RoIAlign。而它的核心概念其實很簡單，用一句話帶過：取整的部份從去小數點變成雙線性內差。</p>
<p><img src="https://i.imgur.com/tXZveNN.png" style="width:50.0%"
alt="Image" /></p>
<p>原本的 RoI Pooling (假設是 2x2 pooling)
做法是直接把長寬的小數點去掉，得到一個大框框，接著再分割出 4 塊
pool，如果還是不整除，再把小數點去掉。這一來一往去掉了兩次小數點，與原框框的誤差變得非常大</p>
<p>而 RoIAlign
則是使用雙線性內差來解決小數點全丟棄的問題，一、大框框的小數點不去掉，二、一個
pool 的值是從 pool 取等分的 4
個點，而每個點都是從附近像素做雙線性內差而來的，最後再對這個 4 個點找
max</p>
<p><img src="https://i.imgur.com/nlgD6OF.png" style="width:50.0%"
alt="Image" /></p>
<p>作者在後續做實驗比較，可看到做了 RoIAlign
提升的百分比非常多，可見去掉小數點是很傷的事情。(不過要記得，當運算量上升時，理論效果好是應當的，但這個上升的比例好像很值得呢
XD)</p>
<p><img src="https://i.imgur.com/8txHiM0.png" style="width:50.0%"
alt="Image" /></p>
<h2 id="網路訓練">網路訓練</h2>
<p>那網路的 Loss 如果設計呢？除了原本 Faster R-CNN 的 <span
class="math inline">\(\mathcal{L}_{cls}\)</span> <span
class="math inline">\(\mathcal{L}_{box}\)</span> 外，加上了來自 mask 的
<span
class="math inline">\(\mathcal{L}_{mask}\)</span>，它的算法精神在：把
Ground truth 的像素與由 feature map 生成的像素做簡單的 L2 loss</p>
<h2 id="一些網路效果">一些網路效果</h2>
<p><img src="https://i.imgur.com/D0LKUG1.png" alt="Image" /></p>
<h2 id="結論">結論</h2>
<p>Mask R-CNN 改進了 Faster R-CNN
許多小細節，同時也引入了分割的想法，對於未來的偵測分割網路功不可沒</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://www.youtube.com/watch?v=5VLI_gbpocE&amp;ab_channel=WilsonHo">圖解兩階段物件偵測算法_Part09
: Mask R-CNN</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title>DLCV 射飛鏢 - 小烏來一日遊</title>
    <url>/2022/06/24/DLCV-%E5%B0%84%E9%A3%9B%E9%8F%A2-%E5%B0%8F%E7%83%8F%E4%BE%86%E4%B8%80%E6%97%A5%E9%81%8A/</url>
    <content><![CDATA[<p>6/19
實驗室租車一起上山去小烏來玩，難得大家裡開電腦螢幕前，當個一日熱血宅男。至於為什麼是小烏來嘛…這一切的一切都從一個瘋狂的想法開始…</p>
<p>keywords: 小烏來瀑布、巴陵大橋、大溪老茶廠 <span id="more"></span></p>
<p>為什麼突然要去山裡面玩呢？讓我娓娓道來。把時間拉回到 2022 年 1 月 1
號元旦跨年凌晨 2 點，我、佩昇、仲瑜三個人約好一起在實驗室打
switch、看電影、看日出，享受一個不一樣的跨年，原本的計劃是我們想辨法把時間拖到凌晨
5 6 點，接著去看日出。而就在我們從 12 點打 switch 打到 2
點後，一行人開始有點神智不清，說著要學剛剛看 Youtube The DoDo Men
嘟嘟人的美國射飛鏢旅行企劃，自己也來搞一個「桃園」版的飛鏢旅行，在大家半夜神智不清氣氛的薰陶下，說時遲那時快，一個熱騰騰的桃園地圖就印出來了。於是乎就有下面這個影片…</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/pqAaltmXbIU" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<p>嗯…果然人還是需要被逼到絕境才會生出一些瘋狂的想法…，而這個出遊計劃就這麼一拖再拖，最後決定在大家期末考都結束的
6
月末出發，射飛鏢容易真的要湊出大家都可以時間出遊才是最難的地方…。這次出遊加我一共
6
個人：我、佩昇、仲瑜、翔宇、蔡杰、佳男，考慮到出路不好開我家的車車可能應付不過來，我們選擇租一台
7 人座的 irent 過去，於是就這麼出發囉！</p>
<p>我們這次的行程規劃排好排滿，想說難得進到山裡面，就沿路把能收的景點都收一收。行程依序是：小烏來瀑布、巴陵大橋、大溪老茶廠、大溪老街</p>
<p><img src="https://i.imgur.com/k5Ge8j6.png" style="width:75.0%"
alt="Image" /></p>
<h2 id="小烏來瀑布">小烏來瀑布</h2>
<p>起了個大早上山，天氣超極好，而且沒什麼車，我們就一路從中央殺到今天行程的重頭戲
-
小烏來瀑布。小烏來瀑布有個售票處，因為現在是疫情期間除了沒有人數管制外，一個人也只要優惠
30 塊。</p>
<p>今天我特別帶了把吉他上山，本來想說當移動音響，而且看起來好像很 chill
的感覺，也的確效果不錯，只是這天氣…反而會覺得它是個累贅。</p>
<p><img src="https://i.imgur.com/OpdYPr3.jpg" style="width:50.0%"
alt="Imgur" /></p>
<p>一行人準備出發囉！</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/ZQLQaI-KB8M" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<img src="https://i.imgur.com/kTXLKIt.png" style="width:50.0%"
alt="Image" />
<p align="center">
小烏來售票處門前
</p>
<img src="https://i.imgur.com/chyrXt5.png" style="width:50.0%"
alt="Image" />
<p align="center">
通往天空步道的超長木棧道
</p>
<p>第一站先去傳說中的天空步道。所謂的天空步道就是腳底下透明玻璃，再下面是瀑布的超高懸崖，一行人在上面享受美景外，同時也覺得超高很可怕</p>
<img src="https://i.imgur.com/SiCz9QB.jpg" style="width:50.0%"
alt="Imgur" />
<p align="center">
天空步道門口
</p>
<img src="https://i.imgur.com/LWCH3ae.png" style="width:50.0%"
alt="Image" />
<p align="center">
在門口來張照片 XD，吉他效果滿分
</p>
<p>每個人都不太感往前靠
XD，而且重點是越往前橋越晃，不是開完笑的，雖然心裡知道不會有什麼事，但整個懸空的感覺還是讓人覺得毛毛的</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/7FBx2ULNzRo" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<p>柳暗花明又一村，如果撐過懼高的考驗，迎之而來的是溪谷瀑布中的彩虹！瀑布的水氣加上超大太陽天然的形成美麗的瀑布，讓人覺得剛剛一路怕怕的感覺一下子湮消而散</p>
<p><img src="https://i.imgur.com/vv5FZGz.png" style="width:50.0%"
alt="Image" /></p>
<p>接下來就是大家的拍照時間 XD</p>
<p><img src="https://i.imgur.com/N7DSV6N.png" style="width:50.0%"
alt="Image" /></p>
<p><img src="https://i.imgur.com/WOuc7in.png" style="width:50.0%"
alt="Image" /></p>
<p><img src="https://i.imgur.com/H2RwcgJ.png" style="width:50.0%"
alt="Image" /></p>
<p><img src="https://i.imgur.com/vqVdOCr.png" style="width:50.0%"
alt="Image" /></p>
<p><img src="https://i.imgur.com/m0omOlX.jpg" style="width:50.0%"
alt="Imgur" /></p>
<p>速速拍完照後，我們接著往下一個點移動。該是吉他出場的時候了，在橋頭把吉他拿出來準備邊走邊彈</p>
<p><img src="https://i.imgur.com/lGyrwKP.jpg" style="width:50.0%"
alt="Image" /></p>
<img src="https://i.imgur.com/IhIkUB4.jpg" style="width:50.0%"
alt="Image" />
<p align="center">
佳男專業攝影師，專業的角度
</p>
<p>接著我們到小烏來第二個有名的景點，也是唯二需要收門票的點 -
天空繩橋，在這裡我們會走過一座比天空步道晃 100
百倍的吊橋，超極晃同時也超極高，今天兩個行程都是來壯膽的嗎 XD</p>
<img src="https://i.imgur.com/fAsIKQh.jpg" style="width:50.0%"
alt="Imgur" />
<p align="center">
來張橋頭照
</p>
<img src="https://i.imgur.com/127Lpcp.jpg" style="width:50.0%"
alt="Imgur" />
<p align="center">
又長又高的橋
</p>
<p>又到了倒處拍拍環節</p>
<p><img src="https://i.imgur.com/GXagPjq.jpg" style="width:50.0%"
alt="Imgur" /></p>
<p><img src="https://i.imgur.com/cByMJpk.jpg?1" style="width:50.0%"
alt="Imgur" /></p>
<p><img src="https://i.imgur.com/wsmSTPN.png" style="width:50.0%"
alt="Image" /></p>
<p><img src="https://i.imgur.com/i9yzSPz.png" style="width:50.0%"
alt="Image" /></p>
<p>後來一行人延著步道爬上來，終於回到剛剛天空吊橋的起點處。隨便找到了一個涼亭開始今天的另一個重頭戲：再丟一次飛鏢決定下一次出遊的地方，當然要直接在原地丟飛鏢才會有感覺阿，但誰也沒想到會發生下面的事情…</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/yAKjNy8ys0c" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<p>再換另一個更清楚的角度：</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/2rA7HkEeptE" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<p>我們唯一的筆就這麼飛到山谷下面去啦，它毫無殘念的直直往前飛，從柵欄下的空洞穿過去，一行人也沒有要阻止那隻筆
XDD
眼睜睜的看它往前飛，真是意想不到的發展。沒辨法企劃失敗，只能再找一天在實驗室補丟囉。不過我們還是有跟地圖合照一張，記念一下
XD</p>
<p><img src="https://i.imgur.com/SDDBU5I.png" style="width:50.0%"
alt="Image" /></p>
<h2 id="巴陵大橋古道">巴陵大橋、古道</h2>
<p>下一站我們再往山裡走，走到巴陵大橋，為什麼要這麼往山裡面走呢？因為想說都到山裡面了，就把能經過的點都收一收啦。</p>
<p>首先我們是到拉拉山遊客中心前面的「巴陵古道」，這個古道的特色是可以從拉拉山上往下走，一路到巴陵大橋旁邊，從山上的角度來欣賞巴陵大橋</p>
<img src="https://i.imgur.com/EwZSJxl.jpg" style="width:50.0%"
alt="Imgur" />
<p align="center">
非常粉的巴陵大橋
</p>
<img src="https://i.imgur.com/2GZ1dyn.jpg" style="width:50.0%"
alt="Imgur" />
<p align="center">
往另一個方向拍的照片，藍天白雲配綠綠的樹，怎麼看都看不膩
</p>
<p>只可惜步道的盡頭「巴陵大橋」的那一側現在正在施工，連舊的吊橋都進不去，大家就只能沿著步道走回去，真的有點可惜</p>
<h2 id="吃午餐">吃午餐</h2>
<p>午餐我們是吃下巴陵的一家瓦旦飲食店，點了一份 6~8 人 6 菜 1 湯 2000
元，以能在山上吃到這麼多的量來說，已經很不錯了，而且重頭戲是還有一隻烤全雞！</p>
<img src="https://i.imgur.com/7eFbmDH.jpg" style="width:50.0%"
alt="Imgur" />
<p align="center">
菜真的超極多的，大家都吃很飽
</p>
<img src="https://i.imgur.com/1T0Wip7.jpg" style="width:50.0%"
alt="Imgur" />
<p align="center">
認真幫大家撥雞佳男還有翔宇
</p>
<h2 id="大溪老茶廠">大溪老茶廠</h2>
<p>接著就回程下山了，我們算在巴陵輕輕點了一下而已，希望下次能有機會再來一次，能更深入的來玩拉拉山。回程路上經過三民，就進最近很熱門的大溪老茶廠逛逛，如果說上午行程是熱血爬山青年的話，那下午就是走文藝風
XD</p>
<img src="https://i.imgur.com/ApjYbyg.jpg" style="width:50.0%"
alt="Image" />
<p align="center">
非常適合當一日網美拍照打卡的地方
</p>
<img src="https://i.imgur.com/ANSaWQR.png" style="width:50.0%"
alt="Image" />
<p align="center">
實驗室顏值擔當來拍一張
</p>
<p>大溪老茶廠大家的照片不多，我們剛到的時候就聽到前方有導覽的聲音，一看才知道原來一天會有兩次的導覽行程，而導覽員正是大溪老茶廠的廠長！廠長非常生動的講解館內每個擺設的意思及背後的歷史，原本死死放在那裡生灰塵的東西一下子就生動起來了，果然會講故事的人就是不一樣呢！很值得推薦大家去聽聽廠長的講解！</p>
<p>最後大家一起用門票錢 (入場費 100 元，可折抵館內消費) 買了一個 680
的綠茶，當成實驗室的「室茶」，第一次看到原來綠茶葉這麼大一片！</p>
<h2 id="大溪老街">大溪老街</h2>
<p>時間也慢慢的來到下午 4
點半，快到了晚餐時間，而我們行程的最後一站也來到了大溪老街，來覓食晚餐。把車停在大溪橋的超大停車場後，大家一起走路過橋去大溪老街！</p>
<p><img src="https://i.imgur.com/QbmNrSS.jpg" style="width:50.0%"
alt="Imgur" /></p>
<p><img src="https://i.imgur.com/SpJYo9d.jpg" style="width:50.0%"
alt="Image" /></p>
<p>到了過橋後的超高樓梯，一行不知道哪來的想法，玩剪刀石頭布，輸的人要用走樓梯上去，另一半人則可以享受電梯的方便…，於是就有了下面的影片…</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/r_JSPNJuMbk" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<p>結果因為天氣太熱，下面一大堆人也不想走樓梯在等電梯，排了好長的隊伍；結果反而是走樓梯的人最快到上面，我、佳男、翔宇雖然輸了，但我們三個人先到了電梯門口打視訊給佩昇，說：「怎麼那麼慢啦！」</p>
<p><video src="https://i.imgur.com/wgUzfIt.mp4" style="width:50.0%"
controls=""><a
href="https://i.imgur.com/wgUzfIt.mp4">Imgur</a></video></p>
<p>果然出去玩還是要ㄎ一ㄤ一點才會好玩
XD，接著就是大家一起逛大溪老街啦！</p>
<img src="https://i.imgur.com/eo2FdJY.png" style="width:50.0%"
alt="Image" />
<p align="center">
靠近晚上的大溪老街店家開得不是很多，但人潮未減
</p>
<p><img src="https://i.imgur.com/AQdxUBz.jpg" style="width:50.0%"
alt="Imgur" /></p>
<p><img src="https://i.imgur.com/Lk4fsHS.png" style="width:50.0%"
alt="Image" /></p>
<p>大家買了一些大溪老街特色小吃：豆干、麻糬、花生捲冰淇淋…，最後一起在中正公園樓梯旁一邊看夕陽一邊吃著晚餐…吹著從山下大漢溪吹來涼涼的微風，為今天的一日遊劃下完美的句點</p>
<p><img src="https://i.imgur.com/J5fdtBW.jpg" style="width:50.0%"
alt="Imgur" /></p>
<p>回程走回大溪橋時天已慢慢暗下，橋上的燈也悄悄亮起，此起彼落的在傍晚夕陽中相互呼應，仿彿在說謝謝你上午的辛勞，該換我上場了</p>
<p><img src="https://i.imgur.com/A9VnPuV.png" style="width:50.0%"
alt="Image" /></p>
<p>最後回車上前再跟大溪老街做最後的告別</p>
<p><img src="https://i.imgur.com/Oye3gz8.jpg" style="width:50.0%"
alt="Imgur" /></p>
<p>…車子慢慢的開…，為了躲避大溪塞車的我選擇繞了一下龍潭市區去。在彎來彎去的市區道路中，大家一整天累積的睏意也全跑出來了，哈哈大家真的累到了，今天真是一個又順利又充實的一日遊呢！</p>
<div class="video-container"><iframe src="https://www.youtube.com/embed/4Enr5Yh7Cg0" frameborder="0" loading="lazy" allowfullscreen></iframe></div>
<p>大約晚上 8 點回到中央租車的地方，總計車費 3000，扣掉 irent
優惠折扣後剩 2400，平均一個人
430，以租車一整天這樣來說一個人這樣真的超極便宜。</p>
<p>還車後大家就地解散，各自回家休息去，隔天聽大家說自己很早就睡了，很久沒有這麼累到了哈哈。至於我跟佳男結束後不回家，還跑到實驗看
3D 印表機、彈吉他那又是另外一件事情了 XD。</p>
<p>咦…那…飛走的麥克筆呢？還有要射飛鏢嗎？這個就是下一次實驗室出遊的故事了…</p>
<p>To be continued...</p>
]]></content>
      <categories>
        <category>遊記</category>
      </categories>
      <tags>
        <tag>DLCV 射飛鏢</tag>
      </tags>
  </entry>
  <entry>
    <title>UNETR: Transformers for 3D Medical Image Segmentation: 把 Transformer 與 U-Net 結合</title>
    <url>/2022/07/07/UNETR-Transformers-for-3D-Medical-Image-Segmentation-%E6%8A%8A-Transformer-%E8%88%87-U-Net-%E7%B5%90%E5%90%88/</url>
    <content><![CDATA[<p>而本篇作者將 Transformer 架構與 U-Net 架構融合，提出混合架構
<strong>UNE</strong>t <strong>TR</strong>ansformers
(UNETR)，其中最重要的特色與 V-Net 相同，UNETR 的輸入同樣是三維的
volumetric (3D) medical image</p>
<p>keywords: UNETR、volumetric medical image <span id="more"></span></p>
<h2 id="abstract">Abstract</h2>
<p>在影像分割任務上 FCNNs (Fully Convolutional Neural Networks)
也就是全部都是卷積的架構取得了相當不錯的成績，其中又以 U-Net
效果提升最為顯著。不過卷積雖然有很好的 Inductive
bias，可以很有效的去學習局部注意力，但在全局上；例如距離很遠的兩像素；效果就不是很好。</p>
<p>在 2020 年 Google 將 Transformer 架構轉移到影像處理領域上後，引入
Self-Attention，靠著 Self-Attention
全局的特色，卷積不夠全局的詬病得以解決。而本篇作者將 Transformer 架構與
U-Net 架構融合，提出混合架構 <strong>UNE</strong>t
<strong>TR</strong>ansformers (UNETR)，其中最重要的特色與 V-Net
相同，UNETR 的輸入同樣是三維的 volumetric (3D) medical image</p>
<h2 id="introduction">Introduction</h2>
<p>作者在這邊快速介紹了一下影像分割的進展：</p>
<p>首先是 U-Net 的提出，U-Net 的 downsampling-upsampling
先提取特徵再從特徵中回歸原圖，這種作法在當時取得了巨大的成功。再來是因卷積的
Long-range dependencey 被 localized receptive field
限制著了，後續有人提出了加入 atrous convolutional layers 來加大
receptive field</p>
<p>後在因 Transformer 在 NLP 界大放異彩，以及 ViT
實驗的提出，Transformer 應用在 CV
界上似乎是個可行的方法，後續也有大量的論文在針對這個做研究。</p>
<p>而作者在搭上了一輛順風車，提出以 Transformer 為基礎的 3D U-Net
分割網路，有其以下三個特色</p>
<ol type="1">
<li>UNETR 可以直接使用 3D volumetric data 當輸入</li>
<li>UNETR 的 Encoder 使用 Transformer 架構，並加入了多層的
skip-connection 可以把不多層的特徵圖融合在一起，達成類似 FPN 的效果</li>
<li>UNETR 可以直接把 3D volumetric data 切成不同的 patch 放進
Transformer，不需經過任何卷積</li>
</ol>
<h2 id="網路架構">網路架構</h2>
<p>先直接上架構圖</p>
<p><img src="https://i.imgur.com/U5VCY7v.png"
alt="image-20220702120847005" /></p>
<p>網路架構分為三個部分：資料前處理、encoder、decoder，整體網路與 U-Net
想法相同，皆是使用多個特徵截取器，並且引入 FPN
的相法，將不同層數的特徵圖那來一個一個做
upsampling，最後全部特徵圖做成一樣，用一個 concat
全部加起來，就是最後的結果。</p>
<p>資料前處理及 Encoder 的部份主要是參考了 ViT
的做法，同樣經過了一些「經典」步驟：</p>
<p><img src="https://i.imgur.com/qsEIpRY.png"
alt="image-20220703002701164" /></p>
<h3 id="資料前處理">資料前處理</h3>
<ol type="1">
<li><p>切分 Patches。將網路輸入的 3D 影像 <span class="math inline">\(x
\in \mathbb{R}^{H\times W\times D\times C}\)</span> ，切成一塊一塊的
Patch。這裡作者拓展二維影像的邏輯，將三維影像 <span
class="math inline">\((H, W, D)\)</span> 看為一張影像的解析度，而 <span
class="math inline">\(C\)</span> 為特徵圖數，並超參數 <span
class="math inline">\(P\)</span> 代表 Patch 的大小。維度變化見下式：
<span class="math display">\[
\begin{gathered}
x \in \mathbb{R}^{(H\times W\times D)\times C} \rightarrow
x_v\in\mathbb{R}^{N\times (P^3\cdot C)}
\end{gathered}
\]</span> 我們把 <span class="math inline">\(H\times W\times D\times
C\)</span> 的影像，依照一個 Patch 為一正方形 <span
class="math inline">\(P\times P\times P\)</span>，將原影像切成 <span
class="math inline">\(N\)</span> 個特徵圖維度為 <span
class="math inline">\(P^3 \times C\)</span> Patch
的一維序列，表示為：<span class="math inline">\(N\times (P^3\cdot
C)\)</span> 。其中 <span class="math inline">\(N=(H\times W\times
D)/P^3\)</span></p></li>
<li><p>Patch Embedding。接著會做一個 Linear
layer，將一維序列的特徵維度改為固定的超參數 <span
class="math inline">\(K\)</span>。維度變化如下： <span
class="math display">\[
\begin{gathered}
x_v\in\mathbb{R}^{N\times (P^3\cdot C)} \rightarrow
x_v\in\mathbb{R}^{N\times K}
\end{gathered}
\]</span></p></li>
<li><p>Positional Embedding。由於不管在二維影像或三維空間中，前面有
reshape
破壞影像結構的動作，所以這裡要加上位置資訊，確保網路在學習的時候是有序的，而不會錯亂彼此的相對位置，變成無序的像素集合。Positional
Embedding 維度為 <span class="math inline">\(x_v\in\mathbb{R}^{N\times
K}\)</span>，加在 Patch Embedding 之後。整體網路前處理的公式如下 (與 ViT
相同)、公式中的 <span class="math inline">\(\mathrm{E}\)</span> 代表
Linear layer： <span class="math display">\[
\mathrm{z}_0=[\mathrm{x}_v^1\mathrm{E};\mathrm{x}_v^2\mathrm{E};...;\mathrm{x}_v^N\mathrm{E}]+\mathrm{E}_{pos}
\]</span> 值得注意的是，在 UNETR 本篇論文中所引用的 ViT 架構並未加入
class token (cls token)，作者說這是因為分割網路後面會有 upsampling
來處理，因此不需要有分類的結果</p></li>
</ol>
<h3 id="encoder">Encoder</h3>
<p>這裡 Encoder 與 ViT 就一模一樣了，一樣是由兩個模組組成：multi-head
self-attention (MSA) 及 multilayer perceptron
(MLP)。小小不一樣的地方是，UNETR 重疊了 12 層 Transformer。公式如下：
<span class="math display">\[
\begin{gathered}
\mathrm{z}&#39;_i=\mathrm{MSA}(\mathrm{Norm}(\mathrm{z}_{i-1}))+\mathrm{z}_{i-1},\quad
i=1...L,\\
\mathrm{z}_i=\mathrm{MLP}(\mathrm{Norm}(\mathrm{z}&#39;_i))+\mathrm{z}&#39;_i,\quad
i=1...L,\\
\end{gathered}
\]</span> Norm 是做 Layer Norm，MLP 層中間會有 activate function
GELU</p>
<p>self-attention 也會分 qkv，也有做一個 softmax 規一化數值，其中 <span
class="math inline">\(K\)</span> 為 q 或 k 的一維長度，用來當作一個平衡
qk 乘積的除數因子，再經過一個 softmax 平滑化 feature
map，方便訓練。接著再乘上 v，得到 self-attention 最後的結果。 <span
class="math display">\[
\begin{gathered}
A=\mathrm{Softmax}(\frac{qk^T}{\sqrt{K_h}})\\
SA(\mathrm{z})=Av
\end{gathered}
\]</span> 接著經過一個全連接層 MSA <span class="math display">\[
\mathrm{MSA}(z) =
[\mathrm{SA}_1(z);\mathrm{SA}_2(z);...;\mathrm{SA}_n(z)]\mathrm{W}_{msa}
\]</span> <img src="https://i.imgur.com/CSZTaDR.png"
alt="image-20220703011929113" /></p>
<h3 id="decoder">Decoder</h3>
<p>藉由 U-Net 的起發，本架構同樣會在第 (3, 6, 9, 12)
層拉出不同層數的特徵圖，藉以達到類似 FPN
多重解析度的功能，而各階的維度變化如下：由一維序列乘上 Patch Embedding
特徵數，變為三維空間乘上 Patch Embedding 特徵數 <span
class="math display">\[
\frac{H\times W\times D}{P^3}\times K
\rightarrow\frac{H}{P}\times\frac{W}{P}\times\frac{D}{P}\times K
\]</span> 接著會經過許多 3x3x3 卷積做 deconvolution，把 Patch
的大小一步步放大，同時特徵圖數也一步步縮小。換句話說，作者作者例用
deconvolution 作者類似 swin transformer 中的「合併
window」，把深層的特徵圖一步步回覆成原輸入影像大小</p>
<p>最後用一個 1x1x1 卷積把特徵圖變成目標分類數量的特徵圖數，再接上一個
softmax 把值距離放大，就可以對每個像素做分類任務得到最後的分割結果。</p>
<h3 id="loss-function">Loss Function</h3>
<p>Loss 的部份作者是使用 Dice Loss 加 Cross-entropy Loss 多任務 Loss
來達成，式子如下：前一項為 Dice Loss 後一項為 Cross-entropy Loss <span
class="math display">\[
\mathcal{L}(G,Y)=1-\frac{2}{J}\sum^J_{j=1}\frac{\sum^I_{i=1}G_{i,j}Y_{i,j}}{\sum^I_{i=1}G^2_{i,j}+\sum^I_{i=1}Y^2_{i,j}}+\frac{1}{I}\sum^I_{i=1}\sum^J_{j=1}G_{i,j}\log
Y_{i,j}
\]</span> Dice Loss 詳解。Dice Loss 是從 V-Net
這篇論文所提出來的想法，它是從 Dice coefficient
改編而來的，是一種計算集合相似度的函數，公式表示如下： <span
class="math display">\[
s=\frac{2|X\bigcap Y|}{|X|+|Y|}
\]</span> 其中 <span class="math inline">\(|X\bigcap Y|\)</span>
代表；<span class="math inline">\(|X|\)</span> 和 <span
class="math inline">\(|Y|\)</span> 分别表示 X 和 Y 的元素個數。
其中，分子中的系數為 2，是因为分母重複計算了 X 和 Y
之間的共同元素的原因，Dice Coefficient 值越大代表兩集合越相似</p>
<p>而如果我們要表示成 Loss
勢比要「越小越好」，有兩種做法，一、直接加負號，二、1 - Dice
Coefficient，第一種做法會是負的 Loss
看起來很怪，因此比較人使用第二種，同時值也會落在 0~1 之間，也就是：
<span class="math display">\[
d=1-\frac{2|X\bigcap Y|}{|X|+|Y|}
\]</span> 為什麼要使用 Dice Loss？Dice Loss
尤其應用在分割任務上特別多，為什麼不使用一般的 Cross-entropy
就好了呢？原提出論文 V-Net
作者給了一個解釋：在醫學影像中分割目標通常都極小一塊，例如腫瘤，這個特性造成網路訓練資料正負樣本不均，使得既使網路全猜負樣本也會有非常高的正確率。而由於
Cross-entropy 是「每一個像素都會參與計算」，去算出所有像素的 Loss
總合，加大了正負樣本不均的問題。作者提出的 Dice Loss
由於只會與「目標集合」做運算，可以省下許多與負樣本的計算誤差，改善正負樣不均的問題。</p>
<p>但是因 Dice Loss 的 Backpropagation
式子較為複雜，原式子與其一次微分：其中 p 為預測輸出、t 為 GT 輸出 <span
class="math display">\[
f&#39;(\frac{2pt}{p^2+t^2})dp\rightarrow \frac{2t^2}{(p+t)^2}
\]</span> 當在極端狀態下，當 p 與 t 都超小時，Loss 無限大，相較於
Cross-entropy 一次微分做 Backpropagation，Dice Loss
不太好訓練，這會使得網路不好收斂。所這 UNETR
這篇作者採用兩個都來的做法</p>
<h2 id="實驗結果">實驗結果</h2>
<p>以下簡單貼一些實驗結果：</p>
<p>BTCV 醫學資料集上的結果</p>
<p><img src="https://i.imgur.com/5QY087k.png"
alt="image-20220704013205692" /></p>
<p>MSD dataset 上的結果</p>
<p><img src="https://i.imgur.com/4G8Q33R.png"
alt="image-20220704013244526" /></p>
<p>最終效果視覺圖：</p>
<p><img src="https://i.imgur.com/kns3IEf.png" alt="image-20220704013310139" style="zoom:50%;" /></p>
<p>一些 Ablation 實驗，作者倒是有特別強調他們的 Inference Time
特別小</p>
<p><img src="https://i.imgur.com/3gSBLfq.png"
alt="image-20220704013440354" /></p>
<h2 id="結論">結論</h2>
<p>這篇特別之處在二：一、直接使用 volumetric 當作網路 input；二、使用
Transformer 模仿
U-Net，如果真的照作者說的：在參量數量運算量上升的情況下，Inference Time
依舊低是真的話，那這篇論文可以參考一下</p>
<h2 id="reference">Reference</h2>
<h3 id="csdn-筆記">CSDN 筆記</h3>
<p><a
href="https://blog.csdn.net/weixin_49627776/article/details/123831261">[深度学习论文笔记]UNETR:
Transformers for 3D Medical Image Segmentation</a></p>
<p><a
href="https://blog.csdn.net/qq_38296005/article/details/119830386">Transformer论文阅读（三）：UNETR:
Transformers for 3D Medical Image Segmentation</a></p>
<h3 id="dice-loss">Dice Loss</h3>
<p><a
href="https://blog.csdn.net/longshaonihaoa/article/details/111824916">图像分割中的Dice
Loss</a></p>
<p><a href="https://www.aiuai.cn/aifarm1159.html">医学图像分割之 Dice
Loss (大推詳細！)</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/362935363">01.医学影像分割LOSS</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>3D image</tag>
        <tag>Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title>深度學習 3D 影像速讀</title>
    <url>/2022/07/07/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-3D-%E5%BD%B1%E5%83%8F%E9%80%9F%E8%AE%80/</url>
    <content><![CDATA[<p>實驗室未來有可能要做 3D 的影像處理，來寫一篇筆記記錄一下我從 0
開始學什麼是 3D 影像</p>
<p>keywords: point cloud、voxel <span id="more"></span></p>
<h2 id="怎麼得到-3d-的影像">怎麼得到 3D 的影像？</h2>
<p>一種方式是模仿人類使用「立體視覺法」，利用兩個不同角度的攝影機去對同一個物體拍攝，就可以利用同一個點不同位置的資訊去建構出
3D 的立體影像。</p>
<p><img src="https://i.imgur.com/dPoZfBs.png" alt="image-20220704201640180" style="zoom:50%;" /></p>
<p>而另一種則是使用 TOF 「飛行時間法」，最有名的例子是 RGB-D
格式，每一個影像上的像素都會額外在新增一維「深度維」，利用計算雷射光來回的時間差就可得知，微軟的
Kincet 是最有名的攝影機</p>
<p><img src="https://i.imgur.com/kN7t2Wv.png" alt="image-20220704201938901" style="zoom:50%;" /></p>
<p>第三種是雷射雷達 LiDAR，與上面的 TOF 原理類似，只不過 LiDAR
能往同心圓四面八方發射，且發射的距離可遠的多，與之對應的儲存格式是 point
cloud</p>
<p><img src="https://i.imgur.com/r5WJTGa.png" alt="image-20220704201955493" style="zoom:50%;" /></p>
<h2 id="怎麼在電腦中表示">怎麼在電腦中表示？</h2>
<p>我們有了許多 3D
影像的各種資訊，我們怎麼統一表示這些資訊，或是有什麼格式可以遵循嗎？</p>
<p>以下格式由左至右是：point cloud 點雲、voxel 體素、mesh
三角多邊型網格、multi-view 多視角集合</p>
<p><img src="https://i.imgur.com/TppWMka.png"
alt="image-20220704202332900" /></p>
<ol type="a">
<li><p>所謂 point cloud 多半是指從 LiDAR
收集而來的影像資料，它是由一個個互相「獨立」的點所構成，每一個都會包含很多資訊：RGB
顏色、深度、來回時間…，而 point cloud
的優點為：資料不太需要二次處理，即收集即能用，且表示出的 3D
影像較不失真；而 point cloud 的缺點也與好是它的反面：point cloud
大多是「無序」的，也可看成它是一個集合，這個集合中的點相互交換對網路的輸出結果應該要是不會變的，同時因它沒有「座標表示」，現有的
CNN 架構無法直接使用上</p></li>
<li><p>voxel 體素一詞是由 pixel 像素變化而來，特指 3D 上的 pixel
影像，也有人稱這種型式叫 2.5D。voxel
也想成由需多二維切片影像，一個疊一個，疊出一個三維的表示，voxel
最常應用在醫學的斷層掃描上。voxel 的優點是有座標系統，可以直接使用現成的
CNN 模型來達成；缺點是：需要影像二次處理，point cloud 影像需要經
Occupancy Grid Map (占據網格網路) 轉換為 Voxel
(詳細方法可參考以下文章：<a
href="https://zhuanlan.zhihu.com/p/21738718">占据栅格地图（Occupancy
Grid Map）知乎</a>，且因有座標系所有存在失真的問題</p></li>
<li><p>mesh 多邊型，常常應用在 3D
圖學上，多用於建模，而常見的處理方式可以利用 GNN Graph 的方式去處理
(這個我比較不清楚，就不多細說了)</p></li>
<li><p>multi-view，則是我們放置了許許多多的攝影機去拍攝同一物體，我們期望藉由影像相互之間的關系，去建構出
3D 關系圖</p></li>
</ol>
<h2 id="發展歷史">發展歷史</h2>
<h3 id="voxnet">VoxNet</h3>
<p>2015 年 由 Daniel Maturana 提出 VoxNet 來解決 voxel
格式的深度學習辨識，他們的作法也很直覺暴力。先把 point cloud 經
occupancy grid 做二次處理得到 voxel 表示，再經過許多的 3D Conv
提取特徵，最後得到結果</p>
<p><img src="https://i.imgur.com/hnROSsl.png" alt="image-20220704203925739" style="zoom:50%;" /></p>
<p>這個 3D Conv
之所以可行，是因為這也只是維度上的問題而已，反正只要能確保兩矩陣乘法最後乘出來的維度是相同的就可以了</p>
<h3 id="mvcnn">MVCNN</h3>
<p>緊接著也在 2015 年發了 Multi-view Convolutional Neural Networks for
3D Shape Recognition 這篇論文，提出 MVCNN 架構，這裡則是使用 Multi-view
的角度去解決 3D 影像問題，其中 CNN 也是用 3D CNN</p>
<p><img src="https://i.imgur.com/czm9ywc.png" alt="image-20220704204431276" style="zoom:50%;" /></p>
<h3 id="pointnet-系列">PointNet 系列</h3>
<p>在 2016 年提出 PointNet 正式開起了直接使用 PointNet
的網路架構，而在這之後，需多的論文也是從 point cloud
的角度作為出發點改進…</p>
<h3 id="醫學斷層掃描系列">醫學斷層掃描系列</h3>
<p>MRI 核磁共震，就是 voxel
影像的最佳代表，一個完全不用二次處理原汁原味的 voxel
影像，我有看到幾篇相關論文，列舉在這邊：2020 Satya P. Singh 提出 <a
href="https://arxiv.org/pdf/2004.00218.pdf">3D Deep Learning on Medical
Images: A Review</a> 、2020 Hasib Zunair提出 <a
href="https://arxiv.org/pdf/2007.13224.pdf">Uniformizing Techniques to
Process CT scans with 3D CNNs for Tuberculosis Prediction</a> 、2018
EMAN AHMED 提出 <a href="https://arxiv.org/pdf/1808.01462.pdf">A survey
on Deep Learning Advances on Different 3D Data Representations</a></p>
<p>上述這些論文的共同特色就是「魔改 CNN」，一路把
VGG、ResNet、Inception、DenseNet... 把裡面全部的 2D Conv 全換為 3D Conv
就完事了</p>
<h2 id="reference">Reference</h2>
<h3 id="d-影像導論">3D 影像導論</h3>
<p><a
href="https://blog.csdn.net/weixin_40805392/article/details/98729367">point_cloud_segmentation的发展过程
(csdn)</a></p>
<p><a
href="https://thegradient.pub/beyond-the-pixel-plane-sensing-and-learning-in-3d/">3D
影像歷史介紹 (英文，大推)</a></p>
<p><a
href="https://www.jiqizhixin.com/articles/091203">上面那篇的中文翻釋
(照抄…)</a></p>
<p><a
href="https://cxyzjd.com/article/xiaoyaolangwj/113572662">3D点云基础知识(一)</a></p>
<h3 id="論文集合">論文集合</h3>
<p><a
href="https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf">VoxNet</a></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>3D image</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 vscode 遠端 docker 開發使用教學</title>
    <url>/2022/07/07/%E4%BD%BF%E7%94%A8-vscode-%E9%81%A0%E7%AB%AF-docker-%E9%96%8B%E7%99%BC%E4%BD%BF%E7%94%A8%E6%95%99%E5%AD%B8/</url>
    <content><![CDATA[<p>vscode
真是勘稱一代開發神器，上面一大堆好用套件，不僅讓你的開發環境變得美美的，同時還提供了相當便捷的功能。以下是我這兩三年來，在實驗室進行遠端開發，所記錄下來的一些使用心得。</p>
<p>keywords: vscode <span id="more"></span></p>
<p>對於新加入 vscode
的人最大的困難就在於熟悉介面，光這一步就不知道勸退多少人了，即然大家不想學習如何使用介面…那我們就先來步置介面吧
XD，把介面弄的美美的眼睛看起來非常舒服，自然就會有想使用 vscode 的動機啦
XD</p>
<h2 id="美美的套件-事前準備">美美的套件 (事前準備)</h2>
<p>我的 vscode 美美之路受以下文章的起萌：<a
href="https://blog.goodjack.tw/2018/03/visual-studio-code-extensions.html">小克的
Visual Studio Code
必裝擴充套件（Extensions）私藏推薦</a>，大家可以點進去多看看，裡面有超多推薦的套件，其中我會再整理出幾個「一定」要裝的套件</p>
<p>One Dark Pro。一個裝下去眼睛就會得到解放的套件 <img
src="https://i.imgur.com/5EFRD1R.png" alt="Image" /></p>
<p>Material Icon Theme。側邊檔案目錄變得清楚明瞭，檔案類型清清楚楚 <img
src="https://i.imgur.com/BijJLgI.png" alt="Image" /></p>
<p>GitLens。有在用 Git 的話必裝，可以方便切換 commit，merge
版本，還自帶全自動 Git blame 讓你簡單找到 bug 戰犯 <img
src="https://i.imgur.com/NlflEAd.png" alt="Image" /></p>
<p>CodeSnap。程式碼截圖神器，就算使用 window 電腦，也可以讓你截出 mac
的味道 <img src="https://i.imgur.com/ytZdWhk.png" alt="Image" /></p>
<p>Path Intellisense。在程式中輸入檔案位置神器，可以讓你用 Tab 輸入目錄
<img src="https://i.imgur.com/FFBYp62.png" alt="Image" /></p>
<h2 id="ssh-遠端工作系列">SSH 遠端工作系列</h2>
<p>以往我們在使用 SSH 遠端工作時，如果是 mac 使用者可以很方便的用
Terminal 打指令進入，如果是 windows 使用者也可以用 mobaXTerm
作為替代。但是如果遇到需要修改程式碼，又不想用 vi、nano 來開起時，這時
vscode Remote 就是一個很好的選擇了</p>
<p>首先先安裝 <img src="https://i.imgur.com/chTKjOK.png"
alt="Image" /></p>
<p>完成後左邊選單就會多一個螢幕的圖示，這個就是 SSH 連線的地方 <img
src="https://i.imgur.com/ZFv3Hhr.png" alt="Image" /></p>
<p>點進 SSH 圖示後，選擇上方齒輪，進入設定 <img
src="https://i.imgur.com/jK0oRxR.png" alt="Image" /></p>
<p>選擇在使用者/Users 底下的 .ssh/config 檔案，vscode
會自動在電腦對應的位置新增空白檔案，以後所有連線的設定都會存在這裡面。下面講解各個欄位用途
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Host server_自定名稱</span><br><span class="line">	HostName IP_位置</span><br><span class="line">	User 使用者名稱</span><br><span class="line">	Port 埠號</span><br><span class="line">	IdentityFile RSA 非對稱式私鑰位置</span><br></pre></td></tr></table></figure></p>
<p>設定好後就會在左邊出現自定名稱的電腦圖示，按下右邊的「加資料夾」圖示後，就可以直接用
vscode 連 ssh 啦 <img src="https://i.imgur.com/ZQDVhvy.png"
alt="Image" /></p>
<h2 id="docker">Docker</h2>
<p>很多時候我們連線到遠端，還會在遠端 server 再建立 docker
虛擬環境，很多時候會需要下到 docker 指令，但是…如果說有 GUI
可以操作呢…？</p>
<p>vscode 可以在遠端 server 上再安裝套件，這個套件是與本地分開的，專屬於
server。我們先來下載 docker 吧</p>
<p><img src="https://i.imgur.com/jKjX7v8.png" alt="Image" /></p>
<p>左手邊就會出現小鯨魚的圖案</p>
<p><img src="https://i.imgur.com/3a8suR4.png" alt="Image" /></p>
<p>按下去之後就可以看到 server 各種
image、container、volume...，非常的視覺化 <img
src="https://i.imgur.com/tuGdJp0.png" alt="Image" /></p>
<p>對任何 image 可以按右鍵把它 run 起來；對任何的 container
也可以按右鍵進到 bash 裡面</p>
<p><img src="https://i.imgur.com/tkgVDXw.png" alt="Image" /></p>
<p>這個時候 vscode 下方的 terminal 就會更改成 docker 虛擬環境裡的
terminal 囉！</p>
<h2 id="remote---container">Remote - Container</h2>
<p>有時在開發的時候會發生以下的問題：</p>
<p><img src="https://i.imgur.com/xpsA9vL.png" alt="Image" /></p>
<p>vscode 沒有讀到虛擬機裡的 python 環境位置，導致不管我們在 docker
裡面裝了什麼套件，vscode 都不會知道，這個問題會導致 python
套件無法給你提示，例如打 torch 後面會很多很多其它 function 之類的</p>
<p>為了要解決這個問題，我們需要再安裝一個套件：Remote - Container</p>
<p><img src="https://i.imgur.com/APw02LC.png" alt="Image" /></p>
<p>安裝完後再按 ssh
的電腦圖示會發現，最上面多了一個下拉式選單，裡面有兩個選項：ssh
舊的連線，以及 containers 新的連線</p>
<p><img src="https://i.imgur.com/laq1Qwm.png" alt="Image" /></p>
<p>這個 container 的作用類似 ssh，有點像是「用 ssh 的方式連線進 docker
的環境中」，所以以我們的這個例子，如果我們要到遠端的 docker
開發，我們先要：一、連線到遠端，二、連線到 docker，兩層的連線 XD</p>
<p>如果遠端已經建立好的 container 時，選 containers
下面會多出許多可連線的 server 選項，這些選項都是 docker container
(也是這個套件名字的由來嘛)，在對應的 container 下按「加分頁圖示」</p>
<p><img src="https://i.imgur.com/VLkKv28.png" alt="Image" /></p>
<p>它會安裝一些東西，這個步驟可能要跑一陣子，我自己的經驗大約是 3 ~ 5
分鐘，跑完之後就成功的進到了 docker container 的環境中啦！在這裡面所有的
python 套件都正常運作，自然也就不會有黃黃底線的問題了！</p>
]]></content>
      <categories>
        <category>雜開發心得</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>remote ssh</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title>Global-Local Temporal Representations For Video Person Re-Identification - 融合 Dilated 與 Self-Attention 的空間時間注意力</title>
    <url>/2022/07/18/Global-Local-Temporal-Representations-For-Video-Person-Re-Identification-%E8%9E%8D%E5%90%88-Dilated-%E8%88%87-Self-Attention-%E7%9A%84%E7%A9%BA%E9%96%93%E6%99%82%E9%96%93%E6%B3%A8%E6%84%8F%E5%8A%9B/</url>
    <content><![CDATA[<p>本篇論文目標同樣為：在一影片序列中，找出不同幀影像間的注意力，來區分重要與不重要的特徵幀，藉此來強化網路的結果</p>
<p><a
href="https://arxiv.org/abs/1908.10049">https://arxiv.org/abs/1908.10049</a></p>
<p>keywords: Dilated Convolution、Self-Attention、GLTR <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p>作者認為在一個影片序列中有兩個重要特徵：short-term temporal
短期的關連性，目標在相鄰幀中找到相似的行人；long-term temporal
長期的關連性，目標在兩較遠的幀中找出關連，使得可以解決行人遮擋或影片雜訊等問題</p>
<p>為了達成上述目標，作者提出了 Global-Local Temporal Representation
(GLTR) 架構，其中包含了兩個子架構，Dilated Temporal Pyramid (DTP)
架構來達成 short-term temporal 短期的關連性； Temporal Self-Attention
(TSA) 架構來達成 long-term temporal 長期的關連性，藉著結合：Dilated Conv
以及 Self-Attention 作者在結果上取得了不錯的成績 (MARS 87.02% Rank-1
Accuracy)</p>
<p>作者提到在這之前有人使用 3D Conv
的方法來解決影片資料的問題，作者認為這樣子的方法有幾個缺點：運算時間大、沒有很有邏輯的去分析空間中的注意力</p>
<h2 id="網路架構">網路架構</h2>
<h3 id="backbone">Backbone</h3>
<p>作者使用 ResNet-50 作為主架構，先將影片拆分出所有的幀，將二維的影像
<span class="math inline">\(H\times W\times d\)</span>
先經骨幹網路學習，再把結果 reshape 成類似三維的 <span
class="math inline">\(H\times W\times d\times T\)</span></p>
<p><img src="https://i.imgur.com/kII3QqO.png"
alt="image-20220718143905285" /></p>
<p>詳細的做法為：一幀影像大小為 <span class="math inline">\(H\times
W\times d\)</span> ，一共有 N 個幀 <span class="math inline">\(N \times
H\times W\times d\)</span> ，再加上 Batch，最後再 reshape 一下得到 <span
class="math inline">\(BN\times D\times H\times W\)</span>
的輸入表示，這個維度可以理解為把 Batch 與幀數視為相同一個維度，Batch
假設是 10，影片假設有 10 幀，則一次放進網路的二維影像總數就是 10x10 =
100 張。最後再把維度 reshape 回原 <span class="math inline">\(B\times
N\times D\times H\times W\)</span>。利用這個方法就不會因多一個維度需要
3D Conv 了。</p>
<h3 id="dilated-temporal-pyramid-convolution">Dilated Temporal Pyramid
Convolution</h3>
<p>作者在架構中引入 Dilated Convolution 擴張卷積，利用其 rate <span
class="math inline">\(r\)</span>
的特色，可以在不改變解釋度、不增加運算量的前提下，增加網路的 receptive
field 視野。當 <span class="math inline">\(r\)</span>
越大除了可看為視野越大外也可理解為「兩相鄰幀時間隔離變遠」，越大越
long-term</p>
<p>同時也引入了 FPN 金字塔網路的概念，設計不同的 rate 最後用 concat
融合在一起，也就是把 short-term 與 long-term
合併特徵，使得網路有更豐富的資訊</p>
<h3 id="temporal-self-attention">Temporal Self Attention</h3>
<p>將剛剛得到不同視野 (時間長短) 的特徵圖做
Self-Attention，作者設計的很剛好，FPN 的金字塔層數是 3 剛好對應
Self-Attention 要切成 QKV 三份。三個不同視野 (時間長短)
的特徵圖彼此做重要度分析，最後經一 average pooling 得到最後的結果</p>
<h2 id="實驗結果">實驗結果</h2>
<p>DTP TSA 的一些 Ablation study，可發現兩個都加上效果最好</p>
<p><img src="https://i.imgur.com/maZMfUA.png"
alt="image-20220718212848428" /></p>
<p>SOTA 表，其中 STA 為上一篇的論文架構名稱</p>
<p><img src="https://i.imgur.com/FLP2FaR.png"
alt="image-20220718213016057" /></p>
]]></content>
      <tags>
        <tag>3D image</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>STA: Spatial-Temporal Attention for Large-Scale Video-based Person Re-Identification 影片空間時間注意力</title>
    <url>/2022/07/18/STA-Spatial-Temporal-Attention-for-Large-Scale-Video-based-Person-Re-Identification-%E5%BD%B1%E7%89%87%E7%A9%BA%E9%96%93%E6%99%82%E9%96%93%E6%B3%A8%E6%84%8F%E5%8A%9B/</url>
    <content><![CDATA[<p>本篇論文雖然目的是在做 Re-ID
任務，但是「手勢辨識」「影片中的行人辨識」，這類任務與我現在的題目都有相似之處：輸入資料並非單純的二維影像。要怎麼利用多一個「時間、空間」維來完成任務，是這些題目所要解決的重點</p>
<p><a
href="https://arxiv.org/abs/1811.04129">https://arxiv.org/abs/1811.04129</a></p>
<p>keywords: Re-ID、Spatial-Temporal Attention (STA) <span id="more"></span></p>
<p>Person re-identification (Re-ID)
行人辨識，所要解決的任務是在一個影片做行人偵測，這個任務最困難的地方在輸入不是一張影像，而是由許多幀數構成的影片。</p>
<p>這種帶有時間維度的資料，在處理上有幾個困難的點：</p>
<ol type="1">
<li>最直接的做法是把影片每一幀當成影像放進網路訊練，最後用一個
maxpooling 統整所有結果。缺點是遇到 occlusions 遮擋時效果會很不好</li>
<li>maxpooling
還有一個缺點是，會破壞影片時間軸的資訊，之後有任務在模型中加入 attention
區別各個幀的重要性，但是本篇作者認為，現有的 attention
方法都沒辨很好的做到：找到最重要的那一幀，以及大部份 attention
因全連接層的關系輸入長度是要固定的。</li>
</ol>
<p>作者基於上面兩個理由提出 <strong>Spatial-Temporal Attention
(STA)</strong> 空間-時間注意力機制</p>
<p><img src="https://i.imgur.com/lKF4lp7.png"
alt="image-20220717151834910" /></p>
<h2 id="網路架構">網路架構</h2>
<h3 id="backbone">Backbone</h3>
<p>作者使用 ResNet-50 作為網路主幹，小小修改的地方在把最後一層的 average
pooling 及全連接層去掉，改接入到作者自己設計的 STA 中</p>
<h3 id="spatial-temporal-attention-model">Spatial-Temporal Attention
Model</h3>
<p>作者認為：現有的 Attention 機制有以下三個問題</p>
<ol type="1">
<li>因經過更多的 Conv 層，代表著更多的計算</li>
<li>不同空間中的 attention
彼此是互相獨立的，這會使得前景的目標行人網路關注的地方並非完整的人，而是零散的區域</li>
<li>空間、時間注意力也是彼此獨立的，權重不共享</li>
</ol>
<p>因此設計出 Spatial-Temporal Attention (STA) 架構，流程如下：</p>
<p>首先一影片 <span class="math inline">\(V\)</span> 由許多幀 <span
class="math inline">\(I\)</span> 構成，<span
class="math inline">\(V={I_1,...,I_N}\)</span>，會先在其中隨機取 n
個幀做運算得出輸入 <span class="math inline">\(f_n\)</span> ，對 <span
class="math inline">\(f_n\)</span> 做 <span
class="math inline">\(l_2\)</span>
正規化，再用平方和來除以它。作者利用平方和的 <span
class="math inline">\(l_2\)</span> 對選出的 n 個幀做空間注意力。 <span
class="math display">\[
g_n(h,w)=\frac{||\sum^{d=D}_{d=1}f_n(h,w,d)^2||_\mathbb{2}}{\sum^{H,W}_{h,w}||\sum^{d=D}_{d=1}f_n(h,w,d)^2||_\mathbb{2}}
\]</span> 對每個幀做完 <span class="math inline">\(l_2\)</span>
後，再將每個幀水平的切成 <span class="math inline">\(K\)</span>
個相同大小的塊 <span class="math display">\[
\begin{aligned}
g_n = [g_{n,1},...,g_{n,K}]\\
f_n = [f_{n,1},...,f_{n,K}]
\end{aligned}
\]</span> 接著對「每一塊」都做一次 <span
class="math inline">\(l_1\)</span>
正規化，作者說這樣可以達到區塊中的空間注意力 <span
class="math display">\[
s_{n,k}=\sum_{i,j}||g_{n,k}(i,j)||_\mathbb{1}
\]</span> 計算完一幀內的「全局」「局部」空間注意力後，再合併剛剛的 n
幀，對所有 n 個空間注意力結果再做 <span
class="math inline">\(l_1\)</span> 而非複雜的 Conv
層，作者說得到的結果可看成時間注意力分數 <span class="math display">\[
S(n,k)=\frac{s_{n,k}}{\sum_n||s_{n,k}||_\mathbb{1}}
\]</span> 最後我們就可以得到一個二維 <span class="math inline">\(n\times
k\)</span> 的注意力矩陣，n -&gt; 幀數、k -&gt; 塊數</p>
<p><img src="https://i.imgur.com/vNdPVZs.png"
alt="image-20220718001928912" /></p>
<h3 id="inter-frame-regularization">Inter-Frame Regularization</h3>
<p>作者為了避免網路過度依賴單一區塊的權重，設計了 Inter-Frame
Regularization 來正規化彼此差異</p>
<p>作者會在做完第一次 <span class="math inline">\(l_2\)</span>
得到幀的空間注意力後，隨機選出兩個幀，彼此做 Frobenius
Norm，公式如下：(我個人覺得就是 pixel-wise 的像素平方差而已…) <span
class="math display">\[
\begin{align}
Reg&amp;=||g_i=g_j||_F\\
&amp;=\sqrt{\sum^H_{h=1}\sum^W_{w=1}|g_i(h,w)-g_j(h,w)|^2}
\end{align}
\]</span> 為了不要兩幀差異太大，所以這個 Reg 值越小越好，作者並且加到
Loss 裡面變成：(<span class="math inline">\(\lambda\)</span>
為控制比例超參數) <span class="math display">\[
\min(\mathcal{L}_{total}+\lambda Reg)
\]</span></p>
<h3 id="feature-fusion-合併方法">Feature Fusion 合併方法</h3>
<p>做完上述 Attention 的得到一個 <span class="math inline">\(n\times
k\)</span> 的 <span class="math inline">\(s_{n,k}\)</span>
分數(注意力)矩陣，先會把特徵圖也分為 K 塊，接著做兩種不同的合併方法</p>
<ol type="1">
<li>對所有幀中的塊，直接選擇分數最高的那一塊。例如 n=4 ，我們要在 k
中選一個最大的值為結果，一共會選 4 次，稱為 Pick max index</li>
<li>對每一個幀、每一個塊做 element-wise
的乘法，把分數加權在特徵圖中，稱為 Weighted sum</li>
<li>最後把 Pick max index 、 Weighted sum 兩矩陣 concat 起來</li>
</ol>
<p>最後就是一連串的：GAP -&gt; FC -&gt; FC -&gt; 分類…</p>
<h3 id="loss">Loss</h3>
<p>作者使用 triplet loss + softmax 的混合 loss</p>
<p>triplet loss 更詳細的介紹可以參考下面網站：<a
href="https://zhuanlan.zhihu.com/p/171627918">triplet loss
损失函数</a></p>
<h2 id="實驗結果">實驗結果</h2>
<p>作者有做了一些 Ablation
實驗來證實他們的方法是有用的，先來看修改效果圖</p>
<p><img src="https://i.imgur.com/EqYKmMG.png"
alt="image-20220718005006372" /></p>
<p>對於隨機在影片中選 N 幀的實驗</p>
<p><img src="https://i.imgur.com/UtHPsk4.png"
alt="image-20220718005041227" /></p>
<p>對於一幀中切 K 個塊的實驗，作者發現切太多塊反而不好</p>
<p><img src="https://i.imgur.com/3LWUCn7.png"
alt="image-20220718005050352" /></p>
]]></content>
      <categories>
        <category>電腦視覺整理</category>
      </categories>
      <tags>
        <tag>3D image</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Marp 踩坑心得</title>
    <url>/2022/12/27/Marp-%E8%B8%A9%E5%9D%91%E5%BF%83%E5%BE%97/</url>
    <content><![CDATA[<p>還在為著 PPT
要調圖片調一整天而困擾嗎？還在為著不知道要怎麼排版比較好嗎？推薦
Marp，它超好用的！</p>
<p>keywords: Marp <span id="more"></span></p>
<h2 id="安裝">安裝</h2>
<ul>
<li>在 vscode 上面下載 Marp for VS Code 套件</li>
<li><img src="https://i.imgur.com/PfQWrZg.png"
alt="image-20221004210811691" /></li>
</ul>
<h2 id="自定義-theme">自定義 Theme</h2>
<ul>
<li><p>下載別人寫好的 css 檔案</p></li>
<li><p>將目標投影片的 .md 檔與 css 檔案放在同一個資料夾底下</p></li>
<li><p>並使用 vscode 開起對應資料夾</p></li>
<li><p>按下 <code>ctrl + ,</code>
進入設定，在搜尋列輸入：<code>marp: theme</code></p></li>
<li><p>按下新增項目，輸入 css 的檔案路徑 (記得要加 ./)</p></li>
<li><p><img src="https://i.imgur.com/Hglmg6p.png"
alt="image-20221004211013113" /></p></li>
<li><p>享受別人的成果吧！</p></li>
<li><div class="sourceCode" id="cb1"><pre
class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="at">  ---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">marp</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">theme</span><span class="kw">:</span><span class="at"> olive</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="at">  ---</span></span></code></pre></div></li>
</ul>
]]></content>
      <categories>
        <category>雜開發心得</category>
      </categories>
      <tags>
        <tag>Marp</tag>
      </tags>
  </entry>
  <entry>
    <title>什麼是 python Pass by Assignment？</title>
    <url>/2022/12/27/%E4%BB%80%E9%BA%BC%E6%98%AF-python-Pass-by-Assignment%EF%BC%9F/</url>
    <content><![CDATA[<p>這幾天在刷題寫 leetcode，用最方便的 python 語言來刷，在寫到 <a
href="https://leetcode.com/problems/combination-sum/">39. Combination
Sum</a> 這題的時候，為了要滿足遞迴的條件，所以在遞迴函式中加入當前答案
<code>res=list()</code> 當做參數，結果發現…這個 List
面試的數值怎麼一直變來變去啦…明明我沒有去動到它的阿…</p>
<p>keywords: Pass by Assignment <span id="more"></span></p>
<p>以下是犯人 code：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">List</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.ans = <span class="built_in">list</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combinationSum</span>(<span class="params">self, candidates: <span class="type">List</span>[<span class="built_in">int</span>], target: <span class="built_in">int</span></span>) -&gt; <span class="type">List</span>[<span class="type">List</span>[<span class="built_in">int</span>]]:</span></span><br><span class="line">        </span><br><span class="line">        res = <span class="built_in">list</span>()</span><br><span class="line">        res.append(candidates[<span class="number">0</span>])</span><br><span class="line">        self.findSum(res, candidates, target)</span><br><span class="line"></span><br><span class="line">        candidates = candidates[<span class="number">1</span>:]</span><br><span class="line">        res.pop()</span><br><span class="line">        res.append(candidates[<span class="number">0</span>])</span><br><span class="line">        self.findSum(res, candidates, target)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.ans</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findSum</span>(<span class="params">self, res, candidates, target</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> candidates:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(res) == target:</span><br><span class="line">            self.ans.append(res)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">sum</span>(res) &gt; target:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        res.append(candidates[<span class="number">0</span>])</span><br><span class="line">        self.findSum(res, candidates, target)</span><br><span class="line"></span><br><span class="line">        candidates = candidates[<span class="number">1</span>:]</span><br><span class="line">        res.pop()</span><br><span class="line">        res.append(candidates[<span class="number">0</span>])</span><br><span class="line">        self.findSum(res, candidates, target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span></span><br></pre></td></tr></table></figure>
<p>發生事情當下我心想：該不會是發生了 call by reference
吧…，所以特別花了一點時間來研究一下，python 中傳值的方式倒底是怎麼傳</p>
<h3 id="call-by-value-vs-call-by-reference">Call by Value vs Call by
Reference</h3>
<p>在開始講 python 是怎麼傳值前，我們要先來了解在 python
中是如何定義資料型態的，以下的例子我會舉 JS 當成是另一個語言來做比較</p>
<p>在 JS (以及一部份語言) 中所有的資料型態分為兩種：primitive type 以及
object type</p>
<p>所謂的 primitive type (中文翻原始型別)，在 JS 中有以下類別：</p>
<ul>
<li>String</li>
<li>Number (int, float, ...)</li>
<li>Boolean</li>
<li>Null</li>
<li>Undefined</li>
</ul>
<p>而大部份你看得到的「其它」類別，都是 object type，也就是都是由一個
object 定義出來的</p>
<ul>
<li>Object</li>
<li>Function</li>
<li>Array</li>
<li>Set</li>
</ul>
<p>而在 JS 中：primitive type 全部都是 call by
value，也就是傳進函式前，complier
會先幫你在記憶體中新增一塊不同位置，但是值相同</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">call_by_value</span>(<span class="params">x</span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(x);    <span class="comment">// 5 </span></span><br><span class="line">  x = <span class="number">1</span>;</span><br><span class="line">  <span class="built_in">console</span>.log(x);    <span class="comment">// 1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">let</span> x = <span class="number">5</span>;</span><br><span class="line"><span class="built_in">console</span>.log(x);      <span class="comment">// 5 (進入函數前)</span></span><br><span class="line">call_by_value(x);</span><br><span class="line"><span class="built_in">console</span>.log(x);      <span class="comment">// 5 (雖然進入函數後 x 有更動到，但因在 function 內是其它記憶體，所以外部值沒變)</span></span><br></pre></td></tr></table></figure>
<p>其它的 Object type 則是 call by
reference，也就是不管是在函式內外，變數指向的記憶體位置都是一樣的，所以在
call_by_reference() 函式內的改動，會影響到函式外的變數：</p>
<figure class="highlight js"><table><tr><td class="code"><pre><span class="line"><span class="keyword">let</span> person = &#123;</span><br><span class="line">  <span class="attr">name</span>: <span class="string">&#x27;John&#x27;</span>,</span><br><span class="line">  <span class="attr">age</span>: <span class="number">25</span>,</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">call_by_reference</span>(<span class="params">obj</span>) </span>&#123;</span><br><span class="line">  obj.age += <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">console</span>.log(preson);    <span class="comment">// &#123; name: &#x27;John&#x27;, age: 25 &#125;</span></span><br><span class="line">increaseAge(person);</span><br><span class="line"><span class="built_in">console</span>.log(person);    <span class="comment">// &#123; name: &#x27;John&#x27;, age: 26 &#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="mutable-vs-immutable">Mutable vs Immutable</h3>
<p>而在 python 中比較不一樣的是，python 中所有的型別都是一個物件
(object)，每一個資料都會有一個 __class__ 屬性來看看是由哪一個 class
所生成的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(x.__class__)   <span class="comment"># &lt;class &#x27;int&#x27;&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(x))       <span class="comment"># &lt;class &#x27;int&#x27;&gt;     // 與上面的程式等價</span></span><br></pre></td></tr></table></figure>
<p>同時也可以使用 id 來看看這個變數是放在記憶中哪一個位置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(x))         <span class="comment"># 4342041840</span></span><br></pre></td></tr></table></figure>
<p>因為每一個型別都是一個物件，所以不會像其它語言的分法一樣，在 python
中是透過是不是 mutable 來區分的。根據網路上別人留言的定義 mutable
的意思是</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// a mutable object is an object that can be changed</span><br><span class="line">// while an immutable object can&#x27;t be changed</span><br></pre></td></tr></table></figure>
<p>下表是 python 中常見型別是不是 mutable/immutable 的表格。<a
href="https://medium.com/@meghamohan/mutable-and-immutable-side-of-python-c2145cf72747">圖片參考自
Mutable vs Immutable Objects in Python</a></p>
<p><img src="https://i.imgur.com/UPYZbbs.png" alt="image-20221206110751263" style="zoom:50%;" /></p>
<p>接下來詳細介紹這兩個的差別：</p>
<p>immutable 物件一旦被創造出來，它的值就永遠不可以再更改，像是
int、float、string、bool、<strong>tuple</strong>。用下面簡單的例子來舉例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(x))    <span class="comment"># 4334161232</span></span><br><span class="line"></span><br><span class="line">x = <span class="number">200</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(x))    <span class="comment"># 4334161264</span></span><br></pre></td></tr></table></figure>
<p>可以發現因為 x 是 int 型別，是屬於 immutable，一旦值發生改變 python
是直接會再找一塊新的記憶體來存 x 變數，而非修改原本記憶體的值</p>
<p>不知道大家看到這邊的時候有沒有覺得很奇怪，為什麼 python
要這麼沒有效率的一直新增記憶體空間阿？我們不訪試著想想看，如果今天是在 C
中我們重複定義了一個變數會發生什麼事：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> x = <span class="number">100</span>;</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">200</span>;    <span class="comment">// error: redefinition of ‘x’</span></span><br></pre></td></tr></table></figure>
<p>它會噴重複定義的錯，因為 x
所在的記憶體位置已經被使用了，當我們想再定義一次時，complier
會提醒我們不能這麼做。但是有沒有想過，為什麼 python 可以這麼做呢？python
雖然少了型別 (int) 的部份，但還是可以達成下面程式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="number">100</span></span><br><span class="line">x = <span class="number">200</span></span><br></pre></td></tr></table></figure>
<p>其實我們之所以可以在 python 裡面執行這種操作，就是因為每當使用
<code>=</code> 去 assign 一個變數時，python
都會在記憶體中新增一塊位置存放它，也就是說其實這兩個 x
根本是不一樣的東西，而這也是 immutable 的精神所在：值絕對不會被更改</p>
<p>如果用圖片的方式來表達的話，python 中的執行方式，就會如下圖所式：</p>
<p><img src="https://i.imgur.com/Mu0Wonv.png" alt="image-20221227121240682" style="zoom:67%;" /></p>
<p>到這邊就引出這篇文章最重要的想法：這種每當有 assignment
發生時，immutable 型別所指向的記憶體都會改變，這種方法在 <a
href="https://docs.python.org/3/faq/programming.html#how-do-i-write-a-function-with-output-parameters-call-by-reference">python
official document</a> 中稱作：<strong>Pass by Assignment</strong>。</p>
<p>我們現在來看看如果把 Pass by Assignment 的想法加上 function
會發生什麼事，以下範例我們將 immutable 變數當成參數傳進 function：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">x, y</span>):</span></span><br><span class="line">  x = <span class="number">5</span></span><br><span class="line">  y = y + <span class="number">1</span></span><br><span class="line">  <span class="built_in">print</span>(x, y)    <span class="comment"># 4 24</span></span><br><span class="line">  </span><br><span class="line">a = <span class="number">10</span></span><br><span class="line">b = <span class="number">20</span></span><br><span class="line"><span class="built_in">print</span>(a, b)      <span class="comment"># 10 20</span></span><br><span class="line"></span><br><span class="line">fun(a, b)</span><br><span class="line"><span class="built_in">print</span>(a, b)      <span class="comment"># 10 20</span></span><br></pre></td></tr></table></figure>
<p>當 a b immutable 變數傳進 function 時 python
會像其它的語言一樣，新增一塊記憶體並且 copy
變數的值到這個記憶體中，不管在 function 中的任何操作都不會影響到外面 a,
b 變數。</p>
<p>也就是說 python 的 immutable 型別在 Pass by Assignment
中，很像在其它語中稱作的 Call by Value，記憶體操作如下圖表示：</p>
<p><img src="https://i.imgur.com/HMMzAbJ.png" alt="image-20221227121526401" style="zoom:50%;" /></p>
<p>接下來是換 mutable 的部份。mutable
型別的有：list、set、dict，這些型別如同 mutable
的意義一樣：是可以在宣告後修改的，也就是在同一個記憶體位置中修改存的值，我們用以下的範例來看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_list = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">6</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(my_list))     <span class="comment"># 4337133824</span></span><br><span class="line"></span><br><span class="line">my_list[<span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(my_list))     <span class="comment"># 4337133824</span></span><br><span class="line"></span><br><span class="line">my_list.append(<span class="number">900</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(my_list))     <span class="comment"># 4337133824</span></span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/oL5rrrg.png" alt="image-20221227121550340" style="zoom:50%;" /></p>
<p>可以發現不管我們對 my_list 做：修改值、append 等操作，id(my_list)
都是不會變的，而正是 mutable
的主要表現：可以在相同記憶體下修改其中的值</p>
<p>那如果 mutable 型別遇上 function
會發生什麼事呢？看看下面程式的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">x, y</span>):</span></span><br><span class="line">  x.append(<span class="number">30</span>)</span><br><span class="line">  y[<span class="string">&#x27;id&#x27;</span>] = <span class="number">10</span></span><br><span class="line">  <span class="built_in">print</span>(x, y)    <span class="comment"># [10, 20, 30]</span></span><br><span class="line">  							 <span class="comment"># &#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;age&#x27;: 16, &#x27;id&#x27;: 10&#125;</span></span><br><span class="line">  </span><br><span class="line">a = [<span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line">b = &#123;</span><br><span class="line">  <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;John&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;age&#x27;</span>: <span class="number">16</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">print</span>(a, b)      <span class="comment"># [10, 20]</span></span><br><span class="line">								 <span class="comment"># &#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;age&#x27;: 16&#125;</span></span><br><span class="line"></span><br><span class="line">fun(a, b)</span><br><span class="line"><span class="built_in">print</span>(a, b)      <span class="comment"># [10, 20, 30]</span></span><br><span class="line">  							 <span class="comment"># &#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;age&#x27;: 16, &#x27;id&#x27;: 10&#125;</span></span><br></pre></td></tr></table></figure>
<p>可以發現，當 a b 傳到 function 後，當 function 內部的 x y
修改值後，外部的 a b 同時也會一起修改</p>
<p>當 a b mutable 變數傳進 function 時，python 會將 function 內的變數 x
指向 a 所指的記憶體位置，使得在 function 內修改值時，function
外也會同時被修改 (因為就是同一個東西)</p>
<p>也就是說 python 的 mutable 型別在 Pass by Assignment
中，很像在其它語言中稱作的 Call by Reference，記憶體操作如下圖表示：</p>
<p><img src="https://i.imgur.com/gFS3AVQ.png" alt="image-20221227121609941" style="zoom:50%;" /></p>
<p>但是與正常 Call by Reference 不一樣的是，如果我們在 function 內是用
assignment 重新給定一個 mutable 變數值時，python 會像 Call by Value
一樣重新找一塊新的記憶體放，而 function 內外的值互不相影響，如下圖：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span>(<span class="params">x</span>):</span></span><br><span class="line">  x = [<span class="number">0</span>]</span><br><span class="line">  <span class="built_in">print</span>(<span class="built_in">id</span>(x)) <span class="comment"># 4337133860</span></span><br><span class="line">  </span><br><span class="line">a = [<span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line"><span class="built_in">print</span>(a)      <span class="comment"># [10, 20]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))  <span class="comment"># 4337133824</span></span><br><span class="line"></span><br><span class="line">fun(a)</span><br><span class="line"><span class="built_in">print</span>(a)      <span class="comment"># [10, 20]</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(a))  <span class="comment"># 4337133824</span></span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/nQ3pNSh.png" alt="image-20221227121638838" style="zoom:50%;" /></p>
<p>可以發現當 <code>x = [0]</code> 後 python
竟然是重新找一個記憶體去存放，所以當然也不會動到 a 裡面的值，而正是
python Call by Assignment 最要留意的一個點，它並非「完全的」Call by
Reference 喔 ~</p>
<h3 id="reference">Reference</h3>
<p><a
href="https://medium.com/@jobboy0101/js%E5%9F%BA%E7%A4%8E-primitive-type-v-s-object-types-f88f7c16f225">JS基礎：Primitive
type v.s Object types</a></p>
<p><a
href="https://www.maxlist.xyz/2021/01/26/python-immutable-mutable-objects/">(圖片主要參考來源)
[Python 基礎教學] 什麼是 Immutable &amp; Mutable objects</a></p>
<p><a
href="https://luka.tw/Python/%E5%9F%BA%E7%A4%8E%E6%95%99%E5%AD%B8/past/2021-09-21-is-python-call-by-sharing-122a4bf5a956/">(推薦
說得很清楚！)【 Python 教學 】什麼是 Pass By Assignment？</a></p>
<p><a
href="https://docs.python.org/3/faq/programming.html#how-do-i-write-a-function-with-output-parameters-call-by-reference">官方
Call by Assignment Document</a></p>
]]></content>
      <categories>
        <category>雜開發心得</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(一)：從官方 mnist source code 來學習 Pytorch</title>
    <url>/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B8%80-%EF%BC%9A%E5%BE%9E%E5%AE%98%E6%96%B9-mnist-source-code-%E4%BE%86%E5%AD%B8%E7%BF%92-Pytorch/</url>
    <content><![CDATA[<p>Pytorch
已經成為深度學習的主流框架了，以下系列是我從大學時期，一路學習 Pytorch
筆記整理下來的心得，後來再經整理整理決定放到網路上給大家參考，希望可以幫助到更多從
0 開始想自學的人 ~</p>
<p>整個系列會由淺到深，一路從最基本的 SOP，到 Pytorch
底層的實作邏輯，以下先以深度學習界的 Hallo World mnist 來開始學習</p>
<p>keywords: mnist <span id="more"></span></p>
<ul>
<li>首先來看看 mnist 在 pytorch 上實作的程式</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim.lr_scheduler <span class="keyword">import</span> StepLR</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets, transforms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">        self.dropout1 = nn.Dropout(<span class="number">0.25</span>)</span><br><span class="line">        self.dropout2 = nn.Dropout(<span class="number">0.5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">9216</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>)</span><br><span class="line">        x = self.dropout1(x)</span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        x = self.dropout2(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        output = F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model, device, train_loader, optimizer, epoch, log_interval, is_dry_run</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = F.nll_loss(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, batch_idx * <span class="built_in">len</span>(data), <span class="built_in">len</span>(train_loader.dataset),</span><br><span class="line">                <span class="number">100.</span> * batch_idx / <span class="built_in">len</span>(train_loader), loss.item()</span><br><span class="line">            ))</span><br><span class="line">        <span class="keyword">if</span> is_dry_run:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">model, device, test_loader</span>):</span></span><br><span class="line">    <span class="comment"># evaluate mode</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># stop gradient calculation &amp; decrease GPU processing</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            output = model(data)</span><br><span class="line">            test_loss += F.nll_loss(output, target, reduction=<span class="string">&#x27;sum&#x27;</span>).item()</span><br><span class="line">            pred = output.argmax(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss, correct, <span class="built_in">len</span>(test_loader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_loader.dataset)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="comment"># variables</span></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    epochs = <span class="number">14</span></span><br><span class="line">    learning_rate = <span class="number">1</span></span><br><span class="line">    log_interval = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># is variables</span></span><br><span class="line">    is_save_model = <span class="literal">True</span></span><br><span class="line">    is_dry_run = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># is use cuda?</span></span><br><span class="line">    use_cuda = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set seed</span></span><br><span class="line">    torch.manual_seed(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set device</span></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set train/test dict</span></span><br><span class="line">    train_kwargs = &#123;<span class="string">&#x27;batch_size&#x27;</span>: batch_size&#125;</span><br><span class="line">    test_kwargs = &#123;<span class="string">&#x27;batch_size&#x27;</span>: batch_size&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># if use cuda?</span></span><br><span class="line">    <span class="keyword">if</span> use_cuda:</span><br><span class="line">        cuda_kwargs = &#123;</span><br><span class="line">            <span class="string">&#x27;num_workers&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">            <span class="string">&#x27;pin_memory&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">            <span class="string">&#x27;shuffle&#x27;</span>: <span class="literal">True</span></span><br><span class="line">        &#125;</span><br><span class="line">        train_kwargs.update(cuda_kwargs)</span><br><span class="line">        test_kwargs.update(cuda_kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># mnist transforms</span></span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># load mnist set</span></span><br><span class="line">    train_dataset = datasets.MNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">        train=<span class="literal">True</span>,</span><br><span class="line">        transform=transform,</span><br><span class="line">        download=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    test_dataset = datasets.MNIST(</span><br><span class="line">        root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">        train=<span class="literal">False</span>,</span><br><span class="line">        transform=transform,</span><br><span class="line">        download=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set loader</span></span><br><span class="line">    train_loader = Data.DataLoader(train_dataset, **train_kwargs)</span><br><span class="line">    test_loader = Data.DataLoader(test_dataset, **test_kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># start init net</span></span><br><span class="line">    model = Net().to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set optimizer</span></span><br><span class="line">    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># set lr decrease scheduler</span></span><br><span class="line">    scheduler = StepLR(optimizer, step_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># start training for epoch</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">        train(model, device, train_loader, optimizer, epoch, log_interval, is_dry_run)</span><br><span class="line">        test(model, device, test_loader)</span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_save_model:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&quot;mnist_cnn.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<ul>
<li><p>一個 pytorch 一定會有下面幾個部份</p></li>
<li><p>device</p></li>
<li><p>DataLoader</p></li>
<li><p>optimizer</p></li>
<li><p>scheduler (*)</p></li>
<li><p>init &amp; define Net class</p>
<ul>
<li>__init__</li>
<li>forward</li>
</ul></li>
<li><p>for epoch -&gt;</p>
<ul>
<li>train</li>
<li>test</li>
<li>save model</li>
</ul></li>
</ul>
<h3 id="cuda-的設置">cuda 的設置</h3>
<ul>
<li>可以根據狀況來使用 "cpu" 或者是 "gpu"</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># is use cuda?</span></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="comment"># set seed</span></span><br><span class="line">torch.manual_seed(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># set device</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="seed-的設置">seed 的設置</h3>
<ul>
<li>在初使化參數時是選擇用隨機的方式，所以要選擇用哪一個 seed？</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># set seed</span></span><br><span class="line">torch.manual_seed(<span class="number">5000</span>)</span><br></pre></td></tr></table></figure>
<h3 id="mnist-的設置">MNIST 的設置</h3>
<ul>
<li>使用 torchvision 來用 MNIST
<ul>
<li>root = 儲存的地方</li>
<li>train = 設定是否訓練集</li>
<li>transform = 是否要做資料前處理轉換</li>
<li>download = 是否要下載</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># load mnist set</span></span><br><span class="line">train_dataset = datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">    train=<span class="literal">True</span>,</span><br><span class="line">    transform=transform,</span><br><span class="line">    download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line">test_dataset = datasets.MNIST(</span><br><span class="line">    root=<span class="string">&#x27;./data&#x27;</span>,</span><br><span class="line">    train=<span class="literal">False</span>,</span><br><span class="line">    transform=transform,</span><br><span class="line">    download=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="dataloader-的設置">DataLoader 的設置</h3>
<ul>
<li>在 MNIST 的例子中
<ul>
<li>dataset = 要輸進去的 data</li>
<li>batch_size = 一次要多少個 batch</li>
<li>shuffle = 每個 epoch 是否要洗牌</li>
<li>num_workers = 一次有多少 CPU 執行緒來處理</li>
<li>pin_memory = 會使用 GPU 處理</li>
<li>collate_fn = 是怎麼處理樣本的，可以自定義來實現自己想要的功能</li>
<li>drop_last = 如果總資料除以 batch 大小有餘數的話，還乘下不滿 batch
的資料，要不要丟棄</li>
</ul></li>
<li>在以下的例子中 ** 代表把 dictionary 裡的資料按照 key: value
的方式拿出來</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cuda_kwargs = &#123;</span><br><span class="line">    <span class="string">&#x27;num_workers&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;pin_memory&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&#x27;shuffle&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>: batch_size</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># set loader</span></span><br><span class="line">train_loader = Data.DataLoader(train_dataset, **train_kwargs)</span><br><span class="line">test_loader = Data.DataLoader(test_dataset, **test_kwargs)</span><br></pre></td></tr></table></figure>
<h3 id="optimizer">optimizer</h3>
<ul>
<li>在 pytorch 有很多 optimizer 可以用</li>
<li>所有的 optimizer 都有 step()
<ul>
<li>當計算好 loss 之後就用來更新所有的參數</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h3 id="scheduler">scheduler</h3>
<ul>
<li>是用來管理 learning rate，合理的 learning rate
可以快速收斂，隨著訓練的進行 training rate 應該要越來越小</li>
<li>而 pytroch 提供了 6 種方式來使用</li>
</ul>
<h4 id="steplr">StepLR</h4>
<ul>
<li>等間隔的調整 learning rate</li>
<li>step 以一個 epoch 為一個單位</li>
</ul>
<h4 id="multisteplr">MultiStepLR</h4>
<ul>
<li>不一定要等間隔，按設定的間隔調整 learning rate</li>
</ul>
<h4 id="exponentiallr">ExponentialLR</h4>
<ul>
<li>按指數來調整 learning rate</li>
<li>lr = lr * gamma ** epoch</li>
</ul>
<h4 id="cosineannealinglr">CosineAnnealingLR</h4>
<ul>
<li>以 cosine 為週期，在每個週期最大的時候富新設置 learning rate</li>
</ul>
<h4 id="reducelronplateau">ReduceLROnPlateau</h4>
<ul>
<li>當某個指標不再變化(下降或升高)，就調整 learning rate</li>
</ul>
<h4 id="lambdalr">LambdaLR</h4>
<ul>
<li>每一組 epoch 就是一個學習的策略</li>
<li>每個 epoch 裡都是酪 lambda function</li>
</ul>
<h3 id="model-save-load">model save &amp; load</h3>
<h4 id="state_dist">state_dist</h4>
<ul>
<li>是比較推的做法，只存模型裡面的參數，可以加速載入的速度</li>
<li>使用 torch.save() 保存 state_dict()</li>
<li>記住一定要用 model.eval() 來固定 drop
以及歸一化層，不然每次的結果都不一樣</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h4 id="整個-model">整個 model</h4>
<ul>
<li>比較直觀的做法，但因為是整個 model 比較大</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(model, PATH)</span><br><span class="line">model = torch.load(PATH)</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h4 id="checkpoint">Checkpoint</h4>
<ul>
<li>除了 model 的 state_dict() 之外，有時也可以同時存一些其它的參數</li>
<li>像是 epoch 數，optimizer 也會有 state_dict() 的值，loss 值等等…</li>
<li>值都是用 dict 來存，所以要存取參數也要用 checkpoint['something']
的方法</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.save(&#123;</span><br><span class="line">            <span class="string">&#x27;epoch&#x27;</span>: epoch,</span><br><span class="line">            <span class="string">&#x27;model_state_dict&#x27;</span>: model.state_dict(),</span><br><span class="line">            <span class="string">&#x27;optimizer_state_dict&#x27;</span>: optimizer.state_dict(),</span><br><span class="line">            <span class="string">&#x27;loss&#x27;</span>: loss,</span><br><span class="line">            ...</span><br><span class="line">            &#125;, PATH)</span><br><span class="line">            </span><br><span class="line">model = TheModelClass(*args, **kwargs)</span><br><span class="line">optimizer = TheOptimizerClass(*args, **kwargs)</span><br><span class="line"> </span><br><span class="line">checkpoint = torch.load(PATH)</span><br><span class="line">model.load_state_dict(checkpoint[<span class="string">&#x27;model_state_dict&#x27;</span>])</span><br><span class="line">optimizer.load_state_dict(checkpoint[<span class="string">&#x27;optimizer_state_dict&#x27;</span>])</span><br><span class="line">epoch = checkpoint[<span class="string">&#x27;epoch&#x27;</span>]</span><br><span class="line">loss = checkpoint[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line"> </span><br><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
<h3 id="batch-裡的設置">batch 裡的設置</h3>
<ul>
<li>optimizer.zero_grad()
<ul>
<li>初始化梯度為 0</li>
</ul></li>
<li>output = Net(model)
<ul>
<li>也就是求出 forward propagation</li>
</ul></li>
<li>loss = critertion(output, target)
<ul>
<li>算出 loss</li>
</ul></li>
<li>loss.backward()
<ul>
<li>也就是求出 backward propagation</li>
</ul></li>
<li>optimizer.step()
<ul>
<li>更新參數</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line">output = Net(model)</span><br><span class="line">loss = critertion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h3 id="loss-function">loss function</h3>
<ul>
<li>常見的有以下幾種</li>
</ul>
<h4 id="log_softmax">log_softmax</h4>
<ul>
<li>就是 log 和 softmax 一起做</li>
<li>而 softmax 的公式如下：
<ul>
<li>把每個值做 exponential ，再除以全部 exponential 加總的值</li>
<li>值會在 0 ~ 1 之間</li>
</ul></li>
</ul>
<p><span class="math display">\[
softmax(x_i) = \frac{exp(x_i)}{\Sigma exp(x_i)}
\]</span></p>
<ul>
<li>那 log_softmax 就只是再加上個 log</li>
<li>值為 <span class="math inline">\(-\infin\)</span> ~ 0</li>
</ul>
<p><span class="math display">\[
softmax(x_i) = log(\frac{exp(x_i)}{\Sigma exp(x_i)})
\]</span></p>
<ul>
<li>dim = 0 代表列總合為 1</li>
<li>dim = 1 代表行總合為 1</li>
</ul>
<h4 id="nll_loss">nll_loss</h4>
<ul>
<li>negative log likelihood loss</li>
<li>把 softmax 的結果中每一個 label 的數值拿出來<br />
</li>
<li>取絕對值相加求平均就是 nll_loss</li>
</ul>
<h4 id="cross_entropy">cross_entropy</h4>
<ul>
<li><p>在計算兩個向量的相似度，計算期望向量與實際向量的相似度</p></li>
<li><p>與內積相同，內積也可以看做在做相似度</p></li>
<li><p>在二元的世界中公式可以表示成： <span class="math display">\[
y . p(f(x)) + (1-y) . (1 - p(f(x)))
\]</span></p></li>
<li><p>接著要取 log</p></li>
</ul>
<p><span class="math display">\[
y . ln(p(f(x))) + (1-y) . ln(1 - p(f(x)))
\]</span></p>
<ul>
<li>而因為算出來是機率，越大越相似</li>
<li>所以加個負號才能代表 loss 的精神 -&gt; 越小越好</li>
</ul>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(三)：網路模型 torch.nn.Module</title>
    <url>/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B8%89-%EF%BC%9A%E7%B6%B2%E8%B7%AF%E6%A8%A1%E5%9E%8B-torch-nn-Module/</url>
    <content><![CDATA[<p>設定好訓練
SOP、設定好自定義的資料集後，接下來我們要來設計自己的網路模型，會使用到
Pytorch 中 torch.nn.Module 這個物件</p>
<p>keywords: torch.nn.Module <span id="more"></span></p>
<h2 id="torch.nn.module">torch.nn.Module</h2>
<ul>
<li>有三種創建 module 的方法
<ul>
<li>繼承 nn.module 的普通方法</li>
<li>nn.sequential</li>
<li>nn.ModuleList</li>
</ul></li>
</ul>
<h3 id="nn.module">nn.Module</h3>
<ul>
<li>基本款</li>
<li>有一個 __init__ 設定各個神經層的設定，命名好後在下一個 forward
來使用，通常是放「需要學習的的層」</li>
<li>另一個 forward
來設定各個層的連接以及參數設定，通常放的是「不需要學習的層」，像
activate function</li>
<li>在 pytorch 中 backward 會自動實現，使用的是 Autogard</li>
<li>以及在 pytorch 中 nn.Module 只支持 mini-batch 的輸入方式 N x C x H x
W (1 x 3 x 128 x 128)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Model, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">20</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(conv1(x))</span><br><span class="line">        <span class="keyword">return</span> F.relu(conv2(x))</span><br></pre></td></tr></table></figure>
<h3 id="nn.sequential">nn.Sequential</h3>
<ul>
<li>nn.Sequential
的模組是按照順序排列的，需要確保輸出大小與輸入大小一致</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net_seq</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(net2, self).__init__()</span><br><span class="line">        self.seq = nn.Sequential(</span><br><span class="line">              nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">              nn.ReLU(),</span><br><span class="line">              nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">              nn.ReLU()</span><br><span class="line">        )      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.seq(x)</span><br><span class="line">net_seq = net_seq()</span><br><span class="line"><span class="built_in">print</span>(net_seq)</span><br><span class="line"></span><br><span class="line"><span class="comment">#net_seq(</span></span><br><span class="line"><span class="comment">#  (seq): Sequential(</span></span><br><span class="line"><span class="comment">#    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (1): ReLU()</span></span><br><span class="line"><span class="comment">#    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (3): ReLU()</span></span><br><span class="line"><span class="comment">#  )</span></span><br><span class="line"><span class="comment">#)</span></span><br></pre></td></tr></table></figure>
<ul>
<li>nn.Sequential 中也採用 OrderedDict 来指定 module 的名字，而非 index
(0, 1, 2, ...)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net_seq</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(net_seq, self).__init__()</span><br><span class="line">        self.seq = nn.Sequential(OrderedDict([</span><br><span class="line">            (<span class="string">&#x27;conv1&#x27;</span>, nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>)),</span><br><span class="line">            (<span class="string">&#x27;relu1&#x27;</span>, nn.ReLU()),</span><br><span class="line">            (<span class="string">&#x27;conv2&#x27;</span>, nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>)),</span><br><span class="line">            (<span class="string">&#x27;relu2&#x27;</span>, nn.ReLU())</span><br><span class="line">        ]))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.seq(x)</span><br><span class="line">net_seq = net_seq()</span><br><span class="line"><span class="built_in">print</span>(net_seq)</span><br><span class="line"></span><br><span class="line"><span class="comment">#net_seq(</span></span><br><span class="line"><span class="comment">#  (seq): Sequential(</span></span><br><span class="line"><span class="comment">#    (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (relu1): ReLU()</span></span><br><span class="line"><span class="comment">#    (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (relu2): ReLU()</span></span><br><span class="line"><span class="comment">#  )</span></span><br><span class="line"><span class="comment">#)</span></span><br></pre></td></tr></table></figure>
<h3 id="nn.modulelist">nn.ModuleList</h3>
<ul>
<li>nn.ModuleList 也是一個存不同 module 的 list，可任意得把 nn.Module
加到 list 中</li>
<li>與 python 的 list 操作相同，可以 extend append...</li>
<li>但它會自動把 module 的 parameters 自動加入網路中</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net_modlist</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(net_modlist, self).__init__()</span><br><span class="line">        self.modlist = nn.ModuleList([</span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">20</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        ])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modlist:</span><br><span class="line">            x = m(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net_modlist = net_modlist()</span><br><span class="line"><span class="built_in">print</span>(net_modlist)</span><br><span class="line"><span class="comment">#net_modlist(</span></span><br><span class="line"><span class="comment">#  (modlist): ModuleList(</span></span><br><span class="line"><span class="comment">#    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (1): ReLU()</span></span><br><span class="line"><span class="comment">#    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#    (3): ReLU()</span></span><br><span class="line"><span class="comment">#  )</span></span><br><span class="line"><span class="comment">#)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net_modlist.parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">type</span>(param.data), param.size())</span><br><span class="line"><span class="comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([20, 1, 5, 5])</span></span><br><span class="line"><span class="comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([20])</span></span><br><span class="line"><span class="comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([64, 20, 5, 5])</span></span><br><span class="line"><span class="comment">#&lt;class &#x27;torch.Tensor&#x27;&gt; torch.Size([64])</span></span><br></pre></td></tr></table></figure>
<h3 id="nn.sequential-vs-nn.modulelist">nn.Sequential vs
nn.ModuleList</h3>
<ul>
<li>nn.Sequential 內部自動實現 forward 所以不用再一個一個加，但
nn.ModuleList 沒有，任需一個一個加入</li>
<li>且 nn.Module 中沒有一定的順序，可用 index 來指定</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 不在 nn.Module 的方法</span></span><br><span class="line">seq = nn.Sequential(</span><br><span class="line">    nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">    nn.ReLU()</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(seq)</span><br><span class="line"><span class="comment"># Sequential(</span></span><br><span class="line"><span class="comment">#   (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (1): ReLU()</span></span><br><span class="line"><span class="comment">#   (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="comment">#   (3): ReLU()</span></span><br><span class="line"><span class="comment"># )</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># nn.Sequential</span></span><br><span class="line"><span class="comment"># 繼承 nn.Module 的方法，就要寫出 forward </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net1</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(net1, self).__init__()</span><br><span class="line">        self.seq = nn.Sequential(</span><br><span class="line">             nn.Conv2d(<span class="number">1</span>,<span class="number">20</span>,<span class="number">5</span>),</span><br><span class="line">             nn.ReLU(),</span><br><span class="line">             nn.Conv2d(<span class="number">20</span>,<span class="number">64</span>,<span class="number">5</span>),</span><br><span class="line">             nn.ReLU()</span><br><span class="line">        )      </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.seq(x)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># nn.ModuleList 的方法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net2</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">      <span class="built_in">super</span>(net2, self).__init__()</span><br><span class="line">      self.modlist = nn.ModuleList([</span><br><span class="line">          nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>),</span><br><span class="line">          nn.ReLU(),</span><br><span class="line">          nn.Conv2d(<span class="number">20</span>, <span class="number">64</span>, <span class="number">5</span>),</span><br><span class="line">          nn.ReLU()</span><br><span class="line">      ])</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 注意：只能按照下面利用 for 的方式</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">       <span class="keyword">for</span> m <span class="keyword">in</span> self.modlist:</span><br><span class="line">           x = m(x)</span><br><span class="line">       <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h3 id="reference">Reference</h3>
<ul>
<li>https://blog.csdn.net/u012609509/article/details/81203436</li>
<li>https://zhuanlan.zhihu.com/p/75206669</li>
</ul>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(二)：Dataset DataLoader</title>
    <url>/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%BA%8C-%EF%BC%9ADataset-DataLoader/</url>
    <content><![CDATA[<p>如果今天開發需求不是像 Mnist
這樣，別人已經幫你準備好的資料集，而是自己的影像資料集，那要怎麼放進
DataLoader 裡面訓練呢？</p>
<p>keywords: DataLoader <span id="more"></span></p>
<h2 id="dataset-dataloader">Dataset DataLoader</h2>
<ul>
<li>使用繼承 Dataset 可以自定義 data，再放進 DataLoader 中</li>
<li>一個 Dataset 繼承後要 override 的 function 如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">customDataset</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        <span class="comment"># Initialize paths, transforms, and so on</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        <span class="comment"># 1. Read from file (using numpy.fromfile, PIL.Image.open)</span></span><br><span class="line">        <span class="comment"># 2. Preprocess the data (torchvision.Transform).</span></span><br><span class="line">        <span class="comment"># 3. Return the data (e.g. image and label)</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br><span class="line">        <span class="comment"># Indicate the total size of the dataset</span></span><br><span class="line">        <span class="comment"># --------------------------------------------</span></span><br></pre></td></tr></table></figure>
<ul>
<li>__init__ 負責初使化 path、img list、label list、transform</li>
<li>__getitem__ 負責讀取圖片，並做 transform，回傳 img 以及 label
<ul>
<li>回傳值也可以回傳不只兩個，也可根據需求回傳想要的資料</li>
</ul></li>
<li>__len__ 回傳 imgs 的長度 len(self.imgs)</li>
</ul>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(四)：資料擴增、前處理 torchvision.transforms</title>
    <url>/2022/12/27/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%9B%9B-%EF%BC%9A%E8%B3%87%E6%96%99%E6%93%B4%E5%A2%9E%E3%80%81%E5%89%8D%E8%99%95%E7%90%86-torchvision-transforms/</url>
    <content><![CDATA[<p>在處理
dataset，有時候我們會遇到需要資料前處理、資料擴增…等等對影像處理的步驟：像是希望可以透過旋轉、鏡像做到資料擴增；希望可以利用影像裁切統一輸入影像大小</p>
<p>而 pytorch 也很貼心的提供給我們一個套件使用：torchvision，torchvision
裡面提供了非常多的影像處理方法：像是旋轉、鏡像、裁切</p>
<p>以下這篇文章整理了大部份常用到的函式：<a
href="https://chih-sheng-huang821.medium.com/03-pytorch-dataaug-a712a7a7f55e">Pytorch提供之torchvision
data augmentation技巧</a></p>
<p>keywords: torchvision <span id="more"></span></p>
<h2 id="如何使用">如何使用？</h2>
<p>torchvision 所有函式輸入只支持 PIL Image，也是就使用 PIL Image
這個套件打開圖片的格式，才會被 torchvision 接受，其它像是 torch.tensor
或是 np.array 都是沒有辦法的</p>
<p>以我們要把影像轉為灰階圖片 -&gt; 然後從 PIL Image 轉成
torch.tensor，PIL 的 SOP 如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line">    </span><br><span class="line"><span class="comment"># read image with PIL module</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(imagepath)</span><br><span class="line">img = transforms.Grayscale()(img)</span><br><span class="line">img = transforms.ToTensor()(img)</span><br></pre></td></tr></table></figure>
<p>如果覺得這樣寫太多行的話，torchvision 也有提供 transforms.Compose
的方案，可以把很多的 transform 打包在一起，就可以統一呼叫便於管理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義 compose    </span></span><br><span class="line">transfrom = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># read image with PIL module</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(imagepath)</span><br><span class="line"><span class="comment"># 使用 compose</span></span><br><span class="line">img = transform(img)</span><br></pre></td></tr></table></figure>
<p>而在 transforms.Compose
中，我們也可以放入自定義的影像處理函式，假設我們要定義一個「自定義
Padding」的處理，程式碼如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL.Image <span class="keyword">as</span> Image</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義 compose    </span></span><br><span class="line">transfrom = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(),</span><br><span class="line">    <span class="comment"># 自定義的處理函式</span></span><br><span class="line">    SuarePad(),</span><br><span class="line">    transforms.ToTensor()</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># read image with PIL module</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(imagepath)</span><br><span class="line"><span class="comment"># 使用 compose</span></span><br><span class="line">img = transform(img)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SquarePad</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, targetW, targetH</span>):</span></span><br><span class="line">        self.targetW = targetW</span><br><span class="line">        self.targetH = targetH</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, image</span>):</span></span><br><span class="line">        <span class="comment"># 2003 308</span></span><br><span class="line">        w, h = image.size</span><br><span class="line">        w_fix = self.targetW - w</span><br><span class="line">        padding = (<span class="number">0</span>, <span class="number">0</span>, w_fix, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> F.pad(image, padding, <span class="number">0</span>, <span class="string">&#x27;constant&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>對於上面自定義 class 的補充說明：</p>
<p><code>__init__</code> 的目的在類似 constructor，只在當一個類別
(class) 實作成物件 (object) 時會呼叫</p>
<p><code>__call__</code> 是可以把 class
也模擬有著函式一樣的特色，用傳入參數並呼叫的方式使用</p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>如何在 docker container 中 matplotlib 顯示中文？</title>
    <url>/2022/12/27/%E5%A6%82%E4%BD%95%E5%9C%A8-docker-container-%E4%B8%AD-matplotlib-%E9%A1%AF%E7%A4%BA%E4%B8%AD%E6%96%87%EF%BC%9F/</url>
    <content><![CDATA[<p>一般來說 matplotlib 在產生 figure
時，所套用的字體並未包含中文，所以如果要在 figure
中顯示中文，我們勢必要特別指定一個字體給它</p>
<p>keywords: docker、matplotlib、中文 <span id="more"></span></p>
<h3 id="下載字體">下載字體</h3>
<p>首先要來下載喜歡的字體，選一個自己喜歡的就可以了，這裡我是選 google
開源的 Noto 繁中字體</p>
<p><a
href="https://fonts.google.com/noto/specimen/Noto+Sans+TC">https://fonts.google.com/noto/specimen/Noto+Sans+TC</a></p>
<p>將下載後的檔案解壓縮，選一個自己喜歡的字體組細，這邊我是選
<code>NotoSansTC-Medium.otf</code></p>
<h3 id="加到-matplotlib-裡面">加到 matplotlib 裡面</h3>
<p>進入到 docker container 中 (使用 vscode ssh container 或是指令 docker
exec -it ... 都可以)，到 container 的根目錄中 <code>/</code></p>
<p>找到並進入以下路徑：<code>/opt/conda/lib/python3.7/site-packages/matplotlib</code>。上述路徑是
matplotlib 存放在 docker container 中的位置</p>
<p>接著再找到以下資料夾：<code>mpl-data/fonts/ttf/</code>，這是字體存放的地方，把剛剛下載好的
<code>NotoSansTC-Medium.otf</code> 上傳至這個資料夾中</p>
<h3 id="修改-matplotlib-設定檔">修改 matplotlib 設定檔</h3>
<p>在剛剛的 <code>mpl-data</code>
資料夾中，找到一個名稱叫：<code>matplotlibrc</code> 的設定檔，打開它</p>
<p>找到一下程式，把以下兩行的註解拿掉，並在 font.serif 的第一個
<code>,</code> 前加入剛剛上傳的字體名稱</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">font.family:</span>  <span class="string">sans-serif</span>    <span class="comment"># &lt;- 拿掉註解</span></span><br><span class="line"><span class="comment">#font.style:   normal</span></span><br><span class="line"><span class="comment">#font.variant: normal</span></span><br><span class="line"><span class="comment">#font.weight:  normal</span></span><br><span class="line"><span class="comment">#font.stretch: normal</span></span><br><span class="line"><span class="comment">#font.size:    10.0</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 加入上傳字體名稱</span></span><br><span class="line"><span class="attr">font.serif:</span>     <span class="string">NotoSansTC-Medium,</span> <span class="string">DejaVu</span> <span class="string">Serif,</span> <span class="string">Bitstream</span> <span class="string">Vera</span> <span class="string">Serif,</span> <span class="string">Computer</span> <span class="string">Modern</span> <span class="string">Roman,</span> <span class="string">New</span> <span class="string">Century</span> <span class="string">Schoolbook,</span> <span class="string">Century</span> <span class="string">Schoolbook</span> <span class="string">L,</span> <span class="string">Utopia,</span> <span class="string">ITC</span> <span class="string">Bookman,</span> <span class="string">Bookman,</span> <span class="string">Nimbus</span> <span class="string">Roman</span> <span class="string">No9</span> <span class="string">L,</span> <span class="string">Times</span> <span class="string">New</span> <span class="string">Roman,</span> <span class="string">Times,</span> <span class="string">Palatino,</span> <span class="string">Charter,</span> <span class="string">serif</span>    <span class="comment"># &lt;- 拿掉註解</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#font.sans-serif: DejaVu Sans, Bitstream Vera Sans, Computer Modern Sans Serif, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif</span></span><br></pre></td></tr></table></figure>
<h3 id="修改-python-程式">修改 python 程式</h3>
<p>接著回到程式中，我們要在建立一個 figure
物件後，調整一些字體設定，使用 <code>plt.rcParams</code>
來修改字體及字體大小</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create font</span></span><br><span class="line">fig, ax = plt.subplots()</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt font setting</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = FONT_SIZE</span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>] = [<span class="string">&#x27;NotoSansTC-Medium&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>接著就可以跑原本寫的程式啦 ~ ……嗎？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(<span class="string">&#x27;save.png&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>這個時候會發現…跑了上面存圖片的程式，生出來的中文字還是顯示不出來……為什麼呢？</p>
<h3 id="加到-cache-中">加到 cache 中</h3>
<p>原來 matplotlib 會在 <code>/root/.cache/matplotlib</code> 中新增
cache，所有的設定優先會從邊尋找，所以我們剛剛這麼大費周章的修改，結果對
matplotlib 來講跟本沒差…</p>
<p>所以現在我們要手動修改 cache 中的檔案，打開
<code>/root/.cache/matplotlib/fontlist-v330.json</code> 檔案</p>
<p>在程式的最下面新增這一些東西：</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">    ...</span><br><span class="line">		...</span><br><span class="line">		&#123;</span><br><span class="line">      <span class="attr">&quot;fname&quot;</span>: <span class="string">&quot;fonts/ttf/NotoSansTC-Medium.otf&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;NotoSansTC-Medium&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;style&quot;</span>: <span class="string">&quot;italic&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;variant&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;weight&quot;</span>: <span class="number">400</span>,</span><br><span class="line">      <span class="attr">&quot;stretch&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;size&quot;</span>: <span class="string">&quot;scalable&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;__class__&quot;</span>: <span class="string">&quot;FontEntry&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">		&#123;</span><br><span class="line">      <span class="attr">&quot;fname&quot;</span>: <span class="string">&quot;/usr/share/fonts/truetype/dejavu/NotoSansTC-Medium.otf&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;NotoSansTC-Medium&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;style&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;variant&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;weight&quot;</span>: <span class="number">400</span>,</span><br><span class="line">      <span class="attr">&quot;stretch&quot;</span>: <span class="string">&quot;normal&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;size&quot;</span>: <span class="string">&quot;scalable&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;__class__&quot;</span>: <span class="string">&quot;FontEntry&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  <span class="string">&quot;__class__&quot;</span>: <span class="string">&quot;FontManager&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="重新整理">重新整理</h3>
<p>最後最後，也是最重要也最容易忘記的一步，就是 <strong>重開
container！</strong>，剛剛新增了這麼多東西如果不給它重新整理一下，這個設定是不會生效的！</p>
<p>下 <code>docker container restart</code> 重起 container
後就大功告成啦！！！</p>
]]></content>
      <categories>
        <category>雜開發心得</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>matplotlib</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(七)：訓練小技巧 AMP 混合精度</title>
    <url>/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B8%83-%EF%BC%9A%E8%A8%93%E7%B7%B4%E5%B0%8F%E6%8A%80%E5%B7%A7-AMP-%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6/</url>
    <content><![CDATA[<p>用一串話簡單解釋什麼是 AMP：</p>
<p>在 2017 Nvidia
提出了用於「混合精度的訓練方法」，是一種可使用不同精度來運算 cuda tensor
運算，Nvidia 很貼心的用 python 整理成 apex 套件讓大家方便使用
https://github.com/NVIDIA/apex。而在之後 pytorch 1.6 的更新中，在 Nvidia
的幫忙下，開發了 torch.cuda.amp 函式 (AMP 全稱 Automatic Mixed
Precision)，使得混合精度訓練可以在 pytorch 中直接引入並使用。</p>
<p>keywords: AMP <span id="more"></span></p>
<p>相信大家看完一定還是霧颯颯，那接下來依照下列順序介紹
AMP，更詳細的了解背後的歷史演進：</p>
<ul>
<li>什麼是精度？</li>
<li>為什麼要混合精度？</li>
<li>如何使用 AMP？</li>
</ul>
<h3 id="什麼是精度">什麼是精度？</h3>
<p>一般我們在使用 pytorch 時，如果簡單的初始化一個 tensor，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tensor1 = torch.zeros(<span class="number">20</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor.<span class="built_in">type</span>())   <span class="comment"># &#x27;torch.FloatTensor&#x27;</span></span><br><span class="line"></span><br><span class="line">tensor2 = torch.Tensor([<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(tensor.<span class="built_in">type</span>())   <span class="comment"># &#x27;torch.FloatTensor&#x27;</span></span><br></pre></td></tr></table></figure>
<p>可以看到 pytorch 中，新增預設的精度就是
FloatTensor，習慣上中文會稱它叫：單精度浮點運算 (single)</p>
<p>小小複習一下，通常 float 會用 32 個 bit 來存資料；double
稱雙精度浮點則用 64 bit</p>
<p>而在 pytorch 中一共支援 10 種不同資料型態的 tensor：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.FloatTensor    (32-bit floating point)</span><br><span class="line">torch.DoubleTensor   (64-bit floating point)</span><br><span class="line">torch.HalfTensor     (16-bit floating point 1)</span><br><span class="line">torch.BFloat16Tensor (16-bit floating point 2)</span><br><span class="line">torch.ByteTensor     (8-bit integer (unsigned))</span><br><span class="line">torch.CharTensor     (8-bit integer (signed))</span><br><span class="line">torch.ShortTensor    (16-bit integer (signed))</span><br><span class="line">torch.IntTensor      (32-bit integer (signed))</span><br><span class="line">torch.LongTensor     (64-bit integer (signed))</span><br><span class="line">torch.BoolTensor     (Boolean)</span><br></pre></td></tr></table></figure>
<p>可以發現在 DoubleTensor 下方多了一個 HalfTensor
「半精度浮點」，而這個就是今天的主角，也是為什麼要使用 AMP
的最大理由。</p>
<h3 id="為什麼要混合精度">為什麼要混合精度？</h3>
<p>剛剛上面介紹各種型態的 Tensor 最後都會整理到 Nvidia GPU
中做運算，而在 GPU 負責運算的單元稱 cuda 核心(<strong>C</strong>ompute
<strong>U</strong>nified <strong>D</strong>evice
<strong>A</strong>rchitecture 統一計算架構)，一個 cuda 核心由一個 ALU
(Integer arithmetic logic uint 整數運算單元) 及一個 FPU (Floating point
unit 浮點運算單元) 所組成，也就是說一個 CUDA
核心專門來做<strong>乘法</strong>及<strong>加法</strong>，而 cuda
核心中還有一個特別的指令：FMA (Fused multiply add)
可以用一個指令完成加乘融合的操作。</p>
<p><img src="https://i.imgur.com/IO8GIwY.png"
alt="image-20220820113513902" /></p>
<p>一般我們在深度學習中最常看見的算式是這個： <span
class="math display">\[
x_{l} = x_{l-1}w+b
\]</span> 這種又加又乘的操作藉由 cuda
核心的幫忙，可以在不改變精度下，把原本要兩個指令完成的事縮減成一個指令，大輻減少運算時間。以上
cuda 預設支援 Float32 的運算，也正好與 pytorch 相符。自 2006 年的 Tesla
架構推出以後，cuda 核心就一直內建在 Nvidia GPU 中了。</p>
<p>不過這時有一個聲音悄悄的跑出來：我們能不能再加速呢？如果還要加速的話有以下兩個地方可以改進：</p>
<ul>
<li>設計新的核心，可以硬體加速更高級的運算，例如一個指令完成 Tensor
運算</li>
<li>藉由把浮點數的精度降低，再做乘法，達到減少運算複雜度的加速，但同時又不能失去太多的精度</li>
</ul>
<p>如果你是 Nvidia 工程師會怎麼呢？小朋友才選擇嘛 XD
當然是兩個都做阿！所以 2017 年年底 Nvidia Volta 架構上提出了新的 Tensor
核心單元，完美達成上面兩件事情：在不損失太多精度下，減少整體的運算時間。接下透過以下兩個
GIF 動畫可以了解到 Tensor 核心的力量</p>
<p><img src="https://i.imgur.com/LSa0CvU.gif"
alt="1fd55a3c-9362-11eb-a595-1278b449b310" /></p>
<p><img src="https://i.imgur.com/21VdRyt.gif"
alt="5a1ec0e0-7e84-11eb-aca1-aa09f3df2eff" /></p>
<p>上面兩動畫還隱含了兩個資訊：</p>
<ul>
<li>Tensor 核心可以做到使用一個指令完成一個 Tensor 運算</li>
<li>當資料精度越小時 (FP32 -&gt; FP16 -&gt;
INT8)，同一時間下完成的運算量更高</li>
</ul>
<p>所以整個又回到最一開始的問題，為什麼要使用「混合精度」？因為更低的精度意味著更快的運算，但為了資料不能丟失太多細節，所以有必要使用高精度運算的還是維持
FP32，但是有一些沒那麼重要的運算就可以改使用 FP16，這樣在一個 Tensor
運算中，又有 FP32 又有 FP16 的操作，就是混合精度的原由。</p>
<h3 id="如何使用-amp">如何使用 AMP</h3>
<p>剛剛上述提到的 FP32 對應 pytorch 中的
<code>torch.FloatTensor</code>，而 FP16 則是對應
<code>torch.HalfTensor</code>，這兩種不同的精度各自有什麼優缺點呢？</p>
<p>HalfTensor 的優缺點：</p>
<p>精度低，運算快，但消失精度的代價是算出來的值失去很多細節，這個現象會導致，overfitting/underfitting
的發生。因為在做 Backpropagation
時根據數值不斷的往後計算，越算越小，小到超出 FP16 所能表示的最小數值
<span
class="math inline">\(2^{-14}\)</span>，會使得更先前的層參數無法更新</p>
<p>另一個問題也是因為 FP16 最小的數值間距為 <span
class="math inline">\(2^{-13}\)</span>
如果有小於這個數字的算式都會被當誤差而省略掉了</p>
<p>因此要如何甚選要什麼運算使用 FP16 來加速可是個大問題，好加在 pytorch
已經幫我們整理好了，以下的操作都是可以用 FP16 來加速，因此 pytorch
會自動這型態轉換成 HalfTensor 來計算，而其它則維持 FloatTensor：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">__matmul__</span><br><span class="line">addbmm</span><br><span class="line">addmm</span><br><span class="line">addmv</span><br><span class="line">addr</span><br><span class="line">baddbmm</span><br><span class="line">bmm</span><br><span class="line">chain_matmul</span><br><span class="line">conv1d</span><br><span class="line">conv2d</span><br><span class="line">conv3d</span><br><span class="line">conv_transpose1d</span><br><span class="line">conv_transpose2d</span><br><span class="line">conv_transpose3d</span><br><span class="line">linear</span><br><span class="line">matmul</span><br><span class="line">mm</span><br><span class="line">mv</span><br><span class="line">prelu</span><br></pre></td></tr></table></figure>
<p>那實際上程式碼要怎麼去寫呢？其實也非很簡單，只需引用 torch.cuda.amp
包，再進行以下操作就行了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用 amp 中的 autocast 來實現，自動判哪些運算要用 HalfTensor 哪些運算維持原樣用 FloatTensor</span></span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast <span class="keyword">as</span> autocast</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立新 model，預設是 torch.FloatTensor</span></span><br><span class="line">model = Net().cuda()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), ...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 with 關鍵字，把前傳遞 forward 及算 loss</span></span><br><span class="line">    <span class="comment"># 的部份用 autocast() 包起來</span></span><br><span class="line">    <span class="keyword">with</span> autocast():</span><br><span class="line">        output = model(<span class="built_in">input</span>)</span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation 不必用 autocast 包起來</span></span><br><span class="line">    <span class="comment"># 理由是 Backpropagation 會依據 Forward 的資料型態直接沿用來做</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>是不是很簡單呢？簡簡單單的一行就可以用 Tensor
核心幫你加速訓練/測試的時間，與此同時還有一個好的副作用：顯存下降了！也很合理，因為要存的浮點精度變少了嘛</p>
<p>不過如果只單純這樣用的話，在訓練時會多發生一個問題，訓練會
over/underfitting！，精度的下降果然還是使用在 Backpropagation
時，參數傳不到前面去更新了，因此要再使用 amp
中的另一個黑科技：GradScaler</p>
<p>GradScaler 實際精神在於，把網路算出來的 Loss 用一個倍率放大，在
Backpropagation 存著 .grad 的值也一並放大，但最後用 optimizer
更新參數時還是要把值縮小回原本的大小，這樣子的做法就不會有因為精度損失而導致更新不到前面的參數了</p>
<p>實驗程式碼的實作方式也不困難，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 利用 amp 中的 autocast 來實現，自動判斷哪些運算要用 HalfTensor 哪些運算維持原樣用 FloatTensor</span></span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast <span class="keyword">as</span> autocast</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立新 model，預設是 torch.FloatTensor</span></span><br><span class="line">model = Net().cuda()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), ...)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">input</span>, target <span class="keyword">in</span> data:</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用 with 關鍵字，把前傳遞 forward 及算 loss</span></span><br><span class="line">    <span class="comment"># 的部份用 autocast() 包起來</span></span><br><span class="line">    <span class="keyword">with</span> autocast():</span><br><span class="line">        output = model(<span class="built_in">input</span>)</span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Scales loss. 用一定的倍率放大 Loss，並計算出各個 node 的 .grad 值</span></span><br><span class="line">        scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 這一步詳細流程見下面</span></span><br><span class="line">        scaler.step(optimizer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 準備著，看下一次是否有要做 scaler 放大 Loss</span></span><br><span class="line">        scaler.update()</span><br></pre></td></tr></table></figure>
<p>這個 scaler
放大倍數也是動態調整的，為什麼呢？理應放大倍率越大越好，保留越多的數字，但現實很骨感，如果真放超大會直接
overfitting 出現 infs，但是放大太小又會出現 NaNs，所以這個 scaler
會自動的去調整放大倍率大小，在不發生仍何 over/underfitting
下找到最合適的放大倍率</p>
<p>以上就是 torch.cuda.amp
的完整詳細介紹及用法啦！要再更進階的話還有一個小細節要注意：如果是有使用
DDP 訓練的方法，在加入 autocast() 要特別注意</p>
<p>除了在 train 的 forward 時要加入 autocast() 前文，同時也要記得在 繼承
nn.module 的 forward() 函式中，也要加上 autocast() 的前文，或是使用
decorator 也可</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 方法一：使用 decorator</span></span><br><span class="line">MyModel(nn.Module):</span><br><span class="line"><span class="meta">    @autocast()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 方法一：使用 with 前文</span></span><br><span class="line">MyModel(nn.Module):</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">with</span> autocast():</span><br><span class="line">            ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = MyModel()</span><br><span class="line">dp_model=nn.DataParallel(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 除了訓練 forward 要加，model 中的 forward 也要加</span></span><br><span class="line"><span class="keyword">with</span> autocast():</span><br><span class="line">    output = dp_model(<span class="built_in">input</span>)</span><br><span class="line">    loss = loss_fn(output)</span><br></pre></td></tr></table></figure>
<p>那實際效果跑起來如何呢？基本上網友們的反應是：一、顯存下降；二、時間變長，咦…等等等，怎麼用了混合精度時間變慢，不是說精度越小速度越快嗎？後來發現原因出現在
GradScaler 上面，Loss 及梯度在經過一個 scaler
放大縮小一來一回下，增加了不少時間損耗，至於這個功能最後要不要加上去呢…？這個就見人見智囉！</p>
<h3 id="reference">Reference</h3>
<p><a href="https://www.zhihu.com/question/451127498">cuda core vs
tensor core 知乎</a></p>
<p><a
href="https://www.cnblogs.com/jimchen1218/p/14315008.html">Pytorch自动混合精度(AMP)介绍与使用</a></p>
<p><a
href="https://zhuanlan.zhihu.com/p/165152789">PyTorch的自动混合精度（AMP）</a></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(九)：一些 optimizer 整理</title>
    <url>/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%B9%9D-%EF%BC%9A%E4%B8%80%E4%BA%9B-optimizer-%E6%95%B4%E7%90%86/</url>
    <content><![CDATA[<p>本篇筆記主要參考以下網路文章：<a
href="https://zhuanlan.zhihu.com/p/22252270">https://zhuanlan.zhihu.com/p/22252270</a></p>
<p>整理了一些常用 optimizer 的數學原理，及其重點特色</p>
<p>keywords: optimizer <span id="more"></span></p>
<h2 id="sgd">SGD</h2>
<ul>
<li>stochastic gradient descent</li>
</ul>
<p><span class="math display">\[
g_t = \nabla_{\theta_{t-1}}f(\theta_{t-1}) \\
\Delta\theta_t = - \eta * g_t
\]</span></p>
<ul>
<li><span class="math inline">\(\eta\)</span> 是 learning rate</li>
<li>SGD 完全依賴目前梯度的斜率大小</li>
<li>遇到鞍點等地方會不容易達到最優</li>
<li>且 SGD 整體更新速度慢</li>
</ul>
<h2 id="momentum">Momentum</h2>
<ul>
<li>模仿物理中的動量</li>
<li>把之前算出來的梯度大小一起放到這一次的運算</li>
</ul>
<p><span class="math display">\[
m_t = \mu*m_{t-1} + g_t \\
\Delta\theta_t = -\eta * m_t
\]</span></p>
<ul>
<li>相較於 SGD 更新速度快</li>
<li>在梯度改變方向的時候，<span class="math inline">\(\mu\)</span>
可以減少更新，抑制振盪</li>
</ul>
<h2 id="nesterov">Nesterov</h2>
<ul>
<li>nesterov 在梯度更新時做一個校正，避免前進太快，同時提高靈敏度，與
momentum 有點像</li>
<li>由公式可以看出 momentum 沒有更改當前梯度 <span
class="math inline">\(g_t\)</span></li>
<li>於是在 Nesterov 中就是透過修改 <span
class="math inline">\(g_t\)</span> 來達到修改的目的</li>
</ul>
<p><span class="math display">\[
g_t = \nabla_{\theta_{t-1}}f(\theta_{t-1}-\eta*\mu*m_{t-1}) \\
m_t = \mu * m_{t-1} + g_t \\
\Delta\theta_t = -\eta*m_t
\]</span></p>
<p>雖然 momentum nesterov
都是為了增加梯度更新時的彈性，但人工設定還不如用機器自己來學習</p>
<p>以下介紹機器自己學習的方法</p>
<h2 id="adagrad">Adagrad</h2>
<ul>
<li>是對 learning rate 設定了一項限制</li>
<li><span class="math inline">\(\epsilon\)</span> 用來保證分非 0</li>
<li>把 <span class="math inline">\(\eta\)</span> 除上一個值使得</li>
<li>前期 <span class="math inline">\(g_t\)</span>
較小的時候，regularizer 比較大，能夠放大梯度</li>
<li>後期 <span class="math inline">\(g_t\)</span>
較大的時候，regularizer 比較小，能夠約束梯度</li>
<li>缺點：</li>
<li>仍要人工設定 learning rate</li>
<li><span class="math inline">\(\eta\)</span> 設太大的話，會讓
regularizer 過於敏感，對梯度改變太大</li>
</ul>
<p><span class="math display">\[
n_t = n_{t-1} + g^2\\
\Delta\theta_t = -\frac{\eta}{\sqrt{n_t+\epsilon}}*g_t
\]</span></p>
<h2 id="adadelta">Adadelta</h2>
<ul>
<li>是 Adagrad 的進階版</li>
<li>只累加固定大小的項</li>
</ul>
<p><span class="math display">\[
n_t = v*n_{t-1} + (1-v) *g^2_t \\
\Delta\theta_t = -\frac{\eta}{\sqrt{n_t+\epsilon}} * g_t
\]</span></p>
<ul>
<li>在經過作者一系列，近似牛頓迭代法的方法後</li>
<li>可以實現機器自動學習 learning rate</li>
</ul>
<h2 id="rmsprop">RMSprop</h2>
<ul>
<li>算是 Adadelta 的變形</li>
</ul>
<p><span class="math display">\[
E|g^2|_t = \rho * E|g^2|_t-1 + (1 - \rho) * g^2_t \\
RMS|g|_t = \sqrt{E|g^2|_t + \epsilon} \\
\Delta\theta_t = -\frac{\eta}{RMS|g|_t} * g_t
\]</span></p>
<ul>
<li>RMS 均方根，作為 learning rate 的約束</li>
<li>仍然是人工固定的 learning rate</li>
</ul>
<h2 id="adam">Adam</h2>
<ul>
<li>就是帶有 Momentum 的 RMSprop</li>
<li>因為 m n 是變數，所以梯度可以動態調整</li>
</ul>
<p><span class="math display">\[
m_t = \mu * m_{t-1} + (1-\mu)*g_t \\
n_t = \mu * n_{t-1} + (1-v)*g_t \\
\hat{m}_t = \frac{m_t}{1-\mu^t} \\
\hat{n}_t = \frac{n_t}{1-v^t} \\
\Delta\theta_t = -\frac{\hat{m}_t}{\sqrt{\hat{n}_t} + \epsilon} * \eta
\]</span></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(八)：訓練小技巧 DDP 透過多機多卡來訓練模型</title>
    <url>/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%85%AB-%EF%BC%9A%E8%A8%93%E7%B7%B4%E5%B0%8F%E6%8A%80%E5%B7%A7-DDP-%E9%80%8F%E9%81%8E%E5%A4%9A%E6%A9%9F%E5%A4%9A%E5%8D%A1%E4%BE%86%E8%A8%93%E7%B7%B4%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<p>DDP 的全文是 Distributed Data
Parallel，是一種可以透過多機多卡來訓練模型的一種方法，它的本質上就是一個像
Map-Reduce 的東西，把訓練資料、Gradient、Loss 等資訊平均分配給每一個
GPU，達成多工處理的目的</p>
<p>DDP 也可以就看成，提高 batch-size 來提高網路效果</p>
<p>下面我們直接先來看 code 吧：</p>
<p>keywords: DDP <span id="more"></span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">################</span></span><br><span class="line"><span class="comment">## main.py文件</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 使用 DDP 最主要 import 的兩個包</span></span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"><span class="comment">### 1. 網路架構 (Module) ### </span></span><br><span class="line"><span class="comment"># 隨便設計的模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToyModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ToyModel, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">16</span> * <span class="number">5</span> * <span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 假設會用到的資料集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span>():</span></span><br><span class="line">    transform = torchvision.transforms.Compose([</span><br><span class="line">        torchvision.transforms.ToTensor(),</span><br><span class="line">        torchvision.transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>))</span><br><span class="line">    ])</span><br><span class="line">    my_trainset = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, </span><br><span class="line">        download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">    <span class="comment"># 我們要給 DataLoader 提供 DPP 的 sampler，使用下面的程式實現</span></span><br><span class="line">    train_sampler = torch.utils.data.distributed.DistributedSampler(my_trainset)</span><br><span class="line">    <span class="comment"># 在 DataLoader 中加入 sampler</span></span><br><span class="line">    <span class="comment"># 這裡的 batch_size 指的是一個 rank 中 (一個程序) 的 batch_size</span></span><br><span class="line">    <span class="comment"># 也就是說總 batch_size 是 batch_size x world_size (總程序數量)</span></span><br><span class="line">    trainloader = torch.utils.data.DataLoader(my_trainset, </span><br><span class="line">        batch_size=<span class="number">16</span>, num_workers=<span class="number">2</span>, sampler=train_sampler)</span><br><span class="line">    <span class="keyword">return</span> trainloader</span><br><span class="line">    </span><br><span class="line"><span class="comment">### 2. 初始化模型、數據、各種配置  ####</span></span><br><span class="line"><span class="comment"># 要從外面手動新增 local_rank 參數</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--local_rank&quot;</span>, default=-<span class="number">1</span>, <span class="built_in">type</span>=<span class="built_in">int</span>)</span><br><span class="line">FLAGS = parser.parse_args()</span><br><span class="line">local_rank = FLAGS.local_rank</span><br><span class="line"></span><br><span class="line"><span class="comment"># DDP backend 初使化</span></span><br><span class="line">torch.cuda.set_device(local_rank)</span><br><span class="line">dist.init_process_group(backend=<span class="string">&#x27;nccl&#x27;</span>)  <span class="comment"># nccl 由 Nvidia 用 C++ 寫的 Map-Reduce 後端</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 準備資料集</span></span><br><span class="line">trainloader = get_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立模型</span></span><br><span class="line">model = ToyModel().to(local_rank)</span><br><span class="line"><span class="comment"># 要 Load 預訓練的模型，需要在建立 DDP 模型之前，且只需要在 rank=0 (主要程序) 上 Load 就可以了</span></span><br><span class="line">ckpt_path = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> dist.get_rank() == <span class="number">0</span> <span class="keyword">and</span> ckpt_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    model.load_state_dict(torch.load(ckpt_path))</span><br><span class="line"><span class="comment"># 建立 DDP 模型 (這一句是精隨 XD)</span></span><br><span class="line">model = DDP(model, device_ids=[local_rank], output_device=local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要在建立 DDP 模型之後，才能設定 optimizer</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 設定 Loss function</span></span><br><span class="line">loss_func = nn.CrossEntropyLoss().to(local_rank)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 3. 網路訓練  ###</span></span><br><span class="line">model.train()</span><br><span class="line">iterator = tqdm(<span class="built_in">range</span>(<span class="number">100</span>))</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> iterator:</span><br><span class="line">    <span class="comment"># 設定 sampler 的 epoch</span></span><br><span class="line">    <span class="comment"># DistributedSampler 需要利用這個方式統一 shuffle</span></span><br><span class="line">    <span class="comment"># 使每個程序之間的亂數 seed 都是一樣的，使不同程序有相同的 shuffle 效果</span></span><br><span class="line">    trainloader.sampler.set_epoch(epoch)</span><br><span class="line">    <span class="comment"># 後面就與沒有用 DDP 的部份一樣了</span></span><br><span class="line">    <span class="keyword">for</span> data, label <span class="keyword">in</span> trainloader:</span><br><span class="line">        data, label = data.to(local_rank), label.to(local_rank)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        prediction = model(data)</span><br><span class="line">        loss = loss_func(prediction, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        iterator.desc = <span class="string">&quot;loss = %0.3f&quot;</span> % loss</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="comment"># DDP:</span></span><br><span class="line">    <span class="comment"># 與原相同，使用 torch.save torch.load 就可以了</span></span><br><span class="line">    <span class="comment"># 要只在 rank=0 上儲存，不然會存到很多遍</span></span><br><span class="line">    <span class="keyword">if</span> dist.get_rank() == <span class="number">0</span>:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&quot;%d.ckpt&quot;</span> % epoch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################</span></span><br><span class="line"><span class="comment">## 在 command line 中執行程式</span></span><br><span class="line"><span class="comment"># 使用 torch.distributed.launch 來啟動 DDP 模式</span></span><br><span class="line"><span class="comment"># 使用 CUDA_VISIBLE_DEVICES，來決定使用哪些 GPU</span></span><br><span class="line"><span class="comment"># CUDA_VISIBLE_DEVICES=&quot;0,1&quot; python -m torch.distributed.launch --nproc_per_node 2 main.py</span></span><br></pre></td></tr></table></figure>
<h2 id="ring-reduce">Ring-Reduce</h2>
<p>但是這種有關 Thread 的東西，就不得不請出我們的 Python GIL 啦，Python
GIL 是一個全區鎖，可以看成是使 python 多執行緒效果非常差的兇手</p>
<p>可見以下網站更詳細的講解：</p>
<p><a
href="http://cenalulu.github.io/python/gil-in-python/">http://cenalulu.github.io/python/gil-in-python/</a></p>
<p>而 DDP 為了減少 Python GIL 的限制，因而使用而 Ring-Reduce 架構來使
GPU 內互相溝通</p>
<p><img src="https://i.imgur.com/B3Sslcd.png" /></p>
<p>每個執行緒都只會接收來自上一個節點，並且把結果只丟給下一個節點，這種「圓圈圈」的做法可以大大減少互相通訊的複雜度
(如果假設是每個節點相互連接的話)</p>
<p>進一步詳細的做法可以參考下面的知乎大神：</p>
<p><a
href="https://zhuanlan.zhihu.com/p/69797852">https://zhuanlan.zhihu.com/p/69797852</a></p>
<h2 id="並行計算">並行計算</h2>
<p><img src="https://i.imgur.com/drp22sg.png" /></p>
<p>一般來說神經網路的並行模式有一下三種：</p>
<ol type="1">
<li><p>Data Parallelism</p>
<p>這是最常見的模式，換局話來說就是「增加 Batch-size」</p>
<p>DP DDP 剛剛講的那些 trick 都是屬於這一種的</p></li>
<li><p>Model Parallelism</p>
<p>把模型放在不同 GPU 上，是平行運算 (綠、黃)</p>
<p>看通訊效率，加速效果可能不明顯</p></li>
<li><p>Workload Partitioning</p>
<p>把模型放在不同 GPU 上，是串聯運算 (綠、藍)</p>
<p>不能加速</p></li>
</ol>
<h2 id="ddp-的一些基本名詞">DDP 的一些基本名詞</h2>
<ul>
<li><p>group</p>
<ul>
<li>程序組，一般只有一個組</li>
</ul></li>
<li><p>world size</p>
<ul>
<li>表示「全部」的程序總數</li>
<li>例如有 2 個 server ，每一台每面有 2 張 GPU，world size 為 2x2 =
4</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># world size 在不同程序中，得到的值都是相同的</span></span><br><span class="line">torch.distributed.get_world_size()</span><br></pre></td></tr></table></figure></li>
<li><p>rank</p>
<ul>
<li>表示目前的程序編號，0, 1, 2, 3, ...</li>
<li>其中 rank=0 代表 master 程序</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 每個程序有它自己的 rank 編號</span></span><br><span class="line">torch.distributed.get_rank()</span><br></pre></td></tr></table></figure></li>
<li><p>local rank</p>
<ul>
<li>同樣表示目前的程序編號，0, 1, 2, 3, ...</li>
<li>但特指「一個機器內的 GPU 編號」</li>
<li>(以 2 個 server ，每一台每面有 2 張 GPU
為例，rank：0~3，local_rank：0, 1, 0, 1)</li>
<li>目的是在執從 torch.distributed.launch 時，機器會自動去分配對應的
GPU</li>
</ul></li>
</ul>
<h2 id="ddp-原理">DDP 原理</h2>
<p>假設我們有 N 張 GPU</p>
<ul>
<li>減少 GIL 的限制
<ul>
<li>總共 N 張 GPU 就會有 N 個程序被啟動</li>
<li>每一個 GPU 都執行同一個模型，參數的數值一開始也是相同的</li>
</ul></li>
<li>Ring-Reduce 加速
<ul>
<li>在訓練模型時，使用 Ring-Reduce，彼此交換各自的梯度</li>
<li>藉此來得到所有運行程序中的梯度</li>
</ul></li>
<li>Data Parallelism
<ul>
<li>把每個程序的梯度平均後，各自做 backpropagation 更新權重值</li>
<li>因為各程序的初始參數、更新梯度是一樣的，所以更新後的參數值也是完全一樣的</li>
</ul></li>
</ul>
<h2 id="ddp-vs-gradient-accumulation">DDP vs Gradient Accumulation</h2>
<ul>
<li>上面有提到 DDP 其實也就是「增加 Batch Size」而已</li>
<li>而 Gradient Accumulation 也是變像的增加 Batch Size</li>
<li>那兩者有什麼差別呢？</li>
<li>效能上
<ul>
<li>在沒有 Buffer 參數 (像是 Batch Normalization)
下，理論效能是一樣的</li>
<li>程序數 8 的 DDP 與 Step 8 的 Gradient Accumulation 是一樣的</li>
<li>(因為 Buffer 參數，理論上要每兩步才更新一次，但因是每個 epoch
都會更新的緣故，BN 的分母會有對不上正確數字的問題)</li>
</ul></li>
<li>效率上
<ul>
<li>DDP 因使用平行化處理</li>
<li>會比 Gradient Accumulation 快超多</li>
</ul></li>
</ul>
<h2 id="ddp-調用方式">DDP 調用方式</h2>
<p>與原本使用 python3 main.py 的使用方不同，需要用
torch.distributed.launch 來啟動訓練</p>
<p>torch.distributed.launch 有幾個參數：</p>
<ul>
<li>--nnodes
<ul>
<li>有多少台機器</li>
</ul></li>
<li>--node_rank
<ul>
<li>目前是在哪個機器？</li>
</ul></li>
<li>--nproc_per_node
<ul>
<li>每個機器有多少個程序</li>
</ul></li>
<li>--master_address
<ul>
<li>master (rank=0) 的程序在哪一台 server 上</li>
</ul></li>
<li>--master_port
<ul>
<li>要用哪一個 port 進行通訊？</li>
</ul></li>
</ul>
<p>單機下的例子：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 假設只有一台機器</span></span><br><span class="line"><span class="comment"># 且一台機器內有 8 張 GPU</span></span><br><span class="line">python3 -m torch.distributed.launch --nproc_per_node 8 main.py</span><br></pre></td></tr></table></figure>
<p>多機下的例子：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 假設有兩台機器</span></span><br><span class="line"><span class="comment"># 且每一台機器內有 8 張 GPU</span></span><br><span class="line"><span class="comment"># 需每個機器都執行一次程式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 機器一</span></span><br><span class="line">python3 -m torch.distributed.launch --nnodes=2 --node_rank=0 --nproc_per_node 8 --master_adderss <span class="variable">$address</span> --master_port <span class="variable">$port</span> main.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 機器二</span></span><br><span class="line">python3 -m torch.distributed.launch --nnodes=2 --node_rank=1 --nproc_per_node 8 --master_adderss <span class="variable">$address</span> --master_port <span class="variable">$port</span> main.py</span><br></pre></td></tr></table></figure>
<p>如果我們要求只使用機器內特定的 GPU 呢？像是機器一共有 8
張卡，但只使用 4, 5, 6, 7</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=<span class="string">&quot;4,5,6,7&quot;</span> python -m torch.distributed.launch --nproc_per_node 4 main.py</span><br></pre></td></tr></table></figure>
<h2 id="reference">Reference</h2>
<p><a
href="https://zhuanlan.zhihu.com/p/178402798">DDP系列第一篇：入门教程</a></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(五)：網路一層模型 Parameter vs Buffer</title>
    <url>/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E4%BA%94-%EF%BC%9A%E7%B6%B2%E8%B7%AF%E4%B8%80%E5%B1%A4%E6%A8%A1%E5%9E%8B-Parameter-vs-Buffer/</url>
    <content><![CDATA[<p>有時候我們在看別人的論文時會發現：常常會有一些「超參數」的出現，像是
ResNet shortcut 進入的權重值等等</p>
<p>這個時候就可以用 Pytorch 提供的 Parameter 和 buffer
來實作，想知道詳細差在哪裡就繼續往下看吧 ~</p>
<p>keywords: Parameter、buffer <span id="more"></span></p>
<h2 id="parameter-和-buffer">Parameter 和 buffer</h2>
<p>有時候我們想要在網路中新增一層或是一個參數時，就可以使用 Parameter
或是 buffer</p>
<ul>
<li>Parameter 在反向傳播時「會」隨著網路更新權重值</li>
<li>Buffer 在反向傳播時「不會」隨著網硬更新權重值</li>
</ul>
<p>建立方向：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        buffer = torch.randn(<span class="number">2</span>, <span class="number">3</span>)  <span class="comment"># tensor</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;my_buffer&#x27;</span>, buffer)     <span class="comment"># buffer 的定義方式 (str：定義名字，tensor：傳入權重)</span></span><br><span class="line">        self.param = nn.Parameter(torch.randn(<span class="number">3</span>, <span class="number">3</span>))  <span class="comment"># Parameter 的定義方式 (tensor)</span></span><br><span class="line">        self.register_parameter(<span class="string">&quot;param&quot;</span>, param)       <span class="comment"># 另一種定義 Parameter 的方式 (與上行程式等價)，看你習慣，好處是可自定義名稱</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># 可以通过 self.param 和 self.my_buffer 访问</span></span><br><span class="line">        self.my_buffer(x)     <span class="comment"># 使用剛剛定義的 str 名字</span></span><br><span class="line">        self.param(x)	</span><br></pre></td></tr></table></figure>
<p>兩者的共同點就是，在使用 <code>model.state_dict()</code>
的方法來保存、讀取網路模型時，都會被存入到 OrderDict 中</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">model = MyModel(*args, **kwargs)</span><br><span class="line">model.load_state_dict(torch.load(PATH))</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># get buffer</span></span><br><span class="line">model = MyModel()</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># get param</span></span><br><span class="line"><span class="keyword">for</span> buffer <span class="keyword">in</span> model.buffers():</span><br><span class="line">    <span class="built_in">print</span>(buffer)</span><br></pre></td></tr></table></figure>
<p>在 ViT 的 Patch Embedding 中有使用到，用在 reletive positional
encoding 上，因為相對位置編碼不會隨著網路而更新</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.embs = nn.Embedding(vocab_size, d_model) <span class="comment"># word embedding， 需要 backprop 更新</span></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pe shape: (0, max_len, d_model)</span></span><br><span class="line">        pe = self._build_position_encoding(max_len, d_model)  </span><br><span class="line">        self.register_buffer(<span class="string">&quot;pe&quot;</span>, pe)  <span class="comment"># position encoding，不需 backprop 更新</span></span><br></pre></td></tr></table></figure>
<h3 id="reference">reference</h3>
<p><a
href="https://zhuanlan.zhihu.com/p/89442276">https://zhuanlan.zhihu.com/p/89442276</a></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(十一)：Pytorch 如何實驗 Backpropagation 之 Pytorch AutoGrad 幫我們做了什麼事？</title>
    <url>/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%B8%80-%EF%BC%9APytorch-%E5%A6%82%E4%BD%95%E5%AF%A6%E9%A9%97-Backpropagation-%E4%B9%8B-Pytorch-AutoGrad-%E5%B9%AB%E6%88%91%E5%80%91%E5%81%9A%E4%BA%86%E4%BB%80%E9%BA%BC%E4%BA%8B%EF%BC%9F/</url>
    <content><![CDATA[<p>本文接續著上一篇 [你所不知道的 Pytorch 大補包(十)：Pytorch 如何實做出
Backpropagation 之什麼是 Backpropagation] 繼續更深入的了解 Pytorch
的底層</p>
<p>keywords: AutoGrad <span id="more"></span></p>
<h2 id="pytorch-autograd-幫我們做了什麼事">pytorch AutoGrad
幫我們做了什麼事？</h2>
<p>以上我們成功的用 numpy
手刻了一個超~簡單的神經網路出來，並且訓練它，還取得了 100%
正確率的成果，但剛剛 Foward 的函式很簡單，簡單到它的梯度甚至不用到 chain
rule 就算得出來，如果今天是一個 100
層深的網路，那我們的算式就會變得超長，跟本沒有辨法像剛剛直接用一條算式表達出來</p>
<p>這個時候幫我們實作 Backpropagation 的 pytorch 的派上用場了，pytorch
使用 AutoGrad 自動的幫我們把所有梯度都算出來，並且也做完
Backpropagation，在解釋一切程式碼之前，我們再來回頭看看用 pytorch
寫出來的程式會多麼的簡潔</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義：輸入 x=2，目標 y=4，變量 w=0</span></span><br><span class="line">x = torch.tensor([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>], dtype=torch.float32)</span><br><span class="line">y = torch.tensor([<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 這個後面有 requires_grad 等等會介紹</span></span><br><span class="line">w = torch.tensor(<span class="number">0</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> w * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義使用 SGD 作用最佳化演算法</span></span><br><span class="line">optimizer = torch.optim.SGD([w], lr=lr)</span><br><span class="line"><span class="comment"># 定義 MSE Loss </span></span><br><span class="line">loss = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># (1) Foward Pass 前傳導</span></span><br><span class="line">    y_hat = foward(x)</span><br><span class="line">    <span class="comment"># (1.5) 計算 Loss</span></span><br><span class="line">    l = loss(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (2, 3) 計算 Local Gradient 以及 Backpropagation</span></span><br><span class="line">    l.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (4) 更新權重 w</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># (4.1) 淨空 dw (Gradient) 值</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, w: <span class="subst">&#123;w:<span class="number">.3</span>f&#125;</span>, loss: <span class="subst">&#123;l:<span class="number">.8</span>f&#125;</span>, w.grad: <span class="subst">&#123;w.grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="什麼是-requires_grad">什麼是 requires_grad？</h3>
<p>一般在建立一個新的 tensor 時，我們會使用 <code>torch.tensor</code>
來達成，但是如果 tensor
想要實作自動計算梯度的話，我們必需在後面加一個參數 requires_grad</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>], dtype=torch.float32)</span><br><span class="line"><span class="comment"># tensor([1., 2., 3., 4.])</span></span><br><span class="line"></span><br><span class="line">w = torch.tensor(<span class="number">0</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># tensor(0., requires_grad=True)    &lt;- 這裡多一個屬性</span></span><br></pre></td></tr></table></figure>
<p>打開這個 requires_grad 屬性後，pytorch
會幫我們打開更多的屬性，而這些屬性是只有在做 Backpropagation
時才會用到的，所以平常把它關起來減少記憶體的消耗。還記得前面有提到訓練網路的四大步驟嗎？等等會依照這四個步驟的順序來介紹</p>
<h3 id="foward-pass-前傳導">Foward Pass 前傳導</h3>
<p>前傳導其實就是由一堆運算式所組成，輸入一個值，經過這個複雜的運算式後得到一個結果就稱為
Foward Pass 前傳導，那如果我們在前傳導的式子中加入 required_grad
會發生什麼事情呢？以下用兩個程式來對比</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line">c = a * b</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/NuB11Wn.png"
alt="PyTorch Autograd-A 1.drawio" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Foward Pass</span></span><br><span class="line">c = a * b</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/SvpuwfT.png"
alt="PyTorch Autograd-A 2.drawio" /></p>
<p>可以由上圖分析出幾個重點：</p>
<ol type="1">
<li>打開 requires_grad 後，tensor 的所有運算操作都會畫成一個 Graph</li>
<li>一個運算當中只要有一個變量 requires_grad=True，未來運算所新增的
tensor 一樣會是 requires_grad=True</li>
<li>有三個新的屬性：grad、grad_fn、is_leaf</li>
</ol>
<ul>
<li><p>grad 值在前傳導時為 None，要等到做 Backpropagation
時才會把值填上去</p></li>
<li><p>grad_fn 是在前傳導時 pytorch
自動幫我們加上去的，意思是「對應運算符微分後的算式」pytorch
提供了一大堆的 grad_fn 以應付各種微分運算，加速 Backpropagation
的流程</p></li>
<li><p>is_leaf
為了要表示這個權重節點是不是在葉節點上，為什麼這個這麼重要呢？因為
<strong>pytorch 只會在葉結點上儲存 grad
資訊</strong>，目的是為了保留記憶體</p></li>
</ul>
<h3 id="backpropagation">Backpropagation</h3>
<p>前傳導完，記錄了許多數值後，接著就利用這個數值來做
Backpropagation，程式以及對應的 Graph 如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Forward Pass </span></span><br><span class="line">c = a * b</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">c.backward()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/YlfmW2o.png"
alt="PyTorch Autograd-A 4.drawio" /></p>
<p>可以看到，簡簡單單的一行 <code>c.backward()</code> pytorch
竟然幫我們做了這麼多事情…。宏觀上來看這一行指令幫我們算出來 a 的 grad 值
3，微觀上來看這一行指令幫我們畫了超多的圖…同時也吃掉了不少記憶體</p>
<p>首先當呼叫 <code>c.backward()</code> 時，pytroch 會先去尋找
grad_fn，接著根據 grad_fn 裡面的微分運算計算 grad，然後放進
AccumulateGrad，累計不同次運算的 grad，最後再放到對應節點權重的 grad
屬性中</p>
<p>以數字的例子為：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">假設一開始初始值為 1.0 -&gt;</span><br><span class="line">找到 grad_fn 為 MulBackward -&gt;</span><br><span class="line">計算 dc/da 的偏微分 (dc/db 因為 requires_grad=False 所以不參與計算) -&gt;</span><br><span class="line">dc/da = d(a*b)/da = b = 3.0 -&gt;</span><br><span class="line">用一個累計暫存器存起來 -&gt;</span><br><span class="line">放到 a.grad 中</span><br></pre></td></tr></table></figure>
<p>再用一個更複雜的例子來舉例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 假設 pytorch 的運算變成這個樣子：</span></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Foward Pass </span></span><br><span class="line">c = a * b</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 再定義一個 d</span></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再多一個 Foward Pass</span></span><br><span class="line">e = c * d</span><br><span class="line"></span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">e.backward()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.imgur.com/2DCyMTA.png"
alt="PyTorch Autograd-Simple 5.drawio" /></p>
<p>覺得圖片變太複雜嗎 XD，沒關系我們一起從最下面慢慢算上去：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">假設一開始初始值為 1.0 -&gt;</span><br><span class="line">找到 grad_fn 為 MulBackward -&gt;</span><br><span class="line">計算 de/dc、de/dd 的偏微分 -&gt;</span><br><span class="line">de/dc = d(c*d)/dc = d = 6.0 -&gt;</span><br><span class="line">de/dd = d(c*d)/dd = c = 4.0 -&gt;</span><br><span class="line"></span><br><span class="line">de/dc 因為 c 不是葉結點，所以不用寫回 c.grad，直接把值傳給 c.grad_fn，繼續做下一個偏微分 -&gt;</span><br><span class="line">de/dd 因為 d 是葉結點，用一個累計暫存器存起來，再把結果寫回 d.grad -&gt;</span><br><span class="line"></span><br><span class="line">計算 dc/da 的偏微分 (dc/db 因為 requires_grad=False 所以不參與計算) -&gt;</span><br><span class="line">dc/da = d(a*b)/da = b = 3.0 -&gt;</span><br><span class="line">再乘上傳進來的 4，4*3 = 12</span><br><span class="line">用一個累計暫存器存起來 -&gt;</span><br><span class="line">放到 a.grad 中</span><br></pre></td></tr></table></figure>
<p>我們可以印印看結果是不是正如我們所計算的？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 假設 pytorch 的運算變成這個樣子：</span></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Foward Pass </span></span><br><span class="line">c = a * b</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 再定義一個 d</span></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再多一個 Foward Pass</span></span><br><span class="line">e = c * d</span><br><span class="line"></span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">e.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(a.grad)</span><br><span class="line"><span class="built_in">print</span>(c.grad)</span><br><span class="line"><span class="built_in">print</span>(d.grad)</span><br><span class="line"><span class="built_in">print</span>(e.grad)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a.grad -&gt; 12.0</span><br><span class="line">/opt/conda/lib/python3.7/site-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won&#x27;t be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /opt/conda/conda-bld/pytorch_1656352464346/work/build/aten/src/ATen/core/TensorBody.h:477.)</span><br><span class="line">  return self._grad</span><br><span class="line">c.grad -&gt; None</span><br><span class="line">d.grad -&gt; 6.0</span><br><span class="line">e.grad -&gt; None</span><br></pre></td></tr></table></figure>
<p>咦…怎麼跟想像中的答案不一樣…，<code>a.grad</code> <code>d.grad</code>
都是對的，<code>c.grad</code> <code>e.grad</code>
發生了什麼事…？其實剛剛也有提到，在 pytorch 中，<strong>只有葉結點
(is_leaf=True) 才會把 .grad
存起來</strong>，目的是為了節省不必要的記憶體，而且在圖中也可看到，資料流動
flow 如果不是指向葉結點，會直接把值放到下一個 grad_fn 中，不會有
AccumulateGrad 把值存到 .grad 中，因此 pytorch
才會跳提醒說這個操作不合理，並且回傳 None</p>
<p>那如果我們真的真的想要得到不是葉結點的 .grad
值呢？那我們就要在<strong>前傳導的時候先把它註冊下來</strong>，使用
<code>retain_grad()</code> 這個函式：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假設 pytorch 的運算變成這個樣子：</span></span><br><span class="line">a = torch.tensor(<span class="number">2.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b = torch.tensor(<span class="number">3.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Foward Pass </span></span><br><span class="line">c = a * b</span><br><span class="line"><span class="comment"># 用 .retain_grad() 來註冊，告訴 pytorch 要把這個值存起來</span></span><br><span class="line">c.retain_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再定義一個 d</span></span><br><span class="line">d = torch.tensor(<span class="number">4.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 再多一個 Foward Pass</span></span><br><span class="line">e = c * d</span><br><span class="line"><span class="comment"># 用 .retain_grad() 來註冊，告訴 pytorch 要把這個值存起來</span></span><br><span class="line">e.retain_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Backpropagation</span></span><br><span class="line">e.backward()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;a.grad -&gt; <span class="subst">&#123;a.grad&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;c.grad -&gt; <span class="subst">&#123;c.grad&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;d.grad -&gt; <span class="subst">&#123;d.grad&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;e.grad -&gt; <span class="subst">&#123;e.grad&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>詳細 <code>.retain_grad()</code>
的說明可以看下面這個影片，裡面很詳細的介紹為什麼這樣就可以，以及一個新概念：hook
的用法：<a
href="https://www.youtube.com/watch?v=syLFCVYua6Q">https://www.youtube.com/watch?v=syLFCVYua6Q</a></p>
<p>另外也可以發現，在圖中有一個 AccumulateGrad
的方塊，這是用來儲存每一次 Backpropagation 的結果，並把新算出來的 grad
與之前的相加存到 .grad 中間，也就是說它不會自動淨空！</p>
<p>所以通常在程式中我們會手動淨空 .grad 值，以確保每一次訓練時 .grad
都是最新的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># (1) Foward Pass 前傳導</span></span><br><span class="line">    y_hat = foward(x)</span><br><span class="line">    <span class="comment"># (1.5) 計算 Loss</span></span><br><span class="line">    l = loss(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (2, 3) 計算 Local Gradient 以及 Backpropagation</span></span><br><span class="line">    l.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (4) 更新權重 w</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment"># (4.1) 淨空 dw (Gradient) 值    &lt;- 如果不清空 .grad 會累加！</span></span><br><span class="line">    optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>至於為什麼要這樣設計，因為如果我們使用的設備記憶體不足，沒辨法一次結太多資料訓練，我們就可以使用
Gradient accumulation 的技巧，改成每訓練兩次更新一次參數，變向放大 Batch
size</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i,(image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="comment"># 1. input output</span></span><br><span class="line">    pred = model(image)</span><br><span class="line">    loss = criterion(pred, label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.1 loss 要除以累積的總步數，正規化 loss 的值</span></span><br><span class="line">    loss = loss / accumulation_steps  </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 2.2 計算梯度的值</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 當累積的步數到一定的程度後，梯度中的值也會不斷累加，才會更新網路的參數</span></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % accumulation_steps == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># optimizer the net</span></span><br><span class="line">        optimizer.step()        <span class="comment"># 更新網路參數</span></span><br><span class="line">        optimizer.zero_grad()   <span class="comment"># 清空以前的梯度</span></span><br></pre></td></tr></table></figure>
<h3 id="要怎麼去掉-requires_grad">要怎麼去掉 requires_grad？</h3>
<p>從上面的圖來看，只要我們在 tensor 中加入 require_grad 參數，pytorch
就會一直記錄追縱未來所有的運算，一直更新那一張大圖，記憶體開銷非常可觀，那我們要怎麼樣把圖中藍藍的那一堆
Backpropagation 專用的圖給去掉呢？一共有三個做法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># (1) x.requires_grad_(False)</span></span><br><span class="line"><span class="comment"># 這個程式可以 inplace 把 x 的 requires_grad 拿掉</span></span><br><span class="line"><span class="comment"># 在 pytorch 中，所有 xxx_ &lt;- 這個底線的意思代表 inplace 操作的意思，不會回傳任何值</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor(3., requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">x.requires_grad_(<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor(3.)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># (2) x.detach()</span></span><br><span class="line"><span class="comment"># 這個程式一樣可以把 .requires_grad 去掉</span></span><br><span class="line"><span class="comment"># 但是它會建新一個新的且不帶 requires_grad 的 tensor，並回傳</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor(3., requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">y = x.detach()</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(3.)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># (3) torch.no_grad()</span></span><br><span class="line"><span class="comment"># 會搭配 with 一起使用，也是最常使用的一個方法</span></span><br><span class="line"><span class="comment"># 在 with 的縮排範圍內，任何 tensor 都不帶 .requires_grad</span></span><br><span class="line"></span><br><span class="line">x = torch.tensor(<span class="number">3.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment"># tensor(3., requires_grad=True)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  y = x + <span class="number">2</span></span><br><span class="line">	<span class="built_in">print</span>(y)</span><br><span class="line"><span class="comment"># tensor(5.)</span></span><br></pre></td></tr></table></figure>
<p>最後一個 <code>torch.no_grad()</code>
最常看到，通常會在驗證、測試的程式碼中會出現，理由有兩個。</p>
<ul>
<li>一、結省記憶體。有時候 nvidia 會噴記憶體不夠，不一定是 Batch size
設太大的問題，也有可能是驗證、測試時忘記寫到
<code>torch.no_grad()</code> 把那一大堆 Backpropagation
的圖都載入了</li>
<li>二、不會更新參數。因為 <code>torch.no_grad()</code> 把所有
Backpropagation
剛掉了，排除了所有會更新到參數的因素，因此可以放心的驗證、測試，而不用擔心動到網路的參數</li>
</ul>
<p>以上就是全部的內容了！希望看完這篇文章可以更了解 pytorch
倒底背後幫我們做了什麼事情！</p>
<h3 id="reference">Reference</h3>
<p>本篇文章大量參考了以下兩個 youtube：</p>
<p><a href="https://www.youtube.com/watch?v=DbeIqrwb_dE">(系列教學影片)
PyTorch Tutorial 03 - Gradient Calculation With Autograd</a></p>
<p><a href="https://www.youtube.com/watch?v=3Kb0QS6z7WA">(系列教學影片)
PyTorch Tutorial 04 - Backpropagation - Theory With Example</a></p>
<p><a href="https://www.youtube.com/watch?v=E-I2DNVzQLg">(系列教學影片)
PyTorch Tutorial 05 - Gradient Descent with Autograd and
Backpropagation</a></p>
<p><a href="https://www.youtube.com/watch?v=MswxJw-8PvE">PyTorch
Autograd Explained - In-depth Tutorial</a></p>
<p>官方 Document 永遠是你最好的朋友</p>
<p><a
href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">A
GENTLE INTRODUCTION TO <code>TORCH.AUTOGRAD</code></a></p>
<p><a href="https://ithelp.ithome.com.tw/articles/10216440">(iT邦幫忙)
Day 2 動態計算圖：PyTorch's autograd (裡面有提到 in-place
的問題)</a></p>
<p><a
href="https://blog.csdn.net/weixin_41417982/article/details/81393917">(Backpropagation
參考文章) 神经网络的传播（权重更新）</a></p>
<p><a
href="https://discuss.pytorch.org/t/why-cant-i-see-grad-of-an-intermediate-variable/94">(為什麼我看不到
.grad？) Why cant I see .grad of an intermediate variable?</a></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(六)：訓練小技巧 Gradient accumulation</title>
    <url>/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%85%AD-%EF%BC%9A%E8%A8%93%E7%B7%B4%E5%B0%8F%E6%8A%80%E5%B7%A7-Gradient-accumulation/</url>
    <content><![CDATA[<p>你是不是常常覺得 GPU
顯存不夠用？是不是覺得自己太窮買不起好的顯卡、也租不起好的機台？覺得常常因為東卡西卡關就放棄深度學習？</p>
<p>沒關系！接下來有幾招可以在預算不太足的情況下，還是可以讓訓練跑得起來！</p>
<p>keywords: Gradient accumulation <span id="more"></span></p>
<h2 id="gradient-accumulation">Gradient accumulation</h2>
<p>第一招叫做 Gradient accumulation</p>
<p>這是利用 pytorch 每一次在 backpropagation 前都會把梯度清零</p>
<p>正常一個訓練部份程式會這樣寫：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i, (image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="comment"># 1. input output</span></span><br><span class="line">    pred = model(image)</span><br><span class="line">    loss = criterion(pred, label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. backward</span></span><br><span class="line">    optimizer.zero_grad()   <span class="comment"># 把梯度清零</span></span><br><span class="line">    loss.backward()					<span class="comment"># backpropagation 計算當前的梯度</span></span><br><span class="line">    optimizer.step()        <span class="comment"># 拫據梯度更新網路參數</span></span><br></pre></td></tr></table></figure>
<p>而 Gradient accumulation 會這麼寫</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i,(image, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">    <span class="comment"># 1. input output</span></span><br><span class="line">    pred = model(image)</span><br><span class="line">    loss = criterion(pred, label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.1 loss 要除以累積的總步數，正規化 loss 的值</span></span><br><span class="line">    loss = loss / accumulation_steps  </span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 2.2 計算梯度的值</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 當累積的步數到一定的程度後，梯度中的值也會不斷累加，才會更新網路的參數</span></span><br><span class="line">    <span class="keyword">if</span> (i+<span class="number">1</span>) % accumulation_steps == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># optimizer the net</span></span><br><span class="line">        optimizer.step()        <span class="comment"># 更新網路參數</span></span><br><span class="line">        optimizer.zero_grad()   <span class="comment"># 清空以前的梯度</span></span><br></pre></td></tr></table></figure>
<p>這樣子這的理由是，每次的 epoch
不把梯度清零，而是用累加到一定的程度後才會更新網路參數值</p>
<p>目的是要在不增加 RAM 的條件下，變向增加 batch size (窮人做法阿
QQ)</p>
<p>有兩個要注意的小地方：</p>
<ol type="1">
<li><p>learning rate</p>
<p>因為變每兩步為一個單位計算梯度了，而 learning rate
的設定依舊是以一步為一個單位</p>
<p>所以要適當的調大一些些</p></li>
<li><p>batch normalization</p>
<p>原理同上</p>
<p>BN 的分母為全部 Batch szie 的值，但因單位的改變，而 BN 卻沒跟上</p>
<p>所以 BN 的分母的值並非全部的 batch size
(但根據下面文章好像又沒差多少…我不是很清楚 XD)</p>
<p>但可確認的是，效果一定比單純的增加 batch size 來的差一些些</p>
<p>或是可以調低 BN 的 momentum 參數</p></li>
</ol>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(十三)：我可以在 optimizer 中動態的調整學習率嗎？- RMSProp、AdaGrad</title>
    <url>/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%B8%89-%EF%BC%9A%E6%88%91%E5%8F%AF%E4%BB%A5%E5%9C%A8-optimizer-%E4%B8%AD%E5%8B%95%E6%85%8B%E7%9A%84%E8%AA%BF%E6%95%B4%E5%AD%B8%E7%BF%92%E7%8E%87%E5%97%8E%EF%BC%9F-RMSProp%E3%80%81AdaGrad/</url>
    <content><![CDATA[<p>在上章我們介紹了 SGD 與
Momentum，接下來進一步介紹可以自己調整學習率的 RMSProp 與 AdaGrad</p>
<p>keywords: RMSProp、AdaGrad <span id="more"></span></p>
<h2 id="為什麼要自己調整學習率">為什麼要自己調整學習率？</h2>
<p>剛剛加入 Momentum 的 SGD
似乎看起來很完美，收斂又快，又有跨過小山丘的能力，那…還有什麼地方可以改進的呢…？</p>
<p>我們一起來看下面這張圖，來源自：<a
href="https://www.jeremyjordan.me/nn-learning-rate/">Setting the
learning rate of your neural network.</a></p>
<p><img src="https://i.imgur.com/SODSFGm.png" alt="Image" /></p>
<p>假設我們的網路是一個類似二次多項式的曲線</p>
<blockquote>
<p>當我們學習率設太小：收斂太慢 (左圖)</p>
<p>學習率設太大：完全找不到最低點，一直跳來跳去 (右圖)</p>
<p>學習率設得剛剛好：完美！(中圖)</p>
</blockquote>
<p>可見如何選擇學習率是一個重要的課題，其影響程度甚至可以使你的網路永遠不收斂，效果就是比別人差。</p>
<p>那倒底要選擇多大的學習率呢？答案是：我也不知道…，每一個網路有著他自己的特性，所以每個網路最佳的學習率都不太一樣，所以最好的做法就是：學習率不是固定的！而是一個從大慢慢變小的過程。</p>
<p>由剛剛的圖可以得知通常網路在訓練初期梯度較大因此可以設較大的學習率，而隨著網路訓練慢慢的收斂，學習率也要隨之調整變小，以適應較緩的梯度。</p>
<p>而至於從什麼時候開始變小，則是根據網路自己的權重來動態的決定，自己決定自己的學習率最合理，人為定的都猜不準</p>
<p>以下兩個優化器就是按著這個思維來設計，希望可以利用網路自身的權重值來自己決定學習率要如何變小。</p>
<h2 id="最簡單的想法">最簡單的想法</h2>
<p>介紹前我們先來看最最簡單的想法，由此出發，更能體會到下面的優化器想要解決什麼事情</p>
<p><strong>學習率會隨著 epoch
增加而變小</strong>，這是核心中的核心概念，那既然是跟時間有關，我們可以把學習率與時間成反比就好了呀，可以得到下面的公式：</p>
<p><span class="math display">\[
w_{t+1} = w_t + \frac{\eta}{t} \nabla g
\]</span></p>
<p>直接把學習率除以時間
t，這樣學習率就會隨著時間慢慢的變小了！不過這樣子真的就好了嗎？式子中的時間
t
好像跟網路一點關聯也沒有，不同的網路學習率的變化基本是一模一樣，所以剛剛的那一句要稍微改一下</p>
<p><strong>學習率會根據網路權重且隨著 epoch 增加而變小</strong>，AdaGrad
及 RMSProp 就是在討論網路權重對於學習率的影響。以下介紹兩種優化器</p>
<h2 id="adagrad">AdaGrad</h2>
<p>AdaGrad 全名 Adaptive
Gradient，其想法是在網路初期干預不多因此學習率大；網路後期干預多因此學習率小。</p>
<p>公式如下：</p>
<p><span class="math display">\[
\begin{gather}   
w_{t+1} = w_t - \frac{\eta}{\sigma_t} \nabla g\\
\sigma_t = \sqrt{G_t+\epsilon}\\
G_t = \sum^t_{n=1}g_n^2
\end{gather}
\]</span></p>
<p><span class="math inline">\(G_t\)</span> 代表權重值，累積到第 t
時刻的梯度平方和，<span class="math inline">\(\epsilon\)</span> 是平滑項
(smooth term) 用於避免 <span class="math inline">\(\sigma\)</span> 為 0
否則會除 0，一般設為 <span class="math inline">\(10^{-8}\)</span></p>
<p>AdaGrad 使用<strong>網路加權到 t
時刻的權重平方和</strong>來做為除以學習率的分母，因為會隨時間加權的原因，學習率這一項會越來越小，直到接近
0。也可理解為網路越後期優化器干預的越多，學習率因此降低</p>
<p>AdaGrad
的優點是不需人工調整學習率；而缺點是收斂到最後，調整多，學習率幾乎降為
0，而無法再改進參數值</p>
<p>在 Pytorch 中 AdaGrad
可以很方便的直接呼叫函式庫就可以囉，基本上沒有什麼超參數要特別調</p>
<p><img src="https://i.imgur.com/65ZEpjx.png" alt="Image" /></p>
<h2 id="rmsprop">RMSProp</h2>
<p>RMSProp 是 Hinton
教授在上課的講義中提定的一個優化器，並沒有正式發表在論文當中。</p>
<p>公式如下：</p>
<p><span class="math display">\[
\begin{gather}   
w_{t+1} = w_t - \frac{\eta}{\sigma_t} \nabla g\\
\sigma_t = \sqrt{\alpha(\sigma_{t-1})^2+(1-\alpha)g_t^2+\epsilon}
\end{gather}
\]</span></p>
<p>RMSProp 與 AdaGrad 基本上差不多都是學習率 <span
class="math inline">\(\eta\)</span> 除上一個由權重決定的分母 <span
class="math inline">\(\sigma\)</span>，<span
class="math inline">\(\sigma\)</span>
同樣是由當前梯度平方來決定，但是多了一個超參數 <span
class="math inline">\(\alpha\)</span></p>
<p>分母的意思為：除了加總當前梯度平方和之外，也考慮前一個時刻的梯度平方和</p>
<p>實作上 <span class="math inline">\(\alpha\)</span> 會設為
0.9，代表當網路後期，優化器干預學習率越多時，偏好使用舊梯度做平方和運算</p>
<p>這樣做相比於 AdaGrad 計算 1 ~ t
時刻的梯度平方和，每一個時刻的權重值都會加起來，RMSProp 因設定 <span
class="math inline">\(\alpha=0.9\)</span>
偏好使用舊梯度，做到類似加權平均的概念，可以避免 <span
class="math inline">\(\sigma\)</span> 值過大的問題。</p>
<p>同時 RMSProp 這個概念也很像動量
Momentum，在更新權重前除了當前的權重值外也考量前一時刻的權重值，使得
RMSProp 相比 AdaGrad 在梯度曲面較複雜的情況也有著比較好的表現。</p>
<p>在 Pytorch 上有 RMSProp 的實作函式：其中 alpha 參數預設 0.99
代表高度依靠歷史梯度來更新參數</p>
<p><img src="https://i.imgur.com/zoCLYKc.png" alt="Image" /></p>
<h2 id="reference">Reference</h2>
<p><a href="https://www.jeremyjordan.me/nn-learning-rate/">推！。Setting
the learning rate of your neural network.</a></p>
<p><a
href="https://hackmd.io/@allen108108/H1l4zqtp4">Adagrad、RMSprop、Momentum
and Adam – 特殊的學習率調整方式</a></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(十)：Pytorch 如何實做出 Backpropagation 之什麼是 Backpropagation</title>
    <url>/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81-%EF%BC%9APytorch-%E5%A6%82%E4%BD%95%E5%AF%A6%E9%A9%97-Backpropagation-%E4%B9%8B%E4%BB%80%E9%BA%BC%E6%98%AF-Backpropagation/</url>
    <content><![CDATA[<p>常常我們初學 pytroch 的時候都一定會看過下面的程式碼：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">  output = model(dataset)</span><br><span class="line">  loss = criterion(output, target)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># wtf</span></span><br><span class="line">  optimizer = zero_grad()</span><br><span class="line">  loss.backward()</span><br><span class="line">  optimizer.step()</span><br></pre></td></tr></table></figure>
<p>好不容易跨出第一步，並剛接觸程式碼的你，一看到這坨鬼東西一定心裡有三個問號…(至少我是這樣啦哈哈。</p>
<p>keywords: Backpropagation <span id="more"></span></p>
<p>而大部份網路上的教學都會強調：這個就是 Backpropagation
喔！也不用太了解它，知道在寫程式時記得要加上它就好了！…</p>
<p>更進階一點，你是從學校修神經網路相關的課程，也知道 Backpropagation
背後的數學原理，甚至還用 mathlab python 手刻了一個陽春
Backpropagation，只是當你轉換到 pytorch
上來看到程式碼時，不禁覺得…這程式也太簡潔了吧…，只要一行
<code>loss.backward()</code> 就可以了，這真的可靠嗎？</p>
<p>而這篇文章就會從最一開始的脈絡，來慢慢解釋：什麼是
Backpropagation、要怎麼用程式來實作 Backpropagation、pytorch
倒底幫我們做了什麼？不管你是初心者或是小有經驗的開發者，這些底層冷知識可以幫助你加深對
pytorch 的感情喔！</p>
<h3 id="什麼是-backpropagation">什麼是 Backpropagation？</h3>
<p>在理解什麼是 Backpropagation
之前，先來複習一下訓練一個神經網路一定要經過的四個步驟：</p>
<ol type="1">
<li>Forward Pass 前傳導</li>
<li>Calculate Gradient 計算梯度</li>
<li>Backpropagation 後傳導</li>
<li>Weight update 權重更新</li>
</ol>
<p>用一個非常非常簡單的例子來舉例，假設我們要訓練一個可以把輸入資料都都
x2 的網路，並且定義以下參數</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># x 定義為輸入資料 = 2</span></span><br><span class="line">x = torch.tensor(<span class="number">2</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y 定義為目標 Ground Truth </span></span><br><span class="line"><span class="comment"># y = x*2 = 2*2 = 4</span></span><br><span class="line">x = torch.tensor(<span class="number">4</span>, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># w 定義為網路中的一個權重值，初始為 0</span></span><br><span class="line">w = torch.tensor(<span class="number">0</span>, dtype=torch.float32)</span><br></pre></td></tr></table></figure>
<p>把上面的文字及變數定義換成更白話一點的說法就是：我們今天有一個式子
<code>w * x = y</code> 要找到一個適合的 w 值，使得
<code>2x = y</code>。</p>
<p>畫成樹狀圖可以長成如下：</p>
<p><img src="https://i.imgur.com/Y7vRHHO.png"
alt="未命名绘图.drawio" /></p>
<p>當然在現階段我們可以清楚的一看就知道 <code>w = 2</code>
就是答案了，只是在一般的深度學習中，x
的多項式可是會達到幾千甚至幾萬的維度，跟本不能用多項式求解的方式來知道答案。那該怎麼辨呢？就使用漸近求解的方式吧！因此才會多一個計算
Loss 的步驟，Loss
可以得知，網路輸出的結果與真實的結果倒底相差多遠，透過這個相差多遠的資訊，可以進一步得知網路是否有在往正確的方向學習。</p>
<p>我們可以把最後的結果套上 Mean Square Error
(均方誤差)，就是一個相減後平方的公式：<span
class="math inline">\(\mathcal{L}=(\hat y -y)^2\)</span></p>
<p>例如當 <code>w = 1</code> 時，我們算出來的 Loss 為 <span
class="math inline">\((1-2)^2=1\)</span>，可解讀為我們離正確解答的距離還有
1 (單位)，而隨著 w 的數值越來越接近 2，Loss 也會越來越小，直到趨近於
0。</p>
<p>因為我們在網路中加入 Loss，對應的運算樹狀圖也要修改如下：</p>
<p><img src="https://i.imgur.com/vY3QdQ8.png"
alt="未命名绘图.drawio" /></p>
<p>接著我們實際把 xyw 輸入到網路中，並經過一串多項式運算，如同下圖求出
Loss 為 16，這個步驟就是 Forward Pass 前傳導</p>
<p><img src="https://i.imgur.com/ixuxvfW.png"
alt="未命名绘图.drawio" /></p>
<hr />
<p>那計算出來的 Loss 是要做什麼的呢？其實這個 Loss
除了看網路訓練的好不好之外，還可以用來計算梯度並更新每個節點上的權重。</p>
<p>什麼是梯度呢？高中數學我們會學到，在一個二維曲線上畫一條切線，就代表它的斜率；如果是在物理上，在
v-t
圖的一個時間點上找切線斜率，則是代表瞬時速度。只是在深度學習中，我們習慣稱為梯度，英文為
Gradient，數學符號為 <span class="math inline">\(\nabla\)</span></p>
<p>那梯度在深度學習中代表的函意又是什麼呢？代表在多維的空間中，某一點的斜率。以三維空間為例子，三維空間就是一個大曲面，這個曲面有凹有凸，就像一個山脈，一個山脈有山頂、有山谷、有平原、有懸崖…，而梯度類比山脈的例子就相當於是當下等高線的坡度</p>
<p><img src="https://i.imgur.com/aDZmUmo.jpg" alt="image-20220901184131815" style="zoom: 50%;" /></p>
<p>那我們算梯度要做什麼…？還記得前面我們有說過 Loss
的值是要…越小越好對吧？代表網路預測的結果跟真實的結果距離越近，我們要怎麼知道如何修改
w 值才可以使得 Loss
最小？這個問句可以用山脈的例子同等於：我們怎麼走才可以下山？甚至也可以說：我們怎麼走才可以最快的到山下？</p>
<p>答案當然是用滑的阿，有爬過山的都知道上山易下山難，下山時多希望自己有個鋼鐵屁屁可以一口氣滑下山
XD，而深度學習也是利用一模一樣的方法：了解哪裡坡度/梯度最大，就可以快速的滑下山，取得最小的
Loss，同時也取得預測效果最準的結果</p>
<hr />
<p>而要求得梯度只有一種辨法：微分，更詳細的說是偏微分，我們最想要了解最後網路的<strong>Loss
與變量 w 之間的梯度關系</strong>，因此只要求得 loss 對 w
的偏微分，就可以知道 w 要怎麼調整效果會最好了，公式如下： <span
class="math display">\[
\nabla g = \frac{d\,\mathrm{loss}}{dw}
\]</span> 但仔細看會發現…上面這個式跟本算不出來阿，為什麼呢？如果我們把
loss 解壓縮的話： <span class="math display">\[
\nabla g=\frac{d\,\mathrm{loss}}{dw}=\frac{d(\hat y-y)}{dw}
\]</span> loss 中間完全沒有 w 變量阿，倒底要怎麼對 w
做偏微分呢？我們可以先停下腳步來別想要一簇登天直接求得對 w
做偏微分，我們可以先算出 Local
Gradient，也就是每一個權重先對自己的變量求梯度 (loss 先對 s、s 先對
<span class="math inline">\(\hat y\)</span>、<span
class="math inline">\(\hat y\)</span> 先對 w)： <span
class="math display">\[
\frac{d\,\mathrm{loss}}{ds} = \frac{ds^2}{ds}=2s\\
\frac{ds}{d\hat y}=\frac{d(\hat y-y)}{d\hat y}=1\\
\frac{d\hat y}{dw}=\frac{d(x\cdot w)}{dw}=x
\]</span> 更詳細如下圖：</p>
<p><img src="https://i.imgur.com/tOz7cAL.png" alt="test" /></p>
<p>再仔細看看上面的 Local
Gradient，咦…好像怎麼有規律？！這不就是傳說中的 chain rule 嗎？ <span
class="math display">\[
\nabla
g=\frac{d\,\mathrm{loss}}{dw}=\frac{d\,\mathrm{loss}}{ds}\frac{ds}{d\hat
y}\frac{d\hat y}{dw}
\]</span> 也就是說當我們在做 Local Gradient
的時候，其實就是在幫我們最感興趣的<strong>loss 對 w
的偏微分</strong>在計算它的 chain rule，而這個透過 chain rule
一層一層慢慢的找到值的方式，就是 Backpropagation <span
class="math display">\[
\nabla
g=\frac{d\,\mathrm{loss}}{dw}=\frac{d\,\mathrm{loss}}{ds}\frac{ds}{d\hat
y}\frac{d\hat y}{dw}=2s\cdot 1 \cdot x=-16
\]</span> <img src="https://i.imgur.com/bh1nSlm.png" alt="test" /></p>
<p>接著我們再將算出來 Backpropagation
的值，簡單的利用以下的公式去更新每一個權重，其中 <span
class="math inline">\(\nabla g\)</span> 為 Backpropagation 的結果、<span
class="math inline">\(\eta\)</span> 為 learning rate 控制大小用： <span
class="math display">\[
w_{t+1}=w_t-\eta\nabla g
\]</span></p>
<h3 id="要怎麼用程式來實作-backpropagation">要怎麼用程式來實作
Backpropagation？</h3>
<p>以下的例子會回歸最原本的初心，不用高級的 pytorch 工具，而是使用 numpy
來達成以上四個基本操作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義：輸入 x=2，目標 y=4，變量 w=0</span></span><br><span class="line">x = np.array([<span class="number">2</span>], dtype=np.float32)</span><br><span class="line">y = np.array([<span class="number">4</span>], dtype=np.float32)</span><br><span class="line">w = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定義網路前傳導的式子</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * w</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 MSE 均方差來做為 Loss</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> ((y_hat - y)**<span class="number">2</span>).mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 計算 loss 對變數 w 的偏微分</span></span><br><span class="line"><span class="comment"># 這是是直接把式子展開，直接計算偏微分 (沒有用到 chain rule 的概念)</span></span><br><span class="line"><span class="comment"># dloss/dw = d(w*x - y)^2/dw = 2x (w*x - y)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x, y, y_hat</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(<span class="number">2</span>*x, y_hat-y).mean()</span><br><span class="line"></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line">lr = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 開始訓練</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Start Training...&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs+<span class="number">1</span>):</span><br><span class="line">    <span class="comment"># (1) Foward Pass 前傳導</span></span><br><span class="line">    y_hat = forward(x)</span><br><span class="line">    <span class="comment"># (1.5) 計算 Loss</span></span><br><span class="line">    l = loss(y_hat, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (2, 3) 計算 Local Gradient 以及 Backpropagation</span></span><br><span class="line">    dw = gradient(x, y, y_hat)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># (4) 更新權重 w</span></span><br><span class="line">    w -= lr * dw</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;epoch: <span class="subst">&#123;epoch&#125;</span>, loss: <span class="subst">&#123;l:<span class="number">.8</span>f&#125;</span>, w: <span class="subst">&#123;w:<span class="number">.3</span>f&#125;</span>, dw: <span class="subst">&#123;dw:<span class="number">.3</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 印出來的結果</span><br><span class="line">Start Training...</span><br><span class="line">epoch: 1, loss: 16.00000000, w: 0.160, dw: -16.000</span><br><span class="line">epoch: 2, loss: 13.54240036, w: 0.307, dw: -14.720</span><br><span class="line">epoch: 3, loss: 11.46228790, w: 0.443, dw: -13.542</span><br><span class="line">epoch: 4, loss: 9.70168018, w: 0.567, dw: -12.459</span><br><span class="line">epoch: 5, loss: 8.21150303, w: 0.682, dw: -11.462</span><br><span class="line">epoch: 6, loss: 6.95021534, w: 0.787, dw: -10.545</span><br><span class="line">epoch: 7, loss: 5.88266134, w: 0.884, dw: -9.702</span><br><span class="line">epoch: 8, loss: 4.97908545, w: 0.974, dw: -8.926</span><br><span class="line">epoch: 9, loss: 4.21429777, w: 1.056, dw: -8.212</span><br><span class="line">epoch: 10, loss: 3.56698155, w: 1.131, dw: -7.555  &lt;- w=1.131</span><br></pre></td></tr></table></figure>
<p>從印出來的結果可以看到，在 epoch=1 的時候，dw
也就是我們剛剛算出來的值 -16 是完全正確的，接著 dw 值會往 0 靠近，而
Loss 的數值也慢慢降低，以及 w 的值，從 0 慢慢的往答案 2 靠近</p>
<p>但可能會覺得這個訓練效果也太不好了吧…搞了這麼多數學的東西，結果訓練出來的
w 竟然離 2 還很遠！沒關系！這個網路還有很多可以優化的地方：像是增加
epoch 數量、或是增加資料集，都可以使網路效果變更好喔！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 定義：輸入 x，目標 y，新增資料集至 4 組</span></span><br><span class="line">x = np.array([<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>], dtype=np.float32)</span><br><span class="line">y = np.array([<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">16</span>], dtype=np.float32)</span><br><span class="line">...</span><br><span class="line"><span class="comment"># 因為資料不只一組，由於 Gradient 不能是一個 Vector (向量) 必需要是一個 Scalar (純量)，所以要取 mean 平均</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">x, y, y_hat</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(<span class="number">2</span>*x, y_hat-y).mean()  &lt;- 這裡 mean 的作用</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Start Training...</span><br><span class="line">epoch: 1, loss: 30.00000000, w: 1.200, dw: -120.000</span><br><span class="line">epoch: 2, loss: 4.79999924, w: 1.680, dw: -48.000</span><br><span class="line">epoch: 3, loss: 0.76800019, w: 1.872, dw: -19.200</span><br><span class="line">epoch: 4, loss: 0.12288000, w: 1.949, dw: -7.680</span><br><span class="line">epoch: 5, loss: 0.01966083, w: 1.980, dw: -3.072</span><br><span class="line">epoch: 6, loss: 0.00314570, w: 1.992, dw: -1.229</span><br><span class="line">epoch: 7, loss: 0.00050332, w: 1.997, dw: -0.492</span><br><span class="line">epoch: 8, loss: 0.00008053, w: 1.999, dw: -0.197</span><br><span class="line">epoch: 9, loss: 0.00001288, w: 1.999, dw: -0.079</span><br><span class="line">epoch: 10, loss: 0.00000206, w: 2.000, dw: -0.031  &lt;- YA, w=2 了！</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(十二)：一切的開端 - SGD vs Momentum</title>
    <url>/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%BA%8C-%EF%BC%9ASGD-vs-Momentum/</url>
    <content><![CDATA[<p>在以前第九章中，有很 ~ 淺的列舉了一些優化器 optimizer，在第十二 ~
十四章中，會更詳細一點去介紹，這些 optimizer
的原理，以及當初提出是要改進什麼事？</p>
<p>keywords:SGD、Momentum <span id="more"></span></p>
<h2 id="梯度下降-gradient-desent">梯度下降 Gradient Desent</h2>
<p>還記得在第十章中有介紹了什麼是損失
Loss、什麼是梯度，以及網路是如何利用梯度來找到最佳解嗎？如果忘記的話可以來這邊複習喔
<a
href="https://mushding.space/2022/12/29/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81-%EF%BC%9APytorch-%E5%A6%82%E4%BD%95%E5%AF%A6%E9%A9%97-Backpropagation-%E4%B9%8B%E4%BB%80%E9%BA%BC%E6%98%AF-Backpropagation/">你所不知道的
Pytorch 大補包(十)：Pytorch 如何實做出 Backpropagation 之什麼是
Backpropagation</a></p>
<p>在裡面提到了網路中的函數非常複雜，複雜到我們沒辦法用一般多項式的方法來求解，所以我們將損失函數對權重做一階微分，得到網路的梯度。利用梯度下降法，一步步縮小
Loss，像在山坡地上滑溜滑梯一樣，滑到最低點，就可以找到最接近真實答案的結果了。</p>
<p>Loss 損失函數的公式如下：給定資料 x 與權重 w，經過層層運算 f()
得到結果後，再與標記 y 計算損失</p>
<p><span class="math display">\[
\mathcal{L} = \mathcal{L}_{\mathrm{class}}(f(x,w),y)
\]</span></p>
<p>梯度公式如下，在深度學習中我們稱這個符號 <span
class="math inline">\(\nabla\)</span> ，代表梯度的意思</p>
<p><span class="math display">\[
\nabla g = \frac{\partial\mathcal{L}}{\partial w}
\]</span></p>
<p>而梯度下降法則是利用梯度的值來修改權重 <span
class="math inline">\(w\)</span>，其中 <span
class="math inline">\(w_t\)</span> 代表目前的權重，<span
class="math inline">\(w_{t-1}\)</span> 代表上一次的權重，公式如下：</p>
<p><span class="math display">\[
w_{t}=w_{t-1}-\nabla g
\]</span></p>
<h2 id="什麼是優化器-optimizer">什麼是優化器 optimizer</h2>
<p>所謂優化器指「優化」網路做梯度下降的「速度」或「效果」，也就是說剛剛介紹的梯度下降其實還存在著許多的缺點，例如：收斂時間久、效果不穩定…等</p>
<p>而一個最最簡單概念的優化器
(這個概念是我自己想的，有些人可能不這麼覺得…) 就是學習率 learning
rate，符號通常表示 <span class="math inline">\(\eta\)</span></p>
<p>學習率設計用來控制梯度大小用，因為通常梯度算出來都很大，所以學習率會設介於
0.1 ~ 0.0001
的區間來縮小梯度計算結果，詳細可看第十章實驗，實驗結果可知如果不加學習率，梯度會超大，網路永遠都不可能會收斂</p>
<p>加入學習率的梯度下降公式如下：</p>
<p><span class="math display">\[
w_{t}=w_{t-1}-\eta\nabla g
\]</span></p>
<p>像這種找到梯度下降的缺點，並加以改進的方法，就可以稱作為一種優化器。</p>
<h2 id="sgd">SGD</h2>
<p>快速複習完梯度下降 (Gradient Desent, GD)
後，緊接來介紹應用最廣、最穩定，也最元老的優化器：SGD</p>
<p>SGD 全名為 Stochastic Gradient Descent，中文稱：隨機梯度下降法</p>
<p>其實它跟剛剛上面我們介紹的加入學習率後的公式一模一樣，只是在「計算對象」及「方法」做了一點點的小修改，而這個故事中間有一點點關於歷史淵源，下面做簡單的介紹：</p>
<p>理論上的梯度下降會把<strong>全部</strong>的資料都看過一遍之後，用<strong>全部</strong>的資料去計算梯度，並更新一次參數。</p>
<p>但理想很豐滿；現實很骨感，在現實中我們的資料集又大又多，動輒幾 G
甚至幾 T
起跳的，實作上沒有辦法暫存下這麼多資料，然後再一次更新的，於是有人提出
Mini-Batch Gradient
Desent，我們不看完整個資料集更新一次參數，而是設定一個 mini batch 的數值
(其實就是現在說的 batch，可能以前的人覺得 256、512
這些數字相對於全部的資料集來說，數字小了不少)，一個 mini batch
就更新一次參數。</p>
<p>而為了增加網路複雜度，每次都會「隨機」取樣
mini-batch，直到看完全部的資料集，使網路每一次看資料集的順序都不太一樣。這個隨機取樣的方法就稱作
Stochastic Gradient Descent (SGD)，在現在來說，常說 SGD 指的就是
mini-batch 的 SGD，在命名上有一點小小落差。</p>
<p>SGD
的公式如下：基本上與上一章介紹的差不多，只有在計算損失函數中參與計算的資料集，只限定在一個
batch 之中。</p>
<p><span class="math display">\[
\begin{gather}
w_{t+1}=w_t-\eta\nabla g \\
\nabla g = \frac{\partial\mathcal{L}}{\partial w} \\
\mathcal{L} =
\mathcal{L}_{\mathrm{class}}(f(x_{\mathrm{batch}},w),y_{\mathrm{batch}})
\end{gather}
\]</span></p>
<p>在 Pyroch 中，torch.optim 提供了非常多的優化器選擇，要使用 SGD
非常簡單，<a
href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">SGD
document 在這裡</a></p>
<p>只需要給定 learning 與 網路中的參數就可以了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<h2 id="momentum">Momentum</h2>
<p>SGD
的優點從數學公式中可以看出來，就是穩定，每一次在重新參數的時候只會根據當前的梯度來計算要更新的權重值，一步一腳印的慢慢更新。而缺點與優點相同，慢慢更新的代價是網路收斂的速度慢了點。</p>
<p>因此有人提出 Momentum，在原本 SGD
更新權重時除了考量當前的梯度外也會考量前一時刻的算出來的梯度，這個概念類似於物理動量的概念，在一個時間點上物體的速度等於目前當下的速度加上前幾個時刻累積的動量。</p>
<p>寫成數學公式的話如下：</p>
<p><span class="math display">\[
\begin{gather}
w_{t+1} = w_t-m_t \\
m_t = \gamma m_{t-1} + \eta \nabla g
\end{gather}
\]</span></p>
<p>式子中的 m 是指前一刻計算出來的更新值，除了計算當前的 t
的梯度外，也會考量到以往累積計算下來的
m，每一次的更新值都會受到歷史的因素影響。另外加另一個超參數 <span
class="math inline">\(\gamma\)</span>，可以自由控制網路受多少比例的動量控制</p>
<p>而 Momentum
最大的好處就是更新速度快，在當前梯度與前一次動量方向相同下，每一次在更新時可以根據前一時刻的方向與數值，加成往下收斂的速度</p>
<p>而另一方面如果當前梯度與前一次動量方向相反的話，則可以使網路有離開
Local minimum 與 plateau
的能力，就像一顆從山上滑下來的球，如果遇到小山丘或是小平地，球會選擇保留以往的動量而繼續往同方向滑，甚至有機會越過小山丘繼續滑到
Global minimum。</p>
<p>來看動畫會更清楚，下圖 git 來自 <a
href="https://julien-vitay.net/lecturenotes-neurocomputing/intro.html">Neurocomputing</a></p>
<p>可以更清楚的了解 SGD 與 Momentum 的差別</p>
<p><img
src="https://julien-vitay.net/lecturenotes-neurocomputing/_images/momentum-sgd.gif" /></p>
<p>而在 Pytorch 實作中，Momentum 歸類到與 SGD 同一個函式中，Momentum 為
SGD 的一個參數，這個參數就是 Momentum 公式中的 <span
class="math inline">\(\gamma\)</span> 超參數，設為 0 代表傳統 SGD，設為
0 ~ 1 之間代表啟用 Momentum 並設定比例。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<h2 id="reference">Reference</h2>
<p><a
href="https://pytorch.org/docs/stable/generated/torch.optim.SGD.html">Pytorch
optimizer document</a></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(十五)：我的模型訓練好；可是測試不好怎麼辦…？- overfitting 與 regularization</title>
    <url>/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%BA%94-%EF%BC%9A%E6%88%91%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%A8%93%E7%B7%B4%E5%A5%BD%EF%BC%9B%E5%8F%AF%E6%98%AF%E6%B8%AC%E8%A9%A6%E4%B8%8D%E5%A5%BD%E6%80%8E%E9%BA%BC%E8%BE%A6%E2%80%A6%EF%BC%9F-overfitting-%E8%88%87-regularization/</url>
    <content><![CDATA[<p>overfitting、underfitting
這兩個詞相信有在碰深度學習人一定都不陌生，學校裡有都有教。但是在實作中，遇到什麼樣子的情況可以稱作
overfitting？網路會有怎樣的表現？下一步要怎麼來解決？</p>
<p>以下文章會把目光放在 overfitting 上來講解</p>
<p>keywords: Overfitting、Regularization、Weight Decay、Label
Smoothing、Warmup <span id="more"></span></p>
<h2 id="什麼是-overfittingunderfitting">什麼是
overfitting、underfitting</h2>
<p>在深度學習中會使用 Loss
表示網路找到的迴歸區線與現實資料分佈的差異，並且利用 Loss
進一步算出梯度後更新參數，使網路更符合現實資料的分佈</p>
<p>在實作中會把資料集分為三種：訓練集 Training Set、驗證集 Validation
Set、測試集 Testing
Set，不同的資料集會有著不同的資料分佈，但理論上因為是從同一筆資料分出來的，所以彼此之間應該不會差太多</p>
<p>Underfitting 的意思是：訓練得很不好 (訓練 Loss 高)</p>
<p>Overfitting 的意思是：訓練得很好 (訓練 Loss 低)，可是測試時不好 (測試
Loss 高)</p>
<p>如下圖：左圖是 underfitting，中圖是正常，右圖是 overfitting</p>
<p><img src="https://i.imgur.com/i0fDKv1.png" alt="Image" /></p>
<p>用下面的網站來進一步解釋
(這是一個簡單的迴歸線視覺化網站，裡面有很多東西可以自定義，可以解釋很多深度學習的一些現象)</p>
<p><a href="http://playground.tensorflow.org/">Tinker With a Neural
Network Right Here in Your Browser.</a></p>
<p>Underfitting 的意思是，訓練 Loss
還太高，網路迴歸的能力還沒有很好，常發生在</p>
<ul>
<li>網路訓練初期</li>
<li>網路架構太淺</li>
</ul>
<p><img src="https://i.imgur.com/92QDjeE.png" width="50%" height="50%" /></p>
<p>而 Overfitting 的意思是，訓練 Loss
很好、網路在訓練資料集有著很強的能力，可是面對新的驗證資料分佈時，反而效果變很差，驗證
Loss 很高</p>
<p>最明顯的特是：網路訓練到後期，驗證 Loss 與訓練 Loss
有一段小差距，甚至這個差距還會越來越大，驗證 Loss 不斷的在上升</p>
<p><img src="https://i.imgur.com/wjlyCLJ.png" width="50%"></p>
<p>再舉一個我的親身經驗：下面是我其中一個實驗訓練與驗證 Loss
的曲線圖：紅框的部份很明顯訓練跟驗證間隔拉大了</p>
<p><img src="https://i.imgur.com/h3nhJmv.png" alt="Image" /></p>
<h2 id="如何解決-overfitting">如何解決 Overfitting</h2>
<p>相較於 Underfitting，Overfitting
的成因複雜的很多，不過倒是可以總結成一句話：網路泛化能力 Generalization
不好的時候會發生，也就是網路只要換一個資料集就沒用了，完全沒什麼自行推論沒看過的資料的能力</p>
<p>那…泛化能力不好又是如何發生？網路中有過度複雜以及不具無意義的特徵，而網路又過度偏好這些複雜的特徵，使得迴歸區線過於複雜</p>
<p>…聽不懂？一樣來看剛剛網網站的例子，從右上角的圖可以很明顯發現網路
Overfitting
了，而在左手邊權重值的地方，仔細看可以發現其中一些權重的輸出值特別高
(線的顏色特別深)，使得網路過份依賴這些複雜的權重。</p>
<p><img src="https://i.imgur.com/4Lkyskt.png" alt="Image" /></p>
<p>這樣會有什麼問題，如果我們換一個資料集，這些權重過大的特徵很有可能與新資料集的特徵完全不符合，導致訓練很好，但是驗證不好的情況發生</p>
<h2 id="解決-overfitting-的一些方法">解決 Overfitting 的一些方法</h2>
<p>知道了發生原因之後，接下來就介紹幾個解決 Overfitting 的方法</p>
<h3 id="增加資料集">增加資料集</h3>
<p>泛化能力不好 -&gt; 資料集不夠多樣化 -&gt; 需要更多的資料去訓練 -&gt;
增加網路的 Robust (強健性)</p>
<p>這個方法是最最跟本的解決之道，就是…既然問題出在資料集上面嘛…那就想辦法再加更多資料集啦</p>
<p>但有時礙於資料不好取得，沒有辦法拿到太多真實的資料時，還有資料擴增可以使用
(Data Augmentation) 一樣也可以增加資料的複雜程度</p>
<p>增加資料集、使用資料擴增都是解決 Overfitting
的根本之道，那如果我都做了還是發生 Overfitting 呢？下面還有幾個 trick
可以試試看</p>
<h3 id="weight-decay">Weight Decay</h3>
<p>在損失函數中有一項叫做規則項 Regularization Term，通常都會是以 L2
Regularization 為主。定義一損失函數 <span
class="math inline">\(\mathcal{L}\)</span>，在後面加上<strong>網路權重的平方和</strong>，也就是
L2 Regularization，公式如下：</p>
<p><span class="math display">\[
\mathcal{L} = \mathcal{L_{\mathrm{class}}(f(x,w),y)} + \lambda
\sum_{i=0}^n w_i^2
\]</span></p>
<p>規則項目的在處罰網路的權重值，使得網路不要太偏重單一複雜的權重，而出現泛化能力不好的問題。式子中用
<span class="math inline">\(\lambda\)</span>
超參數來調節規則項的強度，而它有一個特別的名字叫做 Weight decay</p>
<p>為什麼要叫做 Weight decay 呢？我們把新的損失函式套進 SGD
計算梯度算一下：</p>
<p>定義損失函數</p>
<p><span class="math display">\[
\mathcal{L} = \mathcal{L_{\mathrm{class}}(f(x,w),y)} + \lambda
\sum_{i=0}^n w_i^2
\]</span></p>
<p>定義 SGD</p>
<p><span class="math display">\[
w_{t+1} = w_t -\eta \nabla g, \quad \nabla g = \frac{\partial
\mathcal{L}}{\partial w_t}
\]</span></p>
<p>把 <span class="math inline">\(\mathcal{L}\)</span> 代入到 SGD
中，並且對 <span class="math inline">\(w_t\)</span> 做偏微分，得：</p>
<p><span class="math display">\[
\begin{aligned}
w_{t+1} &amp;= w_t - \eta \frac{\partial
(\mathcal{L}_{\mathrm{class}}(f(x, w), y)+\lambda \sum_{i=0}^n
w_i^2)}{\partial w_t}\\
&amp;=w_t - \eta \cdot (\frac{\partial
\mathcal{L}_\mathrm{class}}{\partial w_t} + 2\lambda w_t)\\
&amp;= w_t - \eta \frac{\partial \mathcal{L}_\mathrm{class}}{\partial
w_t}-2\eta\lambda w_t
\end{aligned}
\]</span></p>
<p>最後得出來的結果前半項 <span class="math inline">\(w_t - \eta
\frac{\partial \mathcal{L}_\mathrm{class}}{\partial w_t}\)</span> 就是
SGD，而後面多減了一個常數項 <span class="math inline">\(2\eta\lambda
w_t\)</span></p>
<p>這個常數項就是 Weight decay 的來源，<span
class="math inline">\(w_t\)</span>
在梯度下降權重一直更新的同時，也會一直多減掉這個常數項，而且還是自己減自己，使得
<span class="math inline">\(w_t\)</span>
會不斷的越來越小，像元素半衰期一樣，越來越小越來越小…直到接近 0</p>
<p>因為 Weight decay
會使得網路中的每一個權重都不斷的減自己，所以較不會有鶴立雞群的權重，網路不會過度依賴特定複雜的特徵，使得網路泛化能力很差</p>
<p>規則項有另外一個名稱：懲罰項
(penalty)，意思指說，加入這一項後對網路而言效果反而會變差，其實也蠻合理的，如果你叫一個小朋友每天自己檢討自己，久了之後心情一定會變差的嘛</p>
<p>所以其實加上 Weight decay 雖然能解決 overfitting
的問題，但其實背後的原理是犧牲複雜特徵 (減少特徵)
換來的，放棄一些離群的資料來換取比較好的損失結果，並沒有解決到最根本的問題：資料</p>
<p>以下是用網站模擬加了 Weight decay 會發生什麼事，這裡實驗
Regularization 設定 0.1 (就是 Weight decay
的意思)，發現網路中的每一個參數都相對平均沒有某一個特別突出，
且在分類的迴歸線中也可看到網路選擇放棄那些離群的點，利用放棄一些資料來換取更好的
Loss</p>
<p><img src="https://i.imgur.com/O11jPdh.png" alt="Image" /></p>
<p>SGD、RMSProp、AdaGrad、Adam 這些個優化器都可以加上 weight decay，在
Pytorch 中兩個函式都有 <code>weight_decay</code>
的參數，其作用就是在調節 L2 Regularization 的大小，當
<code>weight_dacay</code> 為 0 就代表不使用，應用起來很方便
(調整一下參數就可以了)</p>
<p><img src="https://i.imgur.com/TmVv9Y0.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/NrfP5PV.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/rwph3AM.png" alt="Image" /></p>
<p><img src="https://i.imgur.com/QY9ie8R.png" alt="Image" /></p>
<p>在實作中依據網路的情況再設定就可，沒有什麼一定的正確解答，但我自己的個人經驗是
SGD 可以設大一點 0.1、Adam 要小一點 0.01 左右</p>
<h3 id="label-smoothing">Label Smoothing</h3>
<p>Label Smoothing
中文稱標籤平滑化，是應用在分類任務上的一個特別做法，原本在網路給定標籤時輸出機率都是
0 or 1 的整數，也就是對一定要全對、錯一定全錯 (目標輸出機率為 1 其它都是
0)</p>
<p>例如有一個 5 分類的任務，假設標籤為 1，則網路最好的機率輸出結果為 (1,
0, 0, 0, 0) -&gt; 很肯定的的答案</p>
<p>那 Label Smoothing 就是把目標變一下，全部以 <span
class="math inline">\(\alpha\)</span>
為主往中間靠近一些，把標籤變得模糊一些不要那麼極端，公式如下：</p>
<p><span class="math display">\[
\begin{equation}
  y* =
    \begin{cases}
      1-\alpha &amp; \text{if $y=1$}\\
      \alpha/(n-1) &amp; \text{otherwise}
    \end{cases}       
\end{equation}
\]</span></p>
<p>以上面的例子假設 <span class="math inline">\(\alpha = 0.4\)</span>
就會變成 (0.6, 0.1, 0.1, 0.1,
0.1)，讓網路在預測時不要給一個那麼肯定的答案。</p>
<p>那為什麼要做 label smoothing
呢？隨著時間訓練網路對於肯定的資料效果一定是越來越好，輸出的機率值越來越肯定，但是如果突然來了一張模稜兩可的資料，或是…離群資料，網路會不會就看錯了呢？</p>
<p>隨著這個想法，如果我們能在訓練時加強訓練難度，標籤不要給的太肯定去訓練，但是測試是用原本的標籤來做，這樣訓練難測試簡單就可以一部份避免
overfitting 的問題了</p>
<p>如果使用 cross entropy 作為損失函數的話，實作 label smoothing
會非常簡單，這是因為 cross entropy 的公式很好修改：</p>
<p><span class="math display">\[
\mathcal{L} = -\sum_{c=1}^Cw_c\log p(f(x_c))y_c
\]</span></p>
<p>公式中只需要把 <span class="math inline">\(y_c\)</span> 把設定從 (0,
1) 改成 -&gt; (<span class="math inline">\(\alpha\)</span>, <span
class="math inline">\(1-\alpha\)</span>, ...) 就可以了</p>
<p>cross entropy 實作 label smoothing 很簡單，在 Pytorch 的
<code>torch.nn.CrossEntropyLoss</code> 中有 <code>label_smoothing</code>
這個參數，就是在設定上面公式的 <span
class="math inline">\(\alpha\)</span> 值</p>
<p><img src="https://i.imgur.com/3nggRIh.png" alt="Image" /></p>
<h3 id="warmup">Warmup</h3>
<p>模型訓練權重的初始值為隨機生成，因此第一個 epoch 通常有較大的
loss，較大的梯度使得模型權重每次改變都較大，可能導致訓練時梯度下降至
Local minimum 或 Sharp
minimum，進而導致：一、訓練還沒到最低點；二、網路不
robust，資料一更動就差很多，而這個就是，泛化能力不好 (overfitting)</p>
<p><img src="https://i.imgur.com/BzZu94O.png" alt="Image" /></p>
<p>使用 Learning rate Warmup
學習率暖身策略可避免這個問題，在網路學習初期用較小的學習率訓練，使隨機初始化的參數先「暖身」這筆資料集的分布，再把學習率接回原策略正常訓練，主要解決：避免過高的學習率容易導致模型不穩的問題。</p>
<p>通常的做法：在前 5 個 epoch，學習率由 0
<strong>線性</strong>調至初始學習率</p>
<p><img src="https://i.imgur.com/qXUKtQZ.png" alt="Image" /></p>
<p>在 Pytorch 中沒有一個很像 Warmup 的函式，最接近的是
CosineAnnealingLR</p>
<p><img src="https://i.imgur.com/fDFLFKH.png" alt="Image" /></p>
<p>但如果是用其它 lr_scheduler 像是 MultiStepLR 或是 ReduceLROnPlateau
要加上 Warmup 的話會比較麻煩，需要借助 github
上面大神們的幫助，下面是其中一個我用過的 Warmup，用起來也很簡單，只需要
pip install 一下就好了，詳細參考 Readme.md</p>
<p><a
href="https://github.com/ildoonet/pytorch-gradual-warmup-lr">github
-&gt; pytorch-gradual-warmup-lr</a></p>
<p>上面 Repo 有個要注意的地方，就是它的程式其實是錯的 XDD，有人有發
issue 修改，但可能原作者已經放推了，所以沒再後續維護 (我也懶得 fork 它
XD)，所以有一行程式要修改一下，大家注意一下</p>
<p><a
href="https://github.com/ildoonet/pytorch-gradual-warmup-lr/issues/18">Math
is wrong for multiplier=1 #18</a></p>
<h2 id="reference">Reference</h2>
<p><a href="https://www.ibm.com/topics/overfitting">IBM What is
overfitting? (總覽)</a></p>
<p><a
href="https://www.geeksforgeeks.org/underfitting-and-overfitting-in-machine-learning/">ML
| Underfitting and Overfitting (GeekforGeeks)</a></p>
<p><a href="https://hackmd.io/@allen108108/Bkp-RGfCE">Regularization
方法 : Weight Decay , Early Stopping and Dropout (weight decay
公式推導)</a></p>
<p><a href="https://ithelp.ithome.com.tw/articles/10306518">[Day27]
Weight Decay Regularization</a></p>
<p><a
href="https://ithelp.ithome.com.tw/articles/10305524?sc=iThelpR">[Day25]
Label Smooth</a></p>
<p><a
href="http://playground.tensorflow.org/">好玩的網路訓練模擬網站</a></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
        <tag>regularization</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(十六)：AdamW 與 Adam 差在哪裡…？</title>
    <url>/2023/03/25/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E5%85%AD-%EF%BC%9AAdamW-%E8%88%87-Adam-%E5%B7%AE%E5%9C%A8%E5%93%AA%E8%A3%A1%E2%80%A6%EF%BC%9F/</url>
    <content><![CDATA[<p>AdamW 在 2017 年提出，它與在 2014 年提出的 Adam 差在哪裡，而 AdamW
又是發現了 Adam 有什麼可以改進的地方嗎？</p>
<p>keywords: AdamW、Adam <span id="more"></span></p>
<h2 id="一句話總結">一句話總結</h2>
<p>簡單用一句話總結 AdamW，因為 Adam 加上 Weight decay
實作方法不合理，所以微微修改 Weight decay 加上去的地方，使得 AdamW
有計算量少、數學公式較合理等特色</p>
<h2 id="weight-decay-發生什麼事">Weight decay 發生什麼事？</h2>
<p>在前一章介紹了 Weight decay，它是由 L2 Regularization
延伸出來的概念，當在損失函數中加入權重的平方項，將損失函數值對權重值作偏微分得到
<span class="math inline">\(2\lambda\eta w\)</span> 這一項，這一大坨就是
Weight decay (更詳細的推導過程可以參考：<a
href="https://mushding.space/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E4%BA%94-%EF%BC%9A%E6%88%91%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%A8%93%E7%B7%B4%E5%A5%BD%EF%BC%9B%E5%8F%AF%E6%98%AF%E6%B8%AC%E8%A9%A6%E4%B8%8D%E5%A5%BD%E6%80%8E%E9%BA%BC%E8%BE%A6%E2%80%A6%EF%BC%9F-overfitting-%E8%88%87-regularization/">你所不知道的
Pytorch 大補包(十五)：我的模型訓練好；可是測試不好怎麼辦…？- overfitting
與 regularization</a>)</p>
<p><span class="math display">\[
\mathcal{L} = \mathcal{L_{\mathrm{class}}(f(x,w),y)} + \lambda
\sum_{i=0}^n w_i^2
\]</span></p>
<p><span class="math display">\[
w_{t+1} = w_t - \eta \frac{\partial \mathcal{L}_\mathrm{class}}{\partial
w_t}-2\eta\lambda w_t
\]</span></p>
<p>然而在這篇文章中有一個假設，假設我們的優化器是用最原始的 SGD，連動量
Momentum 都沒有，才會推導出 <span class="math inline">\(2\lambda\eta
w\)</span> 這一項</p>
<p>那如果是 Adam 會變成怎樣呢？首先是 Adam 的公式：</p>
<p><span class="math display">\[
w_{t+1} = w_t-\eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}
\]</span></p>
<p><span class="math display">\[
m_{t} = \beta_1\cdot m_{t} + (1-\beta_1)\cdot \nabla g_{t-1}
\]</span></p>
<p><span class="math display">\[
v_{t} = \beta_2\cdot v_{t} + (1-\beta_2)\cdot (\nabla g_{t-1})^2
\]</span></p>
<p>再把 <span class="math inline">\(\nabla g_t\)</span> 拆開：</p>
<p><span class="math display">\[
\begin{aligned}
m_{t} &amp;= \beta_1\cdot m_{t} + (1-\beta_1)\cdot \nabla g_{t-1}\\
&amp;=\beta_1\cdot m_t + (1-\beta_1) \cdot \nabla g_ {t-1} +
\color{red}(1-\beta_1) \cdot 2\lambda w
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
v_{t} &amp;= \beta_1\cdot v_{t} + (1-\beta_1)\cdot \nabla (g_{t-1})^2\\
&amp;=\beta_1\cdot v_t + (1-\beta_1) \cdot \nabla (g_ {t-1})^2 +
\color{red}(1-\beta_1) \cdot (4w\nabla g+4\lambda w^2)
\end{aligned}
\]</span></p>
<p>可以看到在公式後面紅紅的地方就是因 Weight decay 而多產生的常數項</p>
<p>AdamW 這篇作者認為，在 SGD
時，因為優化器額外項不多不複雜，所以最後的常數項數值都會是 <span
class="math inline">\(2\lambda w\)</span></p>
<p>但後來的優化器加上動量、加上動態學習率的分母，早早就加在損失函數上的
L2
Regularization，會隨著各種微分，數值不僅會散掉，同時還會增加不少額外的計算量</p>
<p>因此作者提出 Adam with decoupled weight decay (AdamW)，如果要在 Adam
中使用 Weight decay，不會使用 L2 Regularization
加在損失函數上的概念，而是直接加在優化器上，如圖 (論文原圖)：</p>
<p><img src="https://i.imgur.com/1SoW9fl.png" alt="Image" /></p>
<p>也就是剛剛 Adam 一大坨看不懂的東西會直接變成這樣：</p>
<p><span class="math display">\[
w_{t+1} =
w_t-\eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}-\color{red}2\lambda
w
\]</span></p>
<p><span class="math display">\[
m_{t} = \beta_1\cdot m_{t} + (1-\beta_1)\cdot \nabla g_{t-1}
\]</span></p>
<p><span class="math display">\[
v_{t} = \beta_2\cdot v_{t} + (1-\beta_2)\cdot (\nabla g_{t-1})^2
\]</span></p>
<p>直接套在優化器後面，就不會因經過很多層微分運算而有：計算量大、數值分散等問題，而且從數學式子角度來看，也比較直白好理解</p>
<p>至於 AdamW 真的會比 Adam
好嗎？論文中當然會是說效果比較好啦，但真正情況就要看各個實驗的資料集了，不過可以確定的是
AdamW 的運算量比 Adam 小的。</p>
<p>當然最重要的是，如果實驗中沒有使用到 Weight decay 的話，那 Adam 與
AdamW 是一模一樣的！</p>
<h2 id="reference">Reference</h2>
<p><a
href="https://www.fast.ai/posts/2018-07-02-adam-weight-decay.html">AdamW
and Super-convergence is now the fastest way to train neural nets
(fast.ai) (英文很詳細)</a></p>
<p><a
href="https://blog.csdn.net/weixin_45743001/article/details/120472616">Adam和AdamW的区别
(一句話總結)</a></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>你所不知道的 Pytorch 大補包(十四)：後起之秀 - Adam 之為什麼我的 Adam 比 SGD 效果差？</title>
    <url>/2023/03/16/%E4%BD%A0%E6%89%80%E4%B8%8D%E7%9F%A5%E9%81%93%E7%9A%84-Pytorch-%E5%A4%A7%E8%A3%9C%E5%8C%85-%E5%8D%81%E5%9B%9B-%EF%BC%9AAdam-%E8%88%87-AdamW-%E5%B7%AE%E5%9C%A8%E5%93%AA%E8%A3%A1%EF%BC%9F/</url>
    <content><![CDATA[<p>問：為什麼剛剛前幾個介紹的優化器最近都不怎麼出現過，反而較近期的
BERT、最近流行的 Transformer 架構 ViT，都是使用 Adam
優化器，是…因為新潮所以使用它嗎？還是 Adam 真的有什麼可取之處？</p>
<p>keywords: Adam <span id="more"></span></p>
<h2 id="adam">Adam</h2>
<p>Adam 優化器在 2014 年提出，相較於 SGD、RMSProp
來說是相對比較新的優化器。論文連結：<a
href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic
Optimization</a></p>
<p>Adam 名稱來自：Adaptive Moment
Estimation，直翻就是「動態動量預估」，其特色是融合了 AdaGrad 與 RMSProp
各自的優點，並且在這之上額外加入了 bias-correction</p>
<p>以下是論文原文：</p>
<blockquote>
<p>the name Adam is derived from adaptive moment estimation. Our method
is designed to combine the advantages of two recently popular methods:
AdaGrad (Duchi et al., 2011), which works well with sparse gradients,
and RMSProp (Tieleman &amp; Hinton, 2012)</p>
</blockquote>
<p>公式有點複雜，先來看核心公式：</p>
<p><span class="math display">\[
w_{t+1} = w_t-\eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}
\]</span></p>
<p>Adam 公式中可分為兩個部份，一個長得像 Momentum 記作 <span
class="math inline">\(\hat{m_t}\)</span> 稱做第一動量 (first moment
estimate)，一個長得像 RMSProp 記做 <span
class="math inline">\(\hat{v_t}\)</span> 稱做第二動量 (second raw moment
estimate)，最後分母的 <span class="math inline">\(\epsilon\)</span>
是平滑項避免除以 0。</p>
<p><span class="math inline">\(\hat{m_t}\)</span> 如同 Momentum
有歷史梯度平均的資訊，優點為更新速度快，<span
class="math inline">\(\hat{v_t}\)</span> 如同 RMSProp
有歷史梯度平方的平均，優點為動態調整學習率，但又不會因數值太使更新值接近
0</p>
<p>而各別的 <span class="math inline">\(\hat{m_t}\)</span> <span
class="math inline">\(\hat{v_t}\)</span> 公式列記在下面：</p>
<p><span class="math display">\[
m_{t} = \beta_1\cdot m_{t} + (1-\beta_1)\cdot \nabla g_{t-1}
\]</span></p>
<p><span class="math display">\[
v_{t} = \beta_2\cdot v_{t} + (1-\beta_2)\cdot (\nabla g_{t-1})^2
\]</span></p>
<p>Adam 有兩個超參數可調整，<span class="math inline">\(\beta_1\)</span>
控制 <span class="math inline">\(m_t\)</span> 預設 0.9，<span
class="math inline">\(\beta_2\)</span> 控制 <span
class="math inline">\(v_t\)</span> 預設 0.999，兩個超參數超接近 1
目的是使權重更新傾向參考<strong>歷史梯度</strong>而非目前梯度，使網路在遇到較複雜的曲面時有比較穩定的表現
(不會因為目前梯度變化大而「三心二意」的)</p>
<p><span class="math inline">\(\beta_2\)</span> 又比 <span
class="math inline">\(\beta_1\)</span> 更靠近 1，因為 <span
class="math inline">\(\beta_2\)</span>
負責控制<strong>權重的平方和</strong>，使網路非常非常以歷史權重值為依據更新，如果太傾向考量當前權重值的話
(<span class="math inline">\(1-\beta_2\)</span>)，容易使 <span
class="math inline">\(v_t\)</span> 過大，進而使 <span
class="math inline">\(m_t/\sqrt{v_t}\)</span> 接近 0 更新不了參數了
(就這與 AdaGrad 的老毛病一樣)。</p>
<p>眼睛尖的人可能已經發現了，為什麼在核心公式中 <span
class="math inline">\(\hat{m_t}\)</span> <span
class="math inline">\(\hat{v_t}\)</span> 頭上會有一頂帽子 hat 呢？</p>
<p>這頂帽子代表的是 bias-correction，經由前時刻的梯度計算出來的 <span
class="math inline">\(m_t\)</span> <span
class="math inline">\(v_t\)</span>
還會再經過一個偏差估算的步驟，校正式子中的計算誤差，使得最後正式參與更新的是
<span class="math inline">\(\hat{m_t}\)</span> <span
class="math inline">\(\hat{v_t}\)</span></p>
<p><span class="math display">\[
\hat{m_t} = \frac{m_t}{1-\beta_1^t}, \quad \hat{v_t} =
\frac{v_t}{1-\beta_2^t}
\]</span></p>
<p>計算誤差！？哪裡有誤差？我怎麼沒看到，我覺得到目前為止都沒什麼問題呀。接下來我們來看看如果不加入
bias-corretion，並且時間 <span class="math inline">\(t\)</span> 從 0
開始慢慢往後推會發生什麼事情：</p>
<p>令 <span class="math inline">\(t = 0\)</span> 時，初始動量 <span
class="math inline">\(m_0 = 0, v_0 = 0\)</span>，且 <span
class="math inline">\(\beta_1 = 0.9\)</span> <span
class="math inline">\(\beta_2 = 0.999\)</span></p>
<p>代入得：</p>
<p><span class="math display">\[
\begin{gather}
m_1 = \beta_1 \cdot 0+ (1-\beta_1) \nabla g_0 = 0.1 \nabla g_0\\
v_1 = \beta_2 \cdot 0+ (1-\beta_2) (\nabla g_0)^2 = 0.001 \nabla g_0^2
\end{gather}
\]</span></p>
<p>如果不做 bias-correction 直接放到核心公式中會得：</p>
<p><span class="math display">\[
\begin{aligned}
w_1 &amp;= \\
&amp;= w_0 - \eta\frac{m_1}{\sqrt{v_1}+\epsilon}\\
&amp;= w_0 - \eta \frac{0.1}{\sqrt{0.001}+10^{-8}}\\
&amp;\approxeq w_0 - 3.16\eta
\end{aligned}
\]</span></p>
<p>式子中的常數 3.16 只是一個很 ~
大概除下來的數字，重點是在網路還沒參考任何歷史資訊下，網路對 <span
class="math inline">\(w_0\)</span> 也就是初始參數 (實作上是亂數生成的)
加權比例竟然有 3 倍之多，這個常數完全是因 <span
class="math inline">\(\beta_1\)</span> <span
class="math inline">\(\beta_2\)</span>
一大一小所導致的。在網路還根本不知道往那裡收斂，就使 <span
class="math inline">\(w_0\)</span> 占這麼重要的一部份，顯然不合理</p>
<p>因此 Adam
為了避免<strong>網路學習初期</strong>出現這種不合理的現象，Adam 加上了
bias-correction</p>
<p><span class="math display">\[
\hat{m_t} = \frac{m_t}{1-\beta_1^t}, \quad \hat{v_t} =
\frac{v_t}{1-\beta_2^t}
\]</span></p>
<p>使得 <span class="math inline">\(w_1\)</span> 改為：</p>
<p><span class="math display">\[
\begin{aligned}
w_1 &amp;= \\
&amp;= w_0 - \eta\frac{\hat{m_1}}{\sqrt{\hat{v_1}}+\epsilon}\\
&amp;= w_0 - \eta \frac{g_t}{g_t+10^{-8}}\\
&amp;\approxeq w_0 -\eta
\end{aligned}
\]</span></p>
<p>把初期因 <span class="math inline">\(\beta_1\)</span> <span
class="math inline">\(\beta_2\)</span>
一大一小所造成的影響降到最低。</p>
<p>那既然這個 bias-correction 是針對初期網路設計的，是不是我們可以把
Adam 設計成兩段式的…？還是 bias-correction
對於後續網路也是有影響的…？</p>
<p>更詳細的討論可以參考這一篇 stackoverflow
裡面討論的很不錯，有興趣的人可以沿伸閱讀一下</p>
<p><a
href="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for">Why
is it important to include a bias correction term for the Adam optimizer
for Deep Learning?</a></p>
<p>在 Pytorch 實作中有 Adam 套件可以直接呼叫使用：其中 betas
不用修改照預設的就可以了，最需要注意的是學習率的部份</p>
<p><img src="https://i.imgur.com/Be7rCpb.png" alt="Image" /></p>
<h2 id="使用-adam-還需要調整學習率嗎">使用 Adam
還需要調整學習率嗎？</h2>
<p>AdaGrad、RMSProp、Adam 這三個優化器都有著同樣的目標 -&gt;
動態的調整學習率，在 Pytorch 中也有一個函式庫
<code>torch.optim.lr_scheduler</code> 它的目標也是 -&gt;
動態的調整學習率，那…如果今天我的網路選用這些優化器，我還會需要
lr_scheduler 嗎？或是反過來，我的程式裡有 lr_scheduler
我還可以用上述三個優化嗎？</p>
<p>要回答這個問題可以來參考公式：</p>
<p><span class="math display">\[
w_{t+1} = w_t-\eta\frac{\hat{m_t}}{\sqrt{\hat{v_t}}+\epsilon}
\]</span></p>
<p>可以發現 Adam 中的學習率與 <span class="math inline">\(m\)</span>
<span class="math inline">\(v\)</span> 無關，因此 Adam
中所謂的動態調整學習率，並沒影響到學習率 <span
class="math inline">\(\eta\)</span>，而在 Pytorch
的實作中這兩個東西也是分開實作的，所以這個問題就可以改成：哪一個排別組合對你的網路是正向幫助的</p>
<p>根據下列文章表示，雖然使用 Adam，但適時的用 lr_scheduler
還是有不錯的效果</p>
<p>參考自：<a
href="https://discuss.pytorch.org/t/with-adam-optimizer-is-it-necessary-to-use-a-learning-scheduler/66477">With
Adam optimizer, is it necessary to use a learning scheduler?</a></p>
<h2 id="為什我的網路用-adam-比-sgd-還要差">為什我的網路用 Adam 比 SGD
還要差…？</h2>
<p>你可以說當 <span class="math inline">\(\beta_1=0\)</span> <span
class="math inline">\(v_t=0\)</span> 時，Adam 其實與 SGD
是一模一樣的，也就是 SGD 是 Adam 的一種特例，SGD 是 Adam 的子集合</p>
<p>也因為 Adam 彈性大、可調參數多，相較於 SGD
分佈範圍大，所以比較難調出一個好參數使網路收斂</p>
<p>一般遇到的問題就是學習率設太大，Adam 一般來說初始學習率會設 <span
class="math inline">\(10^{-3}\)</span>，而 SGD 可以設 <span
class="math inline">\(0.1\)</span>，所以在實作上如果只是單純的把 Pytorch
code 從 <code>torch.optim.SGD</code> 改成 <code>torch.optim.Adam</code>
效果一定不會好到那裡去…</p>
<p>參考：<a
href="https://medium.com/ai-blog-tw/deep-learning-%E7%82%BA%E4%BB%80%E9%BA%BCadam%E5%B8%B8%E5%B8%B8%E6%89%93%E4%B8%8D%E9%81%8Esgd-%E7%99%A5%E7%B5%90%E9%BB%9E%E8%88%87%E6%94%B9%E5%96%84%E6%96%B9%E6%A1%88-fd514176f805">為什麼Adam常常打不過SGD？癥結點與改善方案</a></p>
<h2 id="reference">Reference</h2>
<p><a
href="https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for">Why
is it important to include a bias correction term for the Adam optimizer
for Deep Learning?</a></p>
<p><a
href="https://hackmd.io/@allen108108/H1l4zqtp4">Adagrad、RMSprop、Momentum
and Adam – 特殊的學習率調整方式</a></p>
<p><a
href="https://discuss.pytorch.org/t/with-adam-optimizer-is-it-necessary-to-use-a-learning-scheduler/66477">With
Adam optimizer, is it necessary to use a learning scheduler?</a></p>
]]></content>
      <categories>
        <category>Pytorch 大補包</category>
      </categories>
      <tags>
        <tag>Pytorch</tag>
      </tags>
  </entry>
</search>
